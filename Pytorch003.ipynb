{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlNGOJmgGbbPv9/3zotEiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/Pytorch003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocDDLZSwytqW",
        "outputId": "170fe857-98c4-4bac-8f18-2b1215eb06a4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 18801483.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 337711.44it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6153796.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 8314315.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
            "\n",
            "**********\n",
            "epoch 1\n",
            "[1/100] Loss: 2.074165, Acc: 0.323542\n",
            "[1/100] Loss: 1.879214, Acc: 0.479245\n",
            "[1/100] Loss: 1.739272, Acc: 0.539358\n",
            "Finish 1 epoch, Loss: 1.724394, Acc: 0.543993\n",
            "Test Loss: 1.363554, Acc: 0.659037\n",
            "Time:9.1 s\n",
            "**********\n",
            "epoch 2\n",
            "[2/100] Loss: 1.290690, Acc: 0.672031\n",
            "[2/100] Loss: 1.244613, Acc: 0.674375\n",
            "[2/100] Loss: 1.205874, Acc: 0.676302\n",
            "Finish 2 epoch, Loss: 1.202182, Acc: 0.675873\n",
            "Test Loss: 1.099027, Acc: 0.670681\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 3\n",
            "[3/100] Loss: 1.064713, Acc: 0.685365\n",
            "[3/100] Loss: 1.042297, Acc: 0.689844\n",
            "[3/100] Loss: 1.022677, Acc: 0.693281\n",
            "Finish 3 epoch, Loss: 1.021416, Acc: 0.693430\n",
            "Test Loss: 0.977685, Acc: 0.694168\n",
            "Time:9.6 s\n",
            "**********\n",
            "epoch 4\n",
            "[4/100] Loss: 0.945484, Acc: 0.709167\n",
            "[4/100] Loss: 0.935328, Acc: 0.712474\n",
            "[4/100] Loss: 0.927737, Acc: 0.713420\n",
            "Finish 4 epoch, Loss: 0.927337, Acc: 0.713453\n",
            "Test Loss: 0.906571, Acc: 0.714769\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 5\n",
            "[5/100] Loss: 0.879729, Acc: 0.732240\n",
            "[5/100] Loss: 0.873427, Acc: 0.730573\n",
            "[5/100] Loss: 0.868802, Acc: 0.731736\n",
            "Finish 5 epoch, Loss: 0.867290, Acc: 0.732293\n",
            "Test Loss: 0.857604, Acc: 0.725020\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 6\n",
            "[6/100] Loss: 0.830446, Acc: 0.745052\n",
            "[6/100] Loss: 0.832930, Acc: 0.742474\n",
            "[6/100] Loss: 0.826616, Acc: 0.744826\n",
            "Finish 6 epoch, Loss: 0.824634, Acc: 0.745469\n",
            "Test Loss: 0.821533, Acc: 0.734275\n",
            "Time:8.0 s\n",
            "**********\n",
            "epoch 7\n",
            "[7/100] Loss: 0.802782, Acc: 0.752135\n",
            "[7/100] Loss: 0.794040, Acc: 0.754453\n",
            "[7/100] Loss: 0.793012, Acc: 0.754687\n",
            "Finish 7 epoch, Loss: 0.792303, Acc: 0.754747\n",
            "Test Loss: 0.793481, Acc: 0.742735\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 8\n",
            "[8/100] Loss: 0.770533, Acc: 0.757865\n",
            "[8/100] Loss: 0.767632, Acc: 0.761172\n",
            "[8/100] Loss: 0.766619, Acc: 0.762292\n",
            "Finish 8 epoch, Loss: 0.766368, Acc: 0.762443\n",
            "Test Loss: 0.770627, Acc: 0.749801\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 9\n",
            "[9/100] Loss: 0.754021, Acc: 0.765677\n",
            "[9/100] Loss: 0.752488, Acc: 0.765339\n",
            "[9/100] Loss: 0.746385, Acc: 0.768125\n",
            "Finish 9 epoch, Loss: 0.745035, Acc: 0.768490\n",
            "Test Loss: 0.751468, Acc: 0.755872\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 10\n",
            "[10/100] Loss: 0.725407, Acc: 0.774167\n",
            "[10/100] Loss: 0.728078, Acc: 0.772214\n",
            "[10/100] Loss: 0.726798, Acc: 0.772865\n",
            "Finish 10 epoch, Loss: 0.726996, Acc: 0.772921\n",
            "Test Loss: 0.735470, Acc: 0.761346\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 11\n",
            "[11/100] Loss: 0.715226, Acc: 0.778073\n",
            "[11/100] Loss: 0.711711, Acc: 0.777188\n",
            "[11/100] Loss: 0.711073, Acc: 0.777830\n",
            "Finish 11 epoch, Loss: 0.711555, Acc: 0.777469\n",
            "Test Loss: 0.721193, Acc: 0.763336\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 12\n",
            "[12/100] Loss: 0.704388, Acc: 0.780573\n",
            "[12/100] Loss: 0.699080, Acc: 0.782865\n",
            "[12/100] Loss: 0.697322, Acc: 0.782309\n",
            "Finish 12 epoch, Loss: 0.697940, Acc: 0.781883\n",
            "Test Loss: 0.708842, Acc: 0.766819\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 13\n",
            "[13/100] Loss: 0.687408, Acc: 0.782344\n",
            "[13/100] Loss: 0.686131, Acc: 0.784141\n",
            "[13/100] Loss: 0.686879, Acc: 0.784774\n",
            "Finish 13 epoch, Loss: 0.685900, Acc: 0.785198\n",
            "Test Loss: 0.697745, Acc: 0.769705\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 14\n",
            "[14/100] Loss: 0.684869, Acc: 0.785833\n",
            "[14/100] Loss: 0.678361, Acc: 0.788203\n",
            "[14/100] Loss: 0.675433, Acc: 0.789323\n",
            "Finish 14 epoch, Loss: 0.675053, Acc: 0.789079\n",
            "Test Loss: 0.688100, Acc: 0.774084\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 15\n",
            "[15/100] Loss: 0.668020, Acc: 0.792708\n",
            "[15/100] Loss: 0.666814, Acc: 0.792396\n",
            "[15/100] Loss: 0.666619, Acc: 0.791233\n",
            "Finish 15 epoch, Loss: 0.665355, Acc: 0.791944\n",
            "Test Loss: 0.679226, Acc: 0.777667\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 16\n",
            "[16/100] Loss: 0.662029, Acc: 0.790000\n",
            "[16/100] Loss: 0.655943, Acc: 0.795495\n",
            "[16/100] Loss: 0.656798, Acc: 0.794253\n",
            "Finish 16 epoch, Loss: 0.656544, Acc: 0.794260\n",
            "Test Loss: 0.671465, Acc: 0.781150\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 17\n",
            "[17/100] Loss: 0.650690, Acc: 0.798542\n",
            "[17/100] Loss: 0.648687, Acc: 0.797813\n",
            "[17/100] Loss: 0.648720, Acc: 0.796458\n",
            "Finish 17 epoch, Loss: 0.648498, Acc: 0.796842\n",
            "Test Loss: 0.663637, Acc: 0.781947\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 18\n",
            "[18/100] Loss: 0.641719, Acc: 0.799635\n",
            "[18/100] Loss: 0.642931, Acc: 0.797422\n",
            "[18/100] Loss: 0.640949, Acc: 0.798177\n",
            "Finish 18 epoch, Loss: 0.641172, Acc: 0.798324\n",
            "Test Loss: 0.656598, Acc: 0.783838\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 19\n",
            "[19/100] Loss: 0.638290, Acc: 0.798385\n",
            "[19/100] Loss: 0.634971, Acc: 0.799089\n",
            "[19/100] Loss: 0.634677, Acc: 0.799340\n",
            "Finish 19 epoch, Loss: 0.634293, Acc: 0.799907\n",
            "Test Loss: 0.650310, Acc: 0.787022\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 20\n",
            "[20/100] Loss: 0.623806, Acc: 0.801927\n",
            "[20/100] Loss: 0.626235, Acc: 0.800885\n",
            "[20/100] Loss: 0.627329, Acc: 0.801997\n",
            "Finish 20 epoch, Loss: 0.628240, Acc: 0.801323\n",
            "Test Loss: 0.644429, Acc: 0.788814\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 21\n",
            "[21/100] Loss: 0.628332, Acc: 0.799479\n",
            "[21/100] Loss: 0.622029, Acc: 0.802344\n",
            "[21/100] Loss: 0.621621, Acc: 0.802882\n",
            "Finish 21 epoch, Loss: 0.622230, Acc: 0.802722\n",
            "Test Loss: 0.639051, Acc: 0.789013\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 22\n",
            "[22/100] Loss: 0.616530, Acc: 0.803802\n",
            "[22/100] Loss: 0.613947, Acc: 0.805156\n",
            "[22/100] Loss: 0.615128, Acc: 0.804358\n",
            "Finish 22 epoch, Loss: 0.616554, Acc: 0.804188\n",
            "Test Loss: 0.634054, Acc: 0.791401\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 23\n",
            "[23/100] Loss: 0.620763, Acc: 0.802031\n",
            "[23/100] Loss: 0.611978, Acc: 0.806250\n",
            "[23/100] Loss: 0.610913, Acc: 0.805747\n",
            "Finish 23 epoch, Loss: 0.611445, Acc: 0.805637\n",
            "Test Loss: 0.629222, Acc: 0.792795\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 24\n",
            "[24/100] Loss: 0.605751, Acc: 0.805521\n",
            "[24/100] Loss: 0.606578, Acc: 0.805313\n",
            "[24/100] Loss: 0.606892, Acc: 0.806198\n",
            "Finish 24 epoch, Loss: 0.606675, Acc: 0.806603\n",
            "Test Loss: 0.624918, Acc: 0.793292\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 25\n",
            "[25/100] Loss: 0.611710, Acc: 0.803490\n",
            "[25/100] Loss: 0.602921, Acc: 0.807474\n",
            "[25/100] Loss: 0.602137, Acc: 0.808073\n",
            "Finish 25 epoch, Loss: 0.602116, Acc: 0.808169\n",
            "Test Loss: 0.621025, Acc: 0.793989\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 26\n",
            "[26/100] Loss: 0.598875, Acc: 0.808958\n",
            "[26/100] Loss: 0.593707, Acc: 0.811719\n",
            "[26/100] Loss: 0.597183, Acc: 0.809288\n",
            "Finish 26 epoch, Loss: 0.597839, Acc: 0.809168\n",
            "Test Loss: 0.616609, Acc: 0.795880\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 27\n",
            "[27/100] Loss: 0.595012, Acc: 0.810417\n",
            "[27/100] Loss: 0.595419, Acc: 0.810391\n",
            "[27/100] Loss: 0.593817, Acc: 0.810017\n",
            "Finish 27 epoch, Loss: 0.593731, Acc: 0.810018\n",
            "Test Loss: 0.612521, Acc: 0.798766\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 28\n",
            "[28/100] Loss: 0.593868, Acc: 0.810104\n",
            "[28/100] Loss: 0.591396, Acc: 0.810703\n",
            "[28/100] Loss: 0.591186, Acc: 0.810642\n",
            "Finish 28 epoch, Loss: 0.589728, Acc: 0.811317\n",
            "Test Loss: 0.608982, Acc: 0.798666\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 29\n",
            "[29/100] Loss: 0.578638, Acc: 0.815729\n",
            "[29/100] Loss: 0.586062, Acc: 0.812161\n",
            "[29/100] Loss: 0.584963, Acc: 0.812674\n",
            "Finish 29 epoch, Loss: 0.586156, Acc: 0.812200\n",
            "Test Loss: 0.605853, Acc: 0.801254\n",
            "Time:7.2 s\n",
            "**********\n",
            "epoch 30\n",
            "[30/100] Loss: 0.583148, Acc: 0.815313\n",
            "[30/100] Loss: 0.583192, Acc: 0.812891\n",
            "[30/100] Loss: 0.580977, Acc: 0.813316\n",
            "Finish 30 epoch, Loss: 0.582461, Acc: 0.812783\n",
            "Test Loss: 0.602552, Acc: 0.801851\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 31\n",
            "[31/100] Loss: 0.577220, Acc: 0.815885\n",
            "[31/100] Loss: 0.575304, Acc: 0.815286\n",
            "[31/100] Loss: 0.578923, Acc: 0.814201\n",
            "Finish 31 epoch, Loss: 0.579343, Acc: 0.813849\n",
            "Test Loss: 0.599506, Acc: 0.801851\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 32\n",
            "[32/100] Loss: 0.571377, Acc: 0.817760\n",
            "[32/100] Loss: 0.574417, Acc: 0.815130\n",
            "[32/100] Loss: 0.575092, Acc: 0.814757\n",
            "Finish 32 epoch, Loss: 0.576041, Acc: 0.814582\n",
            "Test Loss: 0.596415, Acc: 0.802647\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 33\n",
            "[33/100] Loss: 0.582229, Acc: 0.812708\n",
            "[33/100] Loss: 0.573322, Acc: 0.815417\n",
            "[33/100] Loss: 0.573647, Acc: 0.815382\n",
            "Finish 33 epoch, Loss: 0.572983, Acc: 0.815648\n",
            "Test Loss: 0.593506, Acc: 0.803443\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 34\n",
            "[34/100] Loss: 0.562237, Acc: 0.820312\n",
            "[34/100] Loss: 0.566131, Acc: 0.817813\n",
            "[34/100] Loss: 0.569717, Acc: 0.816927\n",
            "Finish 34 epoch, Loss: 0.570026, Acc: 0.816531\n",
            "Test Loss: 0.590913, Acc: 0.804638\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 35\n",
            "[35/100] Loss: 0.575042, Acc: 0.815313\n",
            "[35/100] Loss: 0.570293, Acc: 0.816042\n",
            "[35/100] Loss: 0.567621, Acc: 0.817361\n",
            "Finish 35 epoch, Loss: 0.567418, Acc: 0.817247\n",
            "Test Loss: 0.588225, Acc: 0.804936\n",
            "Time:7.2 s\n",
            "**********\n",
            "epoch 36\n",
            "[36/100] Loss: 0.567442, Acc: 0.821042\n",
            "[36/100] Loss: 0.566465, Acc: 0.818516\n",
            "[36/100] Loss: 0.564977, Acc: 0.817708\n",
            "Finish 36 epoch, Loss: 0.564563, Acc: 0.817797\n",
            "Test Loss: 0.585860, Acc: 0.805334\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 37\n",
            "[37/100] Loss: 0.557051, Acc: 0.819010\n",
            "[37/100] Loss: 0.556953, Acc: 0.820495\n",
            "[37/100] Loss: 0.560816, Acc: 0.818715\n",
            "Finish 37 epoch, Loss: 0.562053, Acc: 0.818280\n",
            "Test Loss: 0.583390, Acc: 0.806330\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 38\n",
            "[38/100] Loss: 0.558264, Acc: 0.818490\n",
            "[38/100] Loss: 0.557133, Acc: 0.819427\n",
            "[38/100] Loss: 0.558905, Acc: 0.819184\n",
            "Finish 38 epoch, Loss: 0.559573, Acc: 0.819030\n",
            "Test Loss: 0.580988, Acc: 0.806927\n",
            "Time:7.2 s\n",
            "**********\n",
            "epoch 39\n",
            "[39/100] Loss: 0.559416, Acc: 0.819740\n",
            "[39/100] Loss: 0.556835, Acc: 0.820234\n",
            "[39/100] Loss: 0.557830, Acc: 0.819253\n",
            "Finish 39 epoch, Loss: 0.557139, Acc: 0.819396\n",
            "Test Loss: 0.578823, Acc: 0.807026\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 40\n",
            "[40/100] Loss: 0.554146, Acc: 0.821354\n",
            "[40/100] Loss: 0.550699, Acc: 0.822031\n",
            "[40/100] Loss: 0.554268, Acc: 0.820469\n",
            "Finish 40 epoch, Loss: 0.554784, Acc: 0.820296\n",
            "Test Loss: 0.576799, Acc: 0.808420\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 41\n",
            "[41/100] Loss: 0.551832, Acc: 0.820625\n",
            "[41/100] Loss: 0.551138, Acc: 0.820130\n",
            "[41/100] Loss: 0.551380, Acc: 0.821389\n",
            "Finish 41 epoch, Loss: 0.552585, Acc: 0.820679\n",
            "Test Loss: 0.574668, Acc: 0.809514\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 42\n",
            "[42/100] Loss: 0.548887, Acc: 0.819583\n",
            "[42/100] Loss: 0.545832, Acc: 0.823359\n",
            "[42/100] Loss: 0.549838, Acc: 0.821754\n",
            "Finish 42 epoch, Loss: 0.550534, Acc: 0.821312\n",
            "Test Loss: 0.572528, Acc: 0.809415\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 43\n",
            "[43/100] Loss: 0.551735, Acc: 0.821615\n",
            "[43/100] Loss: 0.545728, Acc: 0.823125\n",
            "[43/100] Loss: 0.547527, Acc: 0.822101\n",
            "Finish 43 epoch, Loss: 0.548253, Acc: 0.822062\n",
            "Test Loss: 0.570821, Acc: 0.809713\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 44\n",
            "[44/100] Loss: 0.550533, Acc: 0.819687\n",
            "[44/100] Loss: 0.548238, Acc: 0.822005\n",
            "[44/100] Loss: 0.545869, Acc: 0.822760\n",
            "Finish 44 epoch, Loss: 0.546260, Acc: 0.822478\n",
            "Test Loss: 0.569231, Acc: 0.810012\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 45\n",
            "[45/100] Loss: 0.538454, Acc: 0.824375\n",
            "[45/100] Loss: 0.544881, Acc: 0.821510\n",
            "[45/100] Loss: 0.545120, Acc: 0.822326\n",
            "Finish 45 epoch, Loss: 0.544277, Acc: 0.822994\n",
            "Test Loss: 0.567021, Acc: 0.810609\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 46\n",
            "[46/100] Loss: 0.545345, Acc: 0.821979\n",
            "[46/100] Loss: 0.541643, Acc: 0.822604\n",
            "[46/100] Loss: 0.542850, Acc: 0.823021\n",
            "Finish 46 epoch, Loss: 0.542678, Acc: 0.823244\n",
            "Test Loss: 0.565277, Acc: 0.810709\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 47\n",
            "[47/100] Loss: 0.541224, Acc: 0.821979\n",
            "[47/100] Loss: 0.544962, Acc: 0.821302\n",
            "[47/100] Loss: 0.540357, Acc: 0.823958\n",
            "Finish 47 epoch, Loss: 0.540804, Acc: 0.823661\n",
            "Test Loss: 0.563691, Acc: 0.811604\n",
            "Time:7.2 s\n",
            "**********\n",
            "epoch 48\n",
            "[48/100] Loss: 0.541012, Acc: 0.824583\n",
            "[48/100] Loss: 0.536353, Acc: 0.827109\n",
            "[48/100] Loss: 0.539361, Acc: 0.824844\n",
            "Finish 48 epoch, Loss: 0.538761, Acc: 0.824727\n",
            "Test Loss: 0.562084, Acc: 0.813197\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 49\n",
            "[49/100] Loss: 0.540994, Acc: 0.824635\n",
            "[49/100] Loss: 0.538431, Acc: 0.825417\n",
            "[49/100] Loss: 0.538058, Acc: 0.824566\n",
            "Finish 49 epoch, Loss: 0.537125, Acc: 0.825327\n",
            "Test Loss: 0.560431, Acc: 0.812699\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 50\n",
            "[50/100] Loss: 0.535519, Acc: 0.824740\n",
            "[50/100] Loss: 0.534906, Acc: 0.825078\n",
            "[50/100] Loss: 0.534711, Acc: 0.825312\n",
            "Finish 50 epoch, Loss: 0.535397, Acc: 0.825493\n",
            "Test Loss: 0.558985, Acc: 0.813396\n",
            "Time:7.1 s\n",
            "**********\n",
            "epoch 51\n",
            "[51/100] Loss: 0.536834, Acc: 0.825104\n",
            "[51/100] Loss: 0.532521, Acc: 0.827500\n",
            "[51/100] Loss: 0.533852, Acc: 0.825990\n",
            "Finish 51 epoch, Loss: 0.533826, Acc: 0.825726\n",
            "Test Loss: 0.557355, Acc: 0.814092\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 52\n",
            "[52/100] Loss: 0.535098, Acc: 0.823594\n",
            "[52/100] Loss: 0.531706, Acc: 0.826875\n",
            "[52/100] Loss: 0.531921, Acc: 0.826684\n",
            "Finish 52 epoch, Loss: 0.532354, Acc: 0.826326\n",
            "Test Loss: 0.556030, Acc: 0.813794\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 53\n",
            "[53/100] Loss: 0.526600, Acc: 0.830052\n",
            "[53/100] Loss: 0.528137, Acc: 0.828021\n",
            "[53/100] Loss: 0.531619, Acc: 0.825972\n",
            "Finish 53 epoch, Loss: 0.530776, Acc: 0.826676\n",
            "Test Loss: 0.554581, Acc: 0.814192\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 54\n",
            "[54/100] Loss: 0.523133, Acc: 0.826979\n",
            "[54/100] Loss: 0.526167, Acc: 0.827526\n",
            "[54/100] Loss: 0.530362, Acc: 0.826615\n",
            "Finish 54 epoch, Loss: 0.529421, Acc: 0.827159\n",
            "Test Loss: 0.553107, Acc: 0.815585\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 55\n",
            "[55/100] Loss: 0.534729, Acc: 0.826250\n",
            "[55/100] Loss: 0.529436, Acc: 0.828255\n",
            "[55/100] Loss: 0.527154, Acc: 0.827587\n",
            "Finish 55 epoch, Loss: 0.527693, Acc: 0.827342\n",
            "Test Loss: 0.552435, Acc: 0.814192\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 56\n",
            "[56/100] Loss: 0.525154, Acc: 0.828594\n",
            "[56/100] Loss: 0.526598, Acc: 0.828568\n",
            "[56/100] Loss: 0.526103, Acc: 0.827674\n",
            "Finish 56 epoch, Loss: 0.526449, Acc: 0.827675\n",
            "Test Loss: 0.550493, Acc: 0.814789\n",
            "Time:7.1 s\n",
            "**********\n",
            "epoch 57\n",
            "[57/100] Loss: 0.523205, Acc: 0.829896\n",
            "[57/100] Loss: 0.525077, Acc: 0.829063\n",
            "[57/100] Loss: 0.524244, Acc: 0.828681\n",
            "Finish 57 epoch, Loss: 0.524971, Acc: 0.828258\n",
            "Test Loss: 0.549287, Acc: 0.815187\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 58\n",
            "[58/100] Loss: 0.518810, Acc: 0.829271\n",
            "[58/100] Loss: 0.520883, Acc: 0.830078\n",
            "[58/100] Loss: 0.522866, Acc: 0.829045\n",
            "Finish 58 epoch, Loss: 0.523508, Acc: 0.828791\n",
            "Test Loss: 0.548053, Acc: 0.815088\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 59\n",
            "[59/100] Loss: 0.518844, Acc: 0.830417\n",
            "[59/100] Loss: 0.521469, Acc: 0.828203\n",
            "[59/100] Loss: 0.522656, Acc: 0.828872\n",
            "Finish 59 epoch, Loss: 0.522235, Acc: 0.829058\n",
            "Test Loss: 0.546861, Acc: 0.815685\n",
            "Time:7.2 s\n",
            "**********\n",
            "epoch 60\n",
            "[60/100] Loss: 0.516543, Acc: 0.831979\n",
            "[60/100] Loss: 0.520223, Acc: 0.829531\n",
            "[60/100] Loss: 0.521677, Acc: 0.828733\n",
            "Finish 60 epoch, Loss: 0.520922, Acc: 0.829291\n",
            "Test Loss: 0.545667, Acc: 0.816282\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 61\n",
            "[61/100] Loss: 0.516245, Acc: 0.832031\n",
            "[61/100] Loss: 0.516965, Acc: 0.831016\n",
            "[61/100] Loss: 0.519151, Acc: 0.829253\n",
            "Finish 61 epoch, Loss: 0.519795, Acc: 0.829141\n",
            "Test Loss: 0.544640, Acc: 0.816481\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 62\n",
            "[62/100] Loss: 0.519756, Acc: 0.828542\n",
            "[62/100] Loss: 0.517847, Acc: 0.830443\n",
            "[62/100] Loss: 0.517634, Acc: 0.830312\n",
            "Finish 62 epoch, Loss: 0.518574, Acc: 0.829691\n",
            "Test Loss: 0.543980, Acc: 0.816381\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 63\n",
            "[63/100] Loss: 0.513623, Acc: 0.830990\n",
            "[63/100] Loss: 0.515194, Acc: 0.830469\n",
            "[63/100] Loss: 0.517832, Acc: 0.830017\n",
            "Finish 63 epoch, Loss: 0.517276, Acc: 0.830107\n",
            "Test Loss: 0.542468, Acc: 0.817377\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 64\n",
            "[64/100] Loss: 0.513847, Acc: 0.829687\n",
            "[64/100] Loss: 0.513190, Acc: 0.830833\n",
            "[64/100] Loss: 0.515263, Acc: 0.830174\n",
            "Finish 64 epoch, Loss: 0.516054, Acc: 0.829757\n",
            "Test Loss: 0.541317, Acc: 0.818073\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 65\n",
            "[65/100] Loss: 0.507978, Acc: 0.835000\n",
            "[65/100] Loss: 0.512877, Acc: 0.832552\n",
            "[65/100] Loss: 0.514983, Acc: 0.830764\n",
            "Finish 65 epoch, Loss: 0.514937, Acc: 0.830674\n",
            "Test Loss: 0.540522, Acc: 0.817377\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 66\n",
            "[66/100] Loss: 0.517258, Acc: 0.828177\n",
            "[66/100] Loss: 0.514918, Acc: 0.828932\n",
            "[66/100] Loss: 0.512966, Acc: 0.831389\n",
            "Finish 66 epoch, Loss: 0.513962, Acc: 0.830957\n",
            "Test Loss: 0.539258, Acc: 0.818571\n",
            "Time:7.6 s\n",
            "**********\n",
            "epoch 67\n",
            "[67/100] Loss: 0.511475, Acc: 0.830052\n",
            "[67/100] Loss: 0.514720, Acc: 0.829531\n",
            "[67/100] Loss: 0.512507, Acc: 0.831458\n",
            "Finish 67 epoch, Loss: 0.512755, Acc: 0.831240\n",
            "Test Loss: 0.538278, Acc: 0.818571\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 68\n",
            "[68/100] Loss: 0.514376, Acc: 0.830521\n",
            "[68/100] Loss: 0.515131, Acc: 0.830703\n",
            "[68/100] Loss: 0.510928, Acc: 0.831719\n",
            "Finish 68 epoch, Loss: 0.511711, Acc: 0.831540\n",
            "Test Loss: 0.537208, Acc: 0.819268\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 69\n",
            "[69/100] Loss: 0.512988, Acc: 0.827448\n",
            "[69/100] Loss: 0.514273, Acc: 0.830417\n",
            "[69/100] Loss: 0.510314, Acc: 0.831892\n",
            "Finish 69 epoch, Loss: 0.510652, Acc: 0.831856\n",
            "Test Loss: 0.536318, Acc: 0.819765\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 70\n",
            "[70/100] Loss: 0.504038, Acc: 0.836719\n",
            "[70/100] Loss: 0.509712, Acc: 0.833047\n",
            "[70/100] Loss: 0.509449, Acc: 0.832066\n",
            "Finish 70 epoch, Loss: 0.509646, Acc: 0.831706\n",
            "Test Loss: 0.535448, Acc: 0.819666\n",
            "Time:7.4 s\n",
            "**********\n",
            "epoch 71\n",
            "[71/100] Loss: 0.506630, Acc: 0.834531\n",
            "[71/100] Loss: 0.508072, Acc: 0.832943\n",
            "[71/100] Loss: 0.507948, Acc: 0.832535\n",
            "Finish 71 epoch, Loss: 0.508607, Acc: 0.832323\n",
            "Test Loss: 0.534462, Acc: 0.820064\n",
            "Time:7.3 s\n",
            "**********\n",
            "epoch 72\n",
            "[72/100] Loss: 0.516164, Acc: 0.827656\n",
            "[72/100] Loss: 0.512256, Acc: 0.830547\n",
            "[72/100] Loss: 0.508718, Acc: 0.831597\n",
            "Finish 72 epoch, Loss: 0.507711, Acc: 0.831973\n",
            "Test Loss: 0.533641, Acc: 0.821059\n",
            "Time:8.9 s\n",
            "**********\n",
            "epoch 73\n",
            "[73/100] Loss: 0.509322, Acc: 0.830365\n",
            "[73/100] Loss: 0.507639, Acc: 0.832318\n",
            "[73/100] Loss: 0.506645, Acc: 0.832483\n",
            "Finish 73 epoch, Loss: 0.506655, Acc: 0.832406\n",
            "Test Loss: 0.532597, Acc: 0.821258\n",
            "Time:8.0 s\n",
            "**********\n",
            "epoch 74\n",
            "[74/100] Loss: 0.501974, Acc: 0.835104\n",
            "[74/100] Loss: 0.503351, Acc: 0.834948\n",
            "[74/100] Loss: 0.505506, Acc: 0.833090\n",
            "Finish 74 epoch, Loss: 0.505656, Acc: 0.833056\n",
            "Test Loss: 0.531722, Acc: 0.821457\n",
            "Time:8.0 s\n",
            "**********\n",
            "epoch 75\n",
            "[75/100] Loss: 0.498861, Acc: 0.835365\n",
            "[75/100] Loss: 0.505073, Acc: 0.832214\n",
            "[75/100] Loss: 0.505230, Acc: 0.832691\n",
            "Finish 75 epoch, Loss: 0.504767, Acc: 0.833172\n",
            "Test Loss: 0.531104, Acc: 0.820561\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 76\n",
            "[76/100] Loss: 0.509244, Acc: 0.831563\n",
            "[76/100] Loss: 0.505177, Acc: 0.833047\n",
            "[76/100] Loss: 0.504383, Acc: 0.833420\n",
            "Finish 76 epoch, Loss: 0.503869, Acc: 0.833439\n",
            "Test Loss: 0.530048, Acc: 0.821557\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 77\n",
            "[77/100] Loss: 0.502271, Acc: 0.831458\n",
            "[77/100] Loss: 0.503478, Acc: 0.832109\n",
            "[77/100] Loss: 0.503029, Acc: 0.833507\n",
            "Finish 77 epoch, Loss: 0.503129, Acc: 0.833439\n",
            "Test Loss: 0.529220, Acc: 0.821955\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 78\n",
            "[78/100] Loss: 0.503530, Acc: 0.835885\n",
            "[78/100] Loss: 0.501236, Acc: 0.835990\n",
            "[78/100] Loss: 0.502516, Acc: 0.833889\n",
            "Finish 78 epoch, Loss: 0.502067, Acc: 0.834072\n",
            "Test Loss: 0.528496, Acc: 0.822353\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 79\n",
            "[79/100] Loss: 0.496908, Acc: 0.835104\n",
            "[79/100] Loss: 0.500882, Acc: 0.833932\n",
            "[79/100] Loss: 0.501552, Acc: 0.834201\n",
            "Finish 79 epoch, Loss: 0.501189, Acc: 0.834388\n",
            "Test Loss: 0.527692, Acc: 0.823149\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 80\n",
            "[80/100] Loss: 0.503455, Acc: 0.832708\n",
            "[80/100] Loss: 0.501999, Acc: 0.833828\n",
            "[80/100] Loss: 0.501798, Acc: 0.833646\n",
            "Finish 80 epoch, Loss: 0.500279, Acc: 0.834222\n",
            "Test Loss: 0.526910, Acc: 0.822552\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 81\n",
            "[81/100] Loss: 0.506596, Acc: 0.833438\n",
            "[81/100] Loss: 0.502690, Acc: 0.833438\n",
            "[81/100] Loss: 0.499132, Acc: 0.835069\n",
            "Finish 81 epoch, Loss: 0.499511, Acc: 0.834821\n",
            "Test Loss: 0.526123, Acc: 0.823149\n",
            "Time:8.0 s\n",
            "**********\n",
            "epoch 82\n",
            "[82/100] Loss: 0.489102, Acc: 0.839583\n",
            "[82/100] Loss: 0.492971, Acc: 0.837865\n",
            "[82/100] Loss: 0.497891, Acc: 0.835330\n",
            "Finish 82 epoch, Loss: 0.498754, Acc: 0.834638\n",
            "Test Loss: 0.525504, Acc: 0.822751\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 83\n",
            "[83/100] Loss: 0.499424, Acc: 0.835990\n",
            "[83/100] Loss: 0.499343, Acc: 0.835182\n",
            "[83/100] Loss: 0.497642, Acc: 0.835156\n",
            "Finish 83 epoch, Loss: 0.497804, Acc: 0.835088\n",
            "Test Loss: 0.524887, Acc: 0.822552\n",
            "Time:8.0 s\n",
            "**********\n",
            "epoch 84\n",
            "[84/100] Loss: 0.496633, Acc: 0.835521\n",
            "[84/100] Loss: 0.494946, Acc: 0.835964\n",
            "[84/100] Loss: 0.495322, Acc: 0.835955\n",
            "Finish 84 epoch, Loss: 0.497144, Acc: 0.835438\n",
            "Test Loss: 0.524076, Acc: 0.823149\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 85\n",
            "[85/100] Loss: 0.496202, Acc: 0.836094\n",
            "[85/100] Loss: 0.498550, Acc: 0.834948\n",
            "[85/100] Loss: 0.496086, Acc: 0.835069\n",
            "Finish 85 epoch, Loss: 0.496384, Acc: 0.835088\n",
            "Test Loss: 0.523429, Acc: 0.824144\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 86\n",
            "[86/100] Loss: 0.489488, Acc: 0.837969\n",
            "[86/100] Loss: 0.496867, Acc: 0.834714\n",
            "[86/100] Loss: 0.496531, Acc: 0.834965\n",
            "Finish 86 epoch, Loss: 0.495514, Acc: 0.835438\n",
            "Test Loss: 0.522536, Acc: 0.824343\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 87\n",
            "[87/100] Loss: 0.498189, Acc: 0.835521\n",
            "[87/100] Loss: 0.492571, Acc: 0.835729\n",
            "[87/100] Loss: 0.495736, Acc: 0.835069\n",
            "Finish 87 epoch, Loss: 0.494730, Acc: 0.835504\n",
            "Test Loss: 0.521916, Acc: 0.824045\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 88\n",
            "[88/100] Loss: 0.492026, Acc: 0.837396\n",
            "[88/100] Loss: 0.494311, Acc: 0.836120\n",
            "[88/100] Loss: 0.493567, Acc: 0.836615\n",
            "Finish 88 epoch, Loss: 0.494113, Acc: 0.836054\n",
            "Test Loss: 0.521263, Acc: 0.823646\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 89\n",
            "[89/100] Loss: 0.491299, Acc: 0.837344\n",
            "[89/100] Loss: 0.492222, Acc: 0.837135\n",
            "[89/100] Loss: 0.493457, Acc: 0.835990\n",
            "Finish 89 epoch, Loss: 0.493317, Acc: 0.835904\n",
            "Test Loss: 0.520636, Acc: 0.824045\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 90\n",
            "[90/100] Loss: 0.498093, Acc: 0.835365\n",
            "[90/100] Loss: 0.494955, Acc: 0.835755\n",
            "[90/100] Loss: 0.492263, Acc: 0.836823\n",
            "Finish 90 epoch, Loss: 0.492512, Acc: 0.836637\n",
            "Test Loss: 0.520108, Acc: 0.824244\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 91\n",
            "[91/100] Loss: 0.497174, Acc: 0.834896\n",
            "[91/100] Loss: 0.493984, Acc: 0.836693\n",
            "[91/100] Loss: 0.492024, Acc: 0.836736\n",
            "Finish 91 epoch, Loss: 0.491910, Acc: 0.836837\n",
            "Test Loss: 0.519389, Acc: 0.824244\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 92\n",
            "[92/100] Loss: 0.489696, Acc: 0.839427\n",
            "[92/100] Loss: 0.485880, Acc: 0.838620\n",
            "[92/100] Loss: 0.490650, Acc: 0.836701\n",
            "Finish 92 epoch, Loss: 0.491139, Acc: 0.836820\n",
            "Test Loss: 0.518715, Acc: 0.824443\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 93\n",
            "[93/100] Loss: 0.482754, Acc: 0.840729\n",
            "[93/100] Loss: 0.492024, Acc: 0.837396\n",
            "[93/100] Loss: 0.490166, Acc: 0.836910\n",
            "Finish 93 epoch, Loss: 0.490593, Acc: 0.836970\n",
            "Test Loss: 0.518505, Acc: 0.824443\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 94\n",
            "[94/100] Loss: 0.486692, Acc: 0.837396\n",
            "[94/100] Loss: 0.489669, Acc: 0.838646\n",
            "[94/100] Loss: 0.490300, Acc: 0.837569\n",
            "Finish 94 epoch, Loss: 0.489811, Acc: 0.837753\n",
            "Test Loss: 0.517578, Acc: 0.825139\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 95\n",
            "[95/100] Loss: 0.491952, Acc: 0.837708\n",
            "[95/100] Loss: 0.485402, Acc: 0.838411\n",
            "[95/100] Loss: 0.488538, Acc: 0.837812\n",
            "Finish 95 epoch, Loss: 0.489274, Acc: 0.837437\n",
            "Test Loss: 0.516873, Acc: 0.825040\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 96\n",
            "[96/100] Loss: 0.484646, Acc: 0.835521\n",
            "[96/100] Loss: 0.493521, Acc: 0.834557\n",
            "[96/100] Loss: 0.488786, Acc: 0.837274\n",
            "Finish 96 epoch, Loss: 0.488456, Acc: 0.837403\n",
            "Test Loss: 0.516305, Acc: 0.825438\n",
            "Time:7.7 s\n",
            "**********\n",
            "epoch 97\n",
            "[97/100] Loss: 0.489048, Acc: 0.838802\n",
            "[97/100] Loss: 0.489000, Acc: 0.837786\n",
            "[97/100] Loss: 0.488884, Acc: 0.837222\n",
            "Finish 97 epoch, Loss: 0.487793, Acc: 0.837720\n",
            "Test Loss: 0.515777, Acc: 0.824741\n",
            "Time:7.9 s\n",
            "**********\n",
            "epoch 98\n",
            "[98/100] Loss: 0.489632, Acc: 0.838229\n",
            "[98/100] Loss: 0.488741, Acc: 0.837682\n",
            "[98/100] Loss: 0.487821, Acc: 0.837535\n",
            "Finish 98 epoch, Loss: 0.487348, Acc: 0.837803\n",
            "Test Loss: 0.515208, Acc: 0.825736\n",
            "Time:7.8 s\n",
            "**********\n",
            "epoch 99\n",
            "[99/100] Loss: 0.484030, Acc: 0.838854\n",
            "[99/100] Loss: 0.486229, Acc: 0.837578\n",
            "[99/100] Loss: 0.486527, Acc: 0.837465\n",
            "Finish 99 epoch, Loss: 0.486657, Acc: 0.837703\n",
            "Test Loss: 0.514613, Acc: 0.826234\n",
            "Time:7.5 s\n",
            "**********\n",
            "epoch 100\n",
            "[100/100] Loss: 0.492047, Acc: 0.833438\n",
            "[100/100] Loss: 0.486452, Acc: 0.837656\n",
            "[100/100] Loss: 0.484561, Acc: 0.838385\n",
            "Finish 100 epoch, Loss: 0.485988, Acc: 0.837970\n",
            "Test Loss: 0.514090, Acc: 0.826135\n",
            "Time:8.0 s\n"
          ]
        }
      ],
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 定义超参数\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 100\n",
        "\n",
        "# 下载训练集 MNIST 手写数字训练集\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root='../datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root='../datasets', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 定义 Logistic Regression 模型\n",
        "class Logistic_Regression(nn.Module):\n",
        "    def __init__(self, in_dim, n_class):\n",
        "        super().__init__()\n",
        "        #super(Logistic_Regression, self).__init__()\n",
        "        self.logistic = nn.Linear(in_dim, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.logistic(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = Logistic_Regression(28 * 28, 10)  # 图片大小是 28x28\n",
        "use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "# 定义loss和optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 开始训练\n",
        "for epoch in range(num_epochs):\n",
        "    print('*' * 10)\n",
        "    print(f'epoch {epoch+1}')\n",
        "    since = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, data in enumerate(train_loader, 1):\n",
        "        img, label = data\n",
        "        img = img.view(img.size(0), -1)  # 将图片展开成 28x28\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        # 向前传播\n",
        "        out = model(img)\n",
        "        loss = criterion(out, label)\n",
        "        running_loss += loss.item()\n",
        "        _, pred = torch.max(out, 1)\n",
        "        running_acc += (pred==label).float().mean()\n",
        "        # 向后传播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
        "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/i:.6f}, Acc: {running_acc/i:.6f}')\n",
        "    model.eval()\n",
        "    eval_loss = 0.\n",
        "    eval_acc = 0.\n",
        "    for data in test_loader:\n",
        "        img, label = data\n",
        "        img = img.view(img.size(0), -1)\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        with torch.no_grad():\n",
        "            out = model(img)\n",
        "            loss = criterion(out, label)\n",
        "        eval_loss += loss.item()\n",
        "        _, pred = torch.max(out, 1)\n",
        "        eval_acc += (pred == label).float().mean()\n",
        "    print(f'Test Loss: {eval_loss/len(test_loader):.6f}, Acc: {eval_acc/len(test_loader):.6f}')\n",
        "    print(f'Time:{(time.time()-since):.1f} s')\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), './logstic.pth')"
      ]
    }
  ]
}