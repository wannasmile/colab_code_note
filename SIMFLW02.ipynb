{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfuJeV2BnVmChdoY7by6n7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/SIMFLW02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UN0oXYDOxdoE"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "''' Operation classes in computational graph.\n",
        "'''\n",
        "from queue import Queue\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Operation(object):\n",
        "    ''' Base class for all operations in simpleflow.\n",
        "    An operation is a node in computational graph receiving zero or more nodes\n",
        "    as input and produce zero or more nodes as output. Vertices could be an\n",
        "    operation, variable or placeholder.\n",
        "    '''\n",
        "    def __init__(self, *input_nodes, name=None):\n",
        "        ''' Operation constructor.\n",
        "        :param input_nodes: Input nodes for the operation node.\n",
        "        :type input_nodes: Objects of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        # Nodes received by this operation.\n",
        "        self.input_nodes = input_nodes\n",
        "\n",
        "        # Nodes that receive this operation node as input.\n",
        "        self.output_nodes = []\n",
        "\n",
        "        # Output value of this operation in session execution.\n",
        "        self.output_value = None\n",
        "\n",
        "        # Operation name.\n",
        "        self.name = name\n",
        "\n",
        "        # Graph the operation belongs to.\n",
        "        self.graph = DEFAULT_GRAPH\n",
        "\n",
        "        # Add this operation node to destination lists in its input nodes.\n",
        "        for node in input_nodes:\n",
        "            node.output_nodes.append(self)\n",
        "\n",
        "        # Add this operation to default graph.\n",
        "        self.graph.operations.append(self)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the output value of the operation.\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute and return the gradient of the operation wrt inputs.\n",
        "        '''\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Add(self, other)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Add(self, Negative(other))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Addition operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Add(Operation):\n",
        "    ''' An addition operation.\n",
        "    '''\n",
        "    def __init__(self, x, y, name=None):\n",
        "        ''' Addition constructor.\n",
        "        :param x: The first input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param y: The second input node.\n",
        "        :type y: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, y, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of addition operation.\n",
        "        '''\n",
        "        x, y = self.input_nodes\n",
        "        self.output_value = np.add(x.output_value, y.output_value)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradients for this operation wrt input values.\n",
        "        :param grad: The gradient of other operation wrt the addition output.\n",
        "        :type grad: number or a ndarray, default value is 1.0.\n",
        "        '''\n",
        "        x, y = [node.output_value for node in self.input_nodes]\n",
        "\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "\n",
        "        grad_wrt_x = grad\n",
        "        while np.ndim(grad_wrt_x) > len(np.shape(x)):\n",
        "            grad_wrt_x = np.sum(grad_wrt_x, axis=0)\n",
        "        for axis, size in enumerate(np.shape(x)):\n",
        "            if size == 1:\n",
        "                grad_wrt_x = np.sum(grad_wrt_x, axis=axis, keepdims=True)\n",
        "\n",
        "        grad_wrt_y = grad\n",
        "        while np.ndim(grad_wrt_y) > len(np.shape(y)):\n",
        "            grad_wrt_y = np.sum(grad_wrt_y, axis=0)\n",
        "        for axis, size in enumerate(np.shape(y)):\n",
        "            if size == 1:\n",
        "                grad_wrt_y = np.sum(grad_wrt_y, axis=axis, keepdims=True)\n",
        "\n",
        "        return [grad_wrt_x, grad_wrt_y]\n",
        "\n",
        "def add(x, y, name=None):\n",
        "    ''' Returns x + y element-wise.\n",
        "    '''\n",
        "    return Add(x, y, name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Multiplication operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Multiply(Operation):\n",
        "    ''' Multiplication operation.\n",
        "    '''\n",
        "    def __init__(self, x, y, name=None):\n",
        "        ''' Multiplication constructor.\n",
        "        :param x: The first input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param y: The second input node.\n",
        "        :type y: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, y, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the multiplication operation result.\n",
        "        '''\n",
        "        x, y = self.input_nodes\n",
        "        self.output_value = np.multiply(x.output_value, y.output_value)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute and return gradients for this operation wrt input values.\n",
        "        :param grad: The gradient of other operation wrt the mutiply output.\n",
        "        :type grad: number or a ndarray.\n",
        "        '''\n",
        "        x, y = [node.output_value for node in self.input_nodes]\n",
        "\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "\n",
        "        grad_wrt_x = grad*y\n",
        "        while np.ndim(grad_wrt_x) > len(np.shape(x)):\n",
        "            grad_wrt_x = np.sum(grad_wrt_x, axis=0)\n",
        "        for axis, size in enumerate(np.shape(x)):\n",
        "            if size == 1:\n",
        "                grad_wrt_x = np.sum(grad_wrt_x, axis=axis, keepdims=True)\n",
        "\n",
        "        grad_wrt_y = grad*x\n",
        "        while np.ndim(grad_wrt_y) > len(np.shape(y)):\n",
        "            grad_wrt_y = np.sum(grad_wrt_y, axis=0)\n",
        "        for axis, size in enumerate(np.shape(y)):\n",
        "            if size == 1:\n",
        "                grad_wrt_y = np.sum(grad_wrt_y, axis=axis, keepdims=True)\n",
        "\n",
        "        return [grad_wrt_x, grad_wrt_y]\n",
        "\n",
        "def multiply(x, y, name=None):\n",
        "    ''' Returns x * y element-wise.\n",
        "    '''\n",
        "    return Multiply(x, y, name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Matrix multiplication operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class MatMul(Operation):\n",
        "    ''' Matrix multiplication operation.\n",
        "    '''\n",
        "    def __init__(self, x, y, name=None):\n",
        "        ''' MatMul constructor.\n",
        "        :param x: The first input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param y: The second input node.\n",
        "        :type y: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, y, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the multiplication operation result.\n",
        "        '''\n",
        "        x, y = self.input_nodes\n",
        "        self.output_value = np.dot(x.output_value, y.output_value)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute and return the gradient for matrix multiplication.\n",
        "        :param grad: The gradient of other operation wrt the matmul output.\n",
        "        :type grad: number or a ndarray, default value is 1.0.\n",
        "        '''\n",
        "        # Get input values.\n",
        "        x, y = [node.output_value for node in self.input_nodes]\n",
        "\n",
        "        # Default gradient wrt the matmul output.\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "\n",
        "        # Gradients wrt inputs.\n",
        "        dfdx = np.dot(grad, np.transpose(y))\n",
        "        dfdy = np.dot(np.transpose(x), grad)\n",
        "\n",
        "        return [dfdx, dfdy]\n",
        "\n",
        "def matmul(x, y, name=None):\n",
        "    ''' Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
        "    '''\n",
        "    return MatMul(x, y, name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Sigmoid operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Sigmoid(Operation):\n",
        "    ''' Sigmoid operation.\n",
        "    '''\n",
        "    def __init__(self, x, name=None):\n",
        "        ''' Sigmoid operation constructor.\n",
        "        :param x: The input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of sigmoid function.\n",
        "        '''\n",
        "        x, = self.input_nodes\n",
        "        self.output_value = 1/(1 + np.exp(-x.output_value))\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradient for sigmoid operation wrt input value.\n",
        "        :param grad: The gradient of other operation wrt the sigmoid output.\n",
        "        :type grad: ndarray.\n",
        "        '''\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "        return grad*self.output_value*(1 - self.output_value)\n",
        "\n",
        "def sigmoid(x, name=None):\n",
        "    ''' Computes sigmoid of `x` element-wise.\n",
        "    '''\n",
        "    return Sigmoid(x, name=name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Logarithm operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Log(Operation):\n",
        "    ''' Natural logarithm operation.\n",
        "    '''\n",
        "    def __init__(self, x, name=None):\n",
        "        ''' Logarithm constructor.\n",
        "        :param x: The input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of sigmoid function.\n",
        "        '''\n",
        "        x, = self.input_nodes\n",
        "        self.output_value = np.log(x.output_value)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradient for natural logarithm operation wrt input value.\n",
        "        :param grad: The gradient of other operation wrt the logarithm output.\n",
        "        :type grad: ndarray.\n",
        "        '''\n",
        "        x = self.input_nodes[0].output_value\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "        return grad*1/x\n",
        "\n",
        "def log(x, name=None):\n",
        "    ''' Computes the natural logarithm of x element-wise.\n",
        "    '''\n",
        "    return Log(x, name=name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Negative operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Negative(Operation):\n",
        "    ''' Negative operation.\n",
        "    '''\n",
        "    def __init__(self, x, name=None):\n",
        "        ''' Operation constructor.\n",
        "        :param x: The input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The operation name.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of sigmoid function.\n",
        "        '''\n",
        "        x, = self.input_nodes\n",
        "        self.output_value = -x.output_value\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradient for negative operation wrt input value.\n",
        "        :param grad: The gradient of other operation wrt the negative output.\n",
        "        :type grad: ndarray.\n",
        "        '''\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "        return -grad\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Reduce sum operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class ReduceSum(Operation):\n",
        "    ''' Reduce sum operation.\n",
        "    '''\n",
        "    def __init__(self, x, axis=None):\n",
        "        ''' Operation constructor.\n",
        "        :param x: The input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param axis: The dimensions to reduce. If `None`, reduces all dimensions.\n",
        "        :type axis: int.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x)\n",
        "        self.axis = axis\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of sigmoid function.\n",
        "        '''\n",
        "        x, = self.input_nodes\n",
        "        self.output_value = np.sum(x.output_value, self.axis)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradient for negative operation wrt input value.\n",
        "        :param grad: The gradient of other operation wrt the negative output.\n",
        "        :type grad: ndarray.\n",
        "        '''\n",
        "        input_value = self.input_nodes[0].output_value\n",
        "\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "\n",
        "        output_shape = np.array(np.shape(input_value))\n",
        "        output_shape[self.axis] = 1.0\n",
        "        tile_scaling = np.shape(input_value) // output_shape\n",
        "        grad = np.reshape(grad, output_shape)\n",
        "        return np.tile(grad, tile_scaling)\n",
        "\n",
        "def reduce_sum(x, axis=None):\n",
        "    ''' Computes the sum of elements across dimensions of a tensor.\n",
        "    '''\n",
        "    return ReduceSum(x, axis=axis)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Square operation\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Square(Operation):\n",
        "    ''' Square operation.\n",
        "    '''\n",
        "    def __init__(self, x, name=None):\n",
        "        ''' Operation constructor.\n",
        "        :param x: The input node.\n",
        "        :type x: Object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param name: The name of the operation.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        super(self.__class__, self).__init__(x, name=name)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the value of square function.\n",
        "        '''\n",
        "        x, = self.input_nodes\n",
        "        self.output_value = np.square(x.output_value)\n",
        "        return self.output_value\n",
        "\n",
        "    def compute_gradient(self, grad=None):\n",
        "        ''' Compute the gradient for square operation wrt input value.\n",
        "        :param grad: The gradient of other operation wrt the square output.\n",
        "        :type grad: ndarray.\n",
        "        '''\n",
        "        input_value = self.input_nodes[0].output_value\n",
        "\n",
        "        if grad is None:\n",
        "            grad = np.ones_like(self.output_value)\n",
        "\n",
        "        return grad*np.multiply(2.0, input_value)\n",
        "\n",
        "def square(x, name=None):\n",
        "    ''' Computes square of x element-wise.\n",
        "    '''\n",
        "    return Square(x, name=name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Constant node\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Constant(object):\n",
        "    ''' Constant node in computational graph.\n",
        "    '''\n",
        "    def __init__(self, value, name=None):\n",
        "        ''' Cosntant constructor.\n",
        "        '''\n",
        "        # Constant value.\n",
        "        self.value = value\n",
        "\n",
        "        # Output value of this operation in session.\n",
        "        self.output_value = None\n",
        "\n",
        "        # Nodes that receive this variable node as input.\n",
        "        self.output_nodes = []\n",
        "\n",
        "        # Operation name.\n",
        "        self.name = name\n",
        "\n",
        "        # Add to graph.\n",
        "        DEFAULT_GRAPH.constants.append(self)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the constant value.\n",
        "        '''\n",
        "        if self.output_value is None:\n",
        "            self.output_value = self.value\n",
        "        return self.output_value\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Add(self, other)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Add(self, Negative(other))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "def constant(value, name=None):\n",
        "    ''' Create a constant node.\n",
        "    '''\n",
        "    return Constant(value, name=name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Variable node\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Variable(object):\n",
        "    ''' Variable node in computational graph.\n",
        "    '''\n",
        "    def __init__(self, initial_value=None, name=None, trainable=True): \n",
        "        ''' Variable constructor.\n",
        "        :param initial_value: The initial value of the variable.\n",
        "        :type initial_value: number or a ndarray.\n",
        "        :param name: Name of the variable.\n",
        "        :type name: str.\n",
        "        '''\n",
        "        # Variable initial value.\n",
        "        self.initial_value = initial_value\n",
        "\n",
        "        # Output value of this operation in session execution.\n",
        "        self.output_value = None\n",
        "\n",
        "        # Nodes that receive this variable node as input.\n",
        "        self.output_nodes = []\n",
        "\n",
        "        # Variable name.\n",
        "        self.name = name\n",
        "\n",
        "        # Graph the variable belongs to.\n",
        "        self.graph = DEFAULT_GRAPH\n",
        "\n",
        "        # Add to the currently active default graph.\n",
        "        self.graph.variables.append(self)\n",
        "        if trainable:\n",
        "            self.graph.trainable_variables.append(self)\n",
        "\n",
        "    def compute_output(self):\n",
        "        ''' Compute and return the variable value.\n",
        "        '''\n",
        "        if self.output_value is None:\n",
        "            self.output_value = self.initial_value\n",
        "        return self.output_value\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Add(self, other)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Add(self, Negative(other))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Placeholder node\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class Placeholder(object):\n",
        "    ''' Placeholder node in computational graph. It has to be provided a value when\n",
        "        when computing the output of a graph.\n",
        "    '''\n",
        "    def __init__(self, name=None):\n",
        "        ''' Placeholdef constructor.\n",
        "        '''\n",
        "        # Output value of this operation in session execution.\n",
        "        self.output_value = None\n",
        "\n",
        "        # Nodes that receive this placeholder node as input.\n",
        "        self.output_nodes = []\n",
        "\n",
        "        # Placeholder node name.\n",
        "        self.name = name\n",
        "\n",
        "        # Graph the placeholder node belongs to.\n",
        "        self.graph = DEFAULT_GRAPH\n",
        "\n",
        "        # Add to the currently active default graph.\n",
        "        self.graph.placeholders.append(self)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return Add(self, other)\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Negative(self)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return Add(self, Negative(other))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return Multiply(self, other)\n",
        "\n",
        "def placeholder(name=None):\n",
        "    ''' Inserts a placeholder for a node that will be always fed.\n",
        "    '''\n",
        "    return Placeholder(name=name)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Function for gradients computation.\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_gradients(target_op):\n",
        "    ''' Backpropagation implementation computing gradient of target operation wrt\n",
        "        all the other connected nodes.\n",
        "    :param target_op: The target operation whose gradient wrt other nodes would\n",
        "                      be computed.\n",
        "    :type target_op: Any operation type.\n",
        "    :return grad_table: A table containing node objects and gradients.\n",
        "    :type grad_table: dict.\n",
        "    '''\n",
        "    # A dict containing a mapping between node and gradient value of target_op wrt the node's output.\n",
        "    # NOTE: It is the gradient wrt the node's OUTPUT NOT input.\n",
        "    grad_table = {}\n",
        "\n",
        "    # The gradient wrt target_op itself is 1.\n",
        "    grad_table[target_op] = np.ones_like(target_op.output_value)\n",
        "\n",
        "    # Perform a breadth-first search staring from the target_op in graph.\n",
        "    # Queue for node traverasl.\n",
        "    queue = Queue()\n",
        "    queue.put(target_op)\n",
        "\n",
        "    # Set for visited nodes.\n",
        "    visited = set()\n",
        "    visited.add(target_op)\n",
        "\n",
        "    while not queue.empty():\n",
        "        node = queue.get()\n",
        "\n",
        "        # Compute gradient wrt the node's output.\n",
        "        if node != target_op:\n",
        "            grads_wrt_node_output = []\n",
        "\n",
        "            for output_node in node.output_nodes:\n",
        "                # Retrieve the gradient wrt output_node's OUTPUT.\n",
        "                grad_wrt_output_node_output = grad_table[output_node]\n",
        "\n",
        "                # Compute the gradient wrt current node's output.\n",
        "                grad_wrt_node_output = output_node.compute_gradient(grad_wrt_output_node_output)\n",
        "                if len(output_node.input_nodes) > 1:\n",
        "                    input_node_index = output_node.input_nodes.index(node)\n",
        "                    grads_wrt_node_output.append(grad_wrt_node_output[input_node_index])\n",
        "                else:\n",
        "                    grads_wrt_node_output.append(grad_wrt_node_output)\n",
        "\n",
        "            # Sum all gradients wrt node's output.\n",
        "            tot_grad_wrt_node_output = sum(grads_wrt_node_output)\n",
        "            grad_table[node] = tot_grad_wrt_node_output\n",
        "\n",
        "        # Put adjecent nodes to queue.\n",
        "        if hasattr(node, 'input_nodes'):\n",
        "            for input_node in node.input_nodes:\n",
        "                if input_node not in visited:\n",
        "                    visited.add(input_node)\n",
        "                    queue.put(input_node)\n",
        "\n",
        "    return grad_table\n",
        "\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "''' Computational graph definition.\n",
        "'''\n",
        "\n",
        "class Graph(object):\n",
        "    ''' Graph containing all computing nodes.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        ''' Graph constructor.\n",
        "        '''\n",
        "        self.operations, self.constants, self.placeholders = [], [], []\n",
        "        self.variables, self.trainable_variables = [], []\n",
        "\n",
        "    def __enter__(self):\n",
        "        ''' Reset default graph.\n",
        "        '''\n",
        "        global DEFAULT_GRAPH\n",
        "        self.old_graph = DEFAULT_GRAPH\n",
        "        DEFAULT_GRAPH = self\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        ''' Recover default graph.\n",
        "        '''\n",
        "        global DEFAULT_GRAPH\n",
        "        DEFAULT_GRAPH = self.old_graph\n",
        "\n",
        "    def as_default(self):\n",
        "        ''' Set this graph as global default graph.\n",
        "        '''\n",
        "        return self\n",
        "\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "''' Session to execute a computational graph.\n",
        "'''\n",
        "from functools import reduce\n",
        "\n",
        "#from .operations import Operation, Variable, Placeholder\n",
        "\n",
        "class Session(object):\n",
        "    ''' A session to compute a particular graph.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        ''' Session constructor.\n",
        "        '''\n",
        "        # Graph the session computes for.\n",
        "        self.graph = DEFAULT_GRAPH\n",
        "\n",
        "    def __enter__(self):\n",
        "        ''' Context management protocal method called before `with-block`.\n",
        "        '''\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        ''' Context management protocal method called after `with-block`.\n",
        "        '''\n",
        "        self.close()\n",
        "\n",
        "    def close(self):\n",
        "        ''' Free all output values in nodes.\n",
        "        '''\n",
        "        all_nodes = (self.graph.constants + self.graph.variables +\n",
        "                     self.graph.placeholders + self.graph.operations +\n",
        "                     self.graph.trainable_variables)\n",
        "        for node in all_nodes:\n",
        "            node.output_value = None\n",
        "\n",
        "    def run(self, operation, feed_dict=None):\n",
        "        ''' Compute the output of an operation.\n",
        "        :param operation: A specific operation to be computed.\n",
        "        :type operation: object of `Operation`, `Variable` or `Placeholder`.\n",
        "        :param feed_dict: A mapping between placeholder and its actual value for the session.\n",
        "        :type feed_dict: dict.\n",
        "        '''\n",
        "        # Get all prerequisite nodes using postorder traversal.\n",
        "        postorder_nodes = _get_prerequisite(operation)\n",
        "\n",
        "        for node in postorder_nodes:\n",
        "            if type(node) is Placeholder:\n",
        "                node.output_value = feed_dict[node]\n",
        "            else:  # Operation and variable\n",
        "                node.compute_output()\n",
        "\n",
        "        return operation.output_value\n",
        "\n",
        "def _get_prerequisite(operation):\n",
        "    ''' Perform a post-order traversal to get a list of nodes to be computed in order.\n",
        "    '''\n",
        "    postorder_nodes = []\n",
        "\n",
        "    # Collection nodes recursively.\n",
        "    def postorder_traverse(operation):\n",
        "        if isinstance(operation, Operation):\n",
        "            for input_node in operation.input_nodes:\n",
        "                postorder_traverse(input_node)\n",
        "        postorder_nodes.append(operation)\n",
        "\n",
        "    postorder_traverse(operation)\n",
        "\n",
        "    return postorder_nodes\n",
        "\n",
        "\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "''' Optimizer classes for parameters optimization.\n",
        "'''\n",
        "#from .operations import Operation, compute_gradients\n",
        "\n",
        "class GradientDescentOptimizer(object):\n",
        "    ''' Optimizer that implements the gradient descent algorithm.\n",
        "    '''\n",
        "    def __init__(self, learning_rate):\n",
        "        ''' Construct a new gradient descent optimizer\n",
        "        :param learning_rate: learning rate of optimizier.\n",
        "        :type learning_rate: float\n",
        "        '''\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def minimize(self, loss):\n",
        "        ''' Generate an gradient descent optimization operation for loss.\n",
        "        :param loss: The loss operation to be optimized.\n",
        "        :type loss: Object of `Operation`\n",
        "        '''\n",
        "        learning_rate = self.learning_rate\n",
        "\n",
        "        class MinimizationOperation(Operation):\n",
        "            def compute_output(self):\n",
        "                # Get gradient table.\n",
        "                grad_table = compute_gradients(loss)\n",
        "\n",
        "                # Iterate all trainable variables in graph.\n",
        "                for var in DEFAULT_GRAPH.trainable_variables:\n",
        "                    if var in grad_table:\n",
        "                        grad = grad_table[var]\n",
        "\n",
        "                    # Update its output value.\n",
        "                    var.output_value -= learning_rate*grad\n",
        "\n",
        "        return MinimizationOperation()\n",
        "\n",
        "\n",
        "\n",
        "import builtins\n",
        "DEFAULT_GRAPH = builtins.DEFAULT_GRAPH = Graph()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with Graph().as_default():\n",
        "    a = constant(1.0, name='a')\n",
        "    b = constant(2.0, name='b')\n",
        "    result = a + b\n",
        "    \n",
        "    # Create a session to run the graph\n",
        "    with Session() as sess:\n",
        "        print(sess.run(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CeLu5rnxsVJ",
        "outputId": "53f4640a-372b-4294-a509-bf61e444b128"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with Graph().as_default():\n",
        "    a = constant([[2.0, 2.0, 3.0], [3.0, 3.0, 3.0]], name='ma')\n",
        "    b = constant([3.0, 4.0, 5.0], name='mb')\n",
        "    result = matmul(a, b)\n",
        "    \n",
        "    with Session() as sess:\n",
        "        print(sess.run(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKlEM17Qxyvl",
        "outputId": "5d476c14-5557-4bad-c180-22fe636c0d65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[29. 36.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_x = np.linspace(-1, 1, 100)\n",
        "input_y = input_x*3 + np.random.randn(input_x.shape[0])*0.5"
      ],
      "metadata": {
        "id": "TEUIh9_FykhJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholders for training data\n",
        "x = Placeholder()\n",
        "y_ = Placeholder()\n",
        "\n",
        "# Weigths\n",
        "w = Variable([[1.0]], name='weight')\n",
        "\n",
        "# Threshold\n",
        "b = Variable(0.0, name='threshold')\n",
        "\n",
        "\n",
        "# Predicted class by model\n",
        "y = x*w + b\n",
        "\n",
        "\n",
        "loss = reduce_sum(square(y - y_))\n",
        "\n",
        "train_op = GradientDescentOptimizer(learning_rate=0.005).minimize(loss)\n",
        "\n",
        "feed_dict = {x: np.reshape(input_x, (-1, 1)), y_: np.reshape(input_y, (-1, 1))}\n",
        "feed_dict = {x: input_x, y_: input_y}\n",
        "with Session() as sess:\n",
        "    for step in range(20):\n",
        "        loss_value = sess.run(loss, feed_dict=feed_dict)\n",
        "        mse = loss_value/len(input_x)\n",
        "        \n",
        "        if step % 1 == 0:\n",
        "            print('step: {}, loss: {}, mse: {}'.format(step, loss_value, mse))\n",
        "        sess.run(train_op, feed_dict)\n",
        "    w_value = sess.run(w, feed_dict=feed_dict)\n",
        "    b_value = sess.run(b, feed_dict=feed_dict)\n",
        "    print('w: {}, b: {}'.format(w_value, b_value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7J4QuIEyIXc",
        "outputId": "92ee9365-ef67-4c70-9403-eae981a115d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 180.47701615594883, mse: 1.8047701615594882\n",
            "step: 1, loss: 94.42297046815727, mse: 0.9442297046815727\n",
            "step: 2, loss: 57.038230997407, mse: 0.57038230997407\n",
            "step: 3, loss: 40.756761402373115, mse: 0.40756761402373115\n",
            "step: 4, loss: 33.66600041468682, mse: 0.3366600041468682\n",
            "step: 5, loss: 30.57789518616188, mse: 0.3057789518616188\n",
            "step: 6, loss: 29.232991032855367, mse: 0.29232991032855365\n",
            "step: 7, loss: 28.64727032461219, mse: 0.2864727032461219\n",
            "step: 8, loss: 28.392182445508347, mse: 0.28392182445508346\n",
            "step: 9, loss: 28.281088838692117, mse: 0.28281088838692114\n",
            "step: 10, loss: 28.23270633804647, mse: 0.2823270633804647\n",
            "step: 11, loss: 28.211635221212546, mse: 0.28211635221212544\n",
            "step: 12, loss: 28.2024585156123, mse: 0.282024585156123\n",
            "step: 13, loss: 28.198461958318386, mse: 0.2819846195831839\n",
            "step: 14, loss: 28.196721413192567, mse: 0.28196721413192566\n",
            "step: 15, loss: 28.195963386442994, mse: 0.28195963386442996\n",
            "step: 16, loss: 28.195633257367597, mse: 0.28195633257367597\n",
            "step: 17, loss: 28.195489482485662, mse: 0.28195489482485664\n",
            "step: 18, loss: 28.195426866926432, mse: 0.2819542686692643\n",
            "step: 19, loss: 28.195399597154378, mse: 0.2819539959715438\n",
            "w: [[3.11412557]], b: 0.04614946147415461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w_value = float(w_value)\n",
        "max_x, min_x = np.max(input_x), np.min(input_x)\n",
        "max_y, min_y = w_value*max_x + b_value, w_value*min_x + b_value\n",
        "\n",
        "plt.plot([max_x, min_x], [max_y, min_y], color='r')\n",
        "plt.scatter(input_x, input_y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "AeKtQeCmyv2S",
        "outputId": "2c93ef94-4340-4d6d-bd14-9e607f2dcb46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}