{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQtNGw3F70B29/kK2Tnqgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/IRC018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "!pip install gbnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72IBtYKiHXWu",
        "outputId": "5331668a-6fee-4b5a-bc28-c934f9cbdb3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n",
            "Requirement already satisfied: gbnet in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gbnet) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from gbnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->gbnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->gbnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->gbnet) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # 导入时间模块\n",
        "\n",
        "import lightgbm as lgb  # 导入lightgbm库\n",
        "import numpy as np  # 导入numpy库\n",
        "import xgboost as xgb  # 导入xgboost库\n",
        "import torch  # 导入pytorch库\n",
        "\n",
        "from gbnet import lgbmodule, xgbmodule  # 从gbnet库导入lgbmodule和xgbmodule模块\n",
        "\n",
        "# 生成数据集\n",
        "np.random.seed(100)  # 设置随机数种子\n",
        "n = 1000  # 设置样本数量\n",
        "input_dim = 20  # 设置输入特征维度\n",
        "output_dim = 1  # 设置输出维度\n",
        "X = np.random.random([n, input_dim])  # 生成随机输入特征\n",
        "B = np.random.random([input_dim, output_dim])  # 生成随机权重\n",
        "Y = X.dot(B) + np.random.random([n, output_dim])  # 生成目标变量\n",
        "\n",
        "iters = 100  # 设置迭代次数\n",
        "t0 = time.time()  # 记录开始时间\n",
        "\n",
        "# 使用XGBoost进行训练以进行比较\n",
        "xbst = xgb.train(\n",
        "    params={'objective': 'reg:squarederror', 'base_score': 0.0},  # 设置参数\n",
        "    dtrain=xgb.DMatrix(X, label=Y),  # 创建DMatrix数据集\n",
        "    num_boost_round=iters  # 设置迭代次数\n",
        ")\n",
        "t1 = time.time()  # 记录XGBoost训练结束时间\n",
        "\n",
        "# 使用LightGBM进行训练以进行比较\n",
        "lbst = lgb.train(\n",
        "    params={'verbose': -1},  # 设置参数\n",
        "    train_set=lgb.Dataset(X, label=Y.flatten(), init_score=[0 for i in range(n)]),  # 创建Dataset数据集\n",
        "    num_boost_round=iters  # 设置迭代次数\n",
        ")\n",
        "t2 = time.time()  # 记录LightGBM训练结束时间\n",
        "\n",
        "# 使用XGBModule进行训练\n",
        "xnet = xgbmodule.XGBModule(n, input_dim, output_dim, params={})  # 初始化XGBModule模型\n",
        "xmse = torch.nn.MSELoss()  # 定义均方误差损失函数\n",
        "\n",
        "X_dmatrix = xgb.DMatrix(X)  # 创建DMatrix数据集\n",
        "for i in range(iters):  # 循环迭代\n",
        "    xnet.zero_grad()  # 清空梯度\n",
        "    xpred = xnet(X_dmatrix)  # 进行预测\n",
        "\n",
        "    loss = 1 / 2 * xmse(xpred, torch.Tensor(Y))  # 计算损失\n",
        "    loss.backward(create_graph=True)  # 反向传播计算梯度\n",
        "\n",
        "    xnet.gb_step()  # 更新模型参数\n",
        "xnet.eval()  # 切换到评估模式\n",
        "t3 = time.time()  # 记录XGBModule训练结束时间\n",
        "\n",
        "# 使用LGBModule进行训练\n",
        "lnet = lgbmodule.LGBModule(n, input_dim, output_dim, params={})  # 初始化LGBModule模型\n",
        "lmse = torch.nn.MSELoss()  # 定义均方误差损失函数\n",
        "\n",
        "X_dataset = lgb.Dataset(X)  # 创建Dataset数据集\n",
        "for i in range(iters):  # 循环迭代\n",
        "    lnet.zero_grad()  # 清空梯度\n",
        "    lpred = lnet(X_dataset)  # 进行预测\n",
        "\n",
        "    loss = lmse(lpred, torch.Tensor(Y))  # 计算损失\n",
        "    loss.backward(create_graph=True)  # 反向传播计算梯度\n",
        "\n",
        "    lnet.gb_step()  # 更新模型参数\n",
        "lnet.eval()  # 切换到评估模式\n",
        "t4 = time.time()  # 记录LGBModule训练结束时间\n",
        "\n",
        "print(np.max(np.abs(xbst.predict(xgb.DMatrix(X)) - xnet(X_dmatrix).detach().numpy().flatten())))  # 打印XGBoost和XGBModule预测结果的差异\n",
        "print(np.max(np.abs(lbst.predict(X) - lnet(X).detach().numpy().flatten())))  # 打印LightGBM和LGBModule预测结果的差异\n",
        "print(f'xgboost time: {t1 - t0}')  # 打印XGBoost训练时间\n",
        "print(f'lightgbm time: {t2 - t1}')  # 打印LightGBM训练时间\n",
        "print(f'xgbmodule time: {t3 - t2}')  # 打印XGBModule训练时间\n",
        "print(f'lgbmodule time: {t4 - t3}')  # 打印LGBModule训练时间"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrGHkhWaIU9h",
        "outputId": "51a14285-b0a7-4463-fac5-14ba14ddbcdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1260.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9073486e-06\n",
            "2.6987259360566895e-07\n",
            "xgboost time: 0.8300571441650391\n",
            "lightgbm time: 0.21436691284179688\n",
            "xgbmodule time: 0.9426262378692627\n",
            "lgbmodule time: 0.3606119155883789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
        "\n",
        "# 1. 配置类\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    配置类，用于存储模型和数据相关的参数。\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.feature_dim = 100  # 输入特征维度\n",
        "        self.num_experts = 2  # Expert 数量 (新老用户)\n",
        "        self.hidden_size = 128  # 隐藏层大小\n",
        "        self.num_layers = 3 # 网络层数\n",
        "        self.dropout_rate = 0.2  # dropout 概率\n",
        "        self.learning_rate = 0.001  # 学习率\n",
        "        self.batch_size = 256  # 批量大小\n",
        "        self.num_epochs = 50  # 训练 epoch 数量\n",
        "        self.test_size = 0.2  # 测试集比例\n",
        "        self.distill_weight = 0.1 #  知识蒸馏损失权重\n",
        "        self.consistency_weight = 0.1 # 一致性损失权重\n",
        "        self.teacher_prob_dim = 4 # teacher model 输出概率维度\n",
        "        self.use_teacher_probs = True # 是否使用教师模型概率\n",
        "        self.focal_loss_gamma = 2.0 # Focal Loss 的 gamma 参数\n",
        "        self.temperature = 3.0 # 知识蒸馏温度参数\n",
        "        self.use_shadow_samples = False # 是否使用影子样本\n",
        "        self.use_time_series = False # 是否使用时间序列特征\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# 2. 模型定义\n",
        "class GatedDenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    门控 Dense Block，包含一个全连接层和一个门控机制。\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(GatedDenseBlock, self).__init__()\n",
        "        self.dense = nn.Linear(input_size, output_size) # 定义一个线性层 (全连接层)，输入大小为 input_size，输出大小为 output_size\n",
        "        self.gate = nn.Sequential( # 定义一个序列化的模块，包含一个线性层和一个 Sigmoid 激活函数\n",
        "            nn.Linear(input_size, output_size), # 线性层，输入大小为 input_size，输出大小为 output_size\n",
        "            nn.Sigmoid() # Sigmoid 激活函数，将输出值映射到 0 到 1 之间\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前向传播函数。\n",
        "        Args:\n",
        "            x (torch.Tensor): 输入张量，形状为 (batch_size, input_size).\n",
        "        Returns:\n",
        "            torch.Tensor: 输出张量，形状为 (batch_size, output_size).\n",
        "        \"\"\"\n",
        "        output = self.dense(x) # 将输入 x 通过线性层 dense，得到输出 output\n",
        "        gate_value = self.gate(x) # 将输入 x 通过门控模块 gate，得到门控值 gate_value\n",
        "        return output * gate_value # 返回输出 output 和门控值 gate_value 的乘积，实现门控机制\n",
        "\n",
        "class UnifiedRiskModel(nn.Module):\n",
        "    \"\"\"\n",
        "    统一风险模型，包含新老用户区分网络和专家网络。\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super(UnifiedRiskModel, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.num_experts = cfg.num_experts # 2，表示新老用户\n",
        "        self.feature_dim = cfg.feature_dim\n",
        "        self.hidden_size = cfg.hidden_size\n",
        "        self.num_layers = cfg.num_layers\n",
        "        self.dropout_rate = cfg.dropout_rate\n",
        "        self.teacher_prob_dim = cfg.teacher_prob_dim\n",
        "\n",
        "        # 新老用户区分网络 (Bi-Gated Dense Block)\n",
        "        self.bi_gated_dense = nn.Sequential(\n",
        "            GatedDenseBlock(self.feature_dim + self.teacher_prob_dim, self.hidden_size), # 输入包含原始特征和教师模型概率\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            GatedDenseBlock(self.hidden_size, self.hidden_size),\n",
        "            nn.Dropout(self.dropout_rate)\n",
        "        )\n",
        "\n",
        "        # 新用户专家通道\n",
        "        self.new_user_suspect_net = nn.Sequential( # 预测嫌疑\n",
        "            GatedDenseBlock(self.hidden_size + self.teacher_prob_dim, self.hidden_size), # 输入包含共享层输出和教师模型概率\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size, 1),\n",
        "        )\n",
        "        self.new_user_chargeback_net = nn.Sequential( # 预测拒付\n",
        "            GatedDenseBlock(self.hidden_size + self.teacher_prob_dim, self.hidden_size),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        # 老用户专家通道\n",
        "        self.old_user_suspect_net = nn.Sequential( # 预测嫌疑\n",
        "            GatedDenseBlock(self.hidden_size + self.teacher_prob_dim, self.hidden_size),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size, 1),\n",
        "        )\n",
        "        self.old_user_chargeback_net = nn.Sequential( # 预测拒付\n",
        "            GatedDenseBlock(self.hidden_size + self.teacher_prob_dim, self.hidden_size),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        # 注意力融合层\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(4, 2),  # 输入维度为4 (新/老用户嫌疑和拒付预测共4个)，输出注意力权重\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # 最终拒付预测层\n",
        "        self.final_chargeback_net = nn.Sequential(\n",
        "            nn.Linear(2, self.hidden_size), # 输入融合的新老用户特征\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, user_type):\n",
        "        \"\"\"\n",
        "        前向传播函数。\n",
        "        Args:\n",
        "            x (torch.Tensor): 输入特征张量，形状为 (batch_size, feature_dim + teacher_prob_dim + 1).\n",
        "            user_type (torch.Tensor): 用户类型张量，形状为 (batch_size,).\n",
        "        Returns:\n",
        "            tuple: 包含新老用户预测概率的元组，形状都为 (batch_size, 1).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        # 创建 one-hot 编码的用户类型表示\n",
        "        user_type_onehot = F.one_hot(user_type.long(), num_classes=self.num_experts).float()  # (batch_size, num_experts)\n",
        "\n",
        "        # 提取原始特征和教师模型概率\n",
        "        original_features = x[:, :self.feature_dim] # (batch_size, feature_dim)\n",
        "        teacher_probs = x[:, self.feature_dim+1:self.feature_dim + self.teacher_prob_dim + 1] # (batch_size, teacher_prob_dim)\n",
        "\n",
        "        # 共享特征提取\n",
        "        combined_features = torch.cat([original_features, teacher_probs], dim=1)\n",
        "        shared_features = self.bi_gated_dense(combined_features) # (batch_size, hidden_size)\n",
        "\n",
        "        # 新用户专家通道\n",
        "        new_user_features = torch.cat([shared_features, teacher_probs], dim=1) # 拼接共享特征和教师模型概率\n",
        "        new_user_suspect_prob = self.sigmoid(self.new_user_suspect_net(new_user_features)) # (batch_size, 1)\n",
        "        new_user_chargeback_prob = self.sigmoid(self.new_user_chargeback_net(new_user_features)) # (batch_size, 1)\n",
        "\n",
        "        # 老用户专家通道\n",
        "        old_user_features = torch.cat([shared_features, teacher_probs], dim=1)\n",
        "        old_user_suspect_prob = self.sigmoid(self.old_user_suspect_net(old_user_features)) # (batch_size, 1)\n",
        "        old_user_chargeback_prob = self.sigmoid(self.old_user_chargeback_net(old_user_features)) # (batch_size, 1)\n",
        "\n",
        "        # 注意力融合\n",
        "        attention_input = torch.cat([new_user_suspect_prob, new_user_chargeback_prob, old_user_suspect_prob, old_user_chargeback_prob], dim=1) # (batch_size, 4)\n",
        "        attention_weights = self.attention_layer(attention_input) # (batch_size, 2)\n",
        "\n",
        "        #  将新老用户的输出拼接\n",
        "        new_user_output = torch.cat([new_user_suspect_prob, new_user_chargeback_prob], dim=1)\n",
        "        old_user_output = torch.cat([old_user_suspect_prob, old_user_chargeback_prob], dim=1)\n",
        "        fused_features = attention_weights[:, 0].unsqueeze(1) * new_user_output + attention_weights[:, 1].unsqueeze(1) * old_user_output # (batch_size, 2)\n",
        "\n",
        "\n",
        "        # 最终拒付预测\n",
        "        final_chargeback_prob = self.sigmoid(self.final_chargeback_net(fused_features)) # (batch_size, 1)\n",
        "\n",
        "        return new_user_suspect_prob, new_user_chargeback_prob, old_user_suspect_prob, old_user_chargeback_prob, final_chargeback_prob, attention_weights\n",
        "\n",
        "# 3. 损失函数\n",
        "def compute_distillation_loss(student_logits, teacher_logits, temperature):\n",
        "    \"\"\"\n",
        "    计算知识蒸馏损失。\n",
        "    Args:\n",
        "        student_logits (torch.Tensor): 学生模型 logits.\n",
        "        teacher_logits (torch.Tensor): 教师模型 logits.\n",
        "        temperature (float): 温度参数.\n",
        "    Returns:\n",
        "        torch.Tensor: 蒸馏损失.\n",
        "    \"\"\"\n",
        "    p_s = F.log_softmax(student_logits / temperature, dim=-1)\n",
        "    p_t = F.softmax(teacher_logits / temperature, dim=-1)\n",
        "    loss = F.kl_div(p_s, p_t, reduction='batchmean') * (temperature ** 2)\n",
        "    return loss\n",
        "\n",
        "def compute_consistency_loss(suspect_prob_new, chargeback_prob):\n",
        "    \"\"\"\n",
        "    计算新老用户预测一致性损失。这里改为计算新老用户各自的嫌疑预测和拒付预测之间的一致性。\n",
        "\n",
        "    Args:\n",
        "        suspect_prob_new (torch.Tensor): 新用户疑似欺诈概率.\n",
        "        chargeback_prob (torch.Tensor): 新用户拒付概率.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 一致性损失.\n",
        "    \"\"\"\n",
        "    loss = F.mse_loss(suspect_prob_new, chargeback_prob)\n",
        "    return loss\n",
        "\n",
        "def focal_loss(logits, labels, gamma=2.0, reduction='mean'):\n",
        "    \"\"\"\n",
        "    计算 Focal Loss，用于处理类别不平衡问题。\n",
        "    Args:\n",
        "        logits (torch.Tensor): 模型的预测 logits.\n",
        "        labels (torch.Tensor): 真实的标签.\n",
        "        gamma (float):  聚焦参数，gamma越大，越关注难分类样本.\n",
        "        reduction (str): 'none' | 'mean' | 'sum': Specifies the reduction to apply to the output\n",
        "    Returns:\n",
        "        torch.Tensor: 损失值.\n",
        "    \"\"\"\n",
        "    ce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
        "    p_t = torch.exp(-ce_loss)\n",
        "    focal_loss = (1 - p_t) ** gamma * ce_loss\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(focal_loss)\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(focal_loss)\n",
        "    else:\n",
        "        return focal_loss\n",
        "\n",
        "def compute_loss(cfg, new_user_suspect_prob, new_user_chargeback_prob, old_user_suspect_prob, old_user_chargeback_prob, final_chargeback_prob, y, user_type, teacher_probs=None):\n",
        "    \"\"\"\n",
        "    计算总损失，包括疑似欺诈损失、chargeback 损失、知识蒸馏损失和一致性损失。\n",
        "    Args:\n",
        "        cfg (Config): 配置对象.\n",
        "        new_user_suspect_prob (torch.Tensor): 新用户疑似欺诈概率.\n",
        "        new_user_chargeback_prob (torch.Tensor): 新用户 chargeback 概率.\n",
        "        old_user_suspect_prob (torch.Tensor): 老用户疑似欺诈概率.\n",
        "        old_user_chargeback_prob (torch.Tensor): 老用户 chargeback 概率.\n",
        "        final_chargeback_prob (torch.Tensor): 最终 chargeback 概率.\n",
        "        y (torch.Tensor): 标签，形状为 (batch_size,).\n",
        "        user_type (torch.Tensor): 用户类型，形状为 (batch_size,).\n",
        "        teacher_probs (dict, optional): 包含四个教师模型预测概率的字典.\n",
        "            key 为模型名称 (A, B, C, D)，value 为对应的预测概率 DataFrame。\n",
        "            默认为 None，表示不使用教师模型概率。\n",
        "    Returns:\n",
        "        tuple: 包含总损失和各项损失的元组.\n",
        "    \"\"\"\n",
        "    # 疑似欺诈损失 (新老用户)\n",
        "    L_suspect_new = F.binary_cross_entropy(new_user_suspect_prob, y.unsqueeze(1))\n",
        "    L_suspect_old = focal_loss(old_user_suspect_prob, y.unsqueeze(1), gamma=cfg.focal_loss_gamma) # 老用户使用Focal Loss\n",
        "    L_suspect = L_suspect_new + L_suspect_old\n",
        "\n",
        "    # chargeback 损失 (新老用户)\n",
        "    L_chargeback = focal_loss(final_chargeback_prob, y.unsqueeze(1), gamma=cfg.focal_loss_gamma) # 全部使用Focal Loss\n",
        "\n",
        "    # 知识蒸馏损失\n",
        "    if teacher_probs is not None and cfg.use_teacher_probs:\n",
        "        distill_losses = []\n",
        "        for i in range(len(user_type)):\n",
        "            user_type_val = user_type[i].item()\n",
        "            if user_type_val == 0:  # 新用户\n",
        "                #teacher_prob_A = teacher_probs['A']['prob_A'].iloc[i]\n",
        "                #teacher_prob_B = teacher_probs['B']['prob_B'].iloc[i]\n",
        "                teacher_prob_A = torch.tensor(teacher_probs['A']['prob_A'].iloc[i], dtype=torch.float32)\n",
        "                teacher_prob_B = torch.tensor(teacher_probs['B']['prob_B'].iloc[i], dtype=torch.float32)\n",
        "                student_logits_suspect = torch.stack([1 - new_user_suspect_prob[i], new_user_suspect_prob[i]]).squeeze()\n",
        "                student_logits_chargeback = torch.stack([1 - new_user_chargeback_prob[i], new_user_chargeback_prob[i]]).squeeze()\n",
        "\n",
        "                teacher_logits_A = torch.stack([1 - teacher_prob_A, teacher_prob_A]).squeeze()\n",
        "                teacher_logits_B = torch.stack([1 - teacher_prob_B, teacher_prob_B]).squeeze()\n",
        "                distill_loss_A = compute_distillation_loss(student_logits_suspect, teacher_logits_A, cfg.temperature)\n",
        "                distill_loss_B = compute_distillation_loss(student_logits_chargeback, teacher_logits_B, cfg.temperature)\n",
        "                distill_losses.extend([distill_loss_A, distill_loss_B])\n",
        "\n",
        "            elif user_type_val == 1: # 老用户\n",
        "                #teacher_prob_C = teacher_probs['C']['prob_C'].iloc[i]\n",
        "                #teacher_prob_D = teacher_probs['D']['prob_D'].iloc[i]\n",
        "                teacher_prob_C = torch.tensor(teacher_probs['C']['prob_C'].iloc[i], dtype=torch.float32)\n",
        "                teacher_prob_D = torch.tensor(teacher_probs['D']['prob_D'].iloc[i], dtype=torch.float32)\n",
        "                student_logits_suspect = torch.stack([1 - old_user_suspect_prob[i], old_user_suspect_prob[i]]).squeeze()\n",
        "                student_logits_chargeback = torch.stack([1 - old_user_chargeback_prob[i], old_user_chargeback_prob[i]]).squeeze()\n",
        "                teacher_logits_C = torch.stack([1 - teacher_prob_C, teacher_prob_C]).squeeze()\n",
        "                teacher_logits_D = torch.stack([1 - teacher_prob_D, teacher_prob_D]).squeeze()\n",
        "                distill_loss_C = compute_distillation_loss(student_logits_suspect, teacher_logits_C, cfg.temperature)\n",
        "                distill_loss_D = compute_distillation_loss(student_logits_chargeback, teacher_logits_D, cfg.temperature)\n",
        "                distill_losses.extend([distill_loss_C, distill_loss_D])\n",
        "        if distill_losses:\n",
        "            L_distill = torch.mean(torch.stack(distill_losses)) # (1,)\n",
        "        else:\n",
        "            L_distill = torch.tensor(0.0)\n",
        "    else:\n",
        "        L_distill = torch.tensor(0.0)\n",
        "\n",
        "    # 一致性损失\n",
        "    L_consistency_new = compute_consistency_loss(new_user_suspect_prob, new_user_chargeback_prob)\n",
        "    L_consistency_old = compute_consistency_loss(old_user_suspect_prob, old_user_chargeback_prob)\n",
        "    L_consistency = L_consistency_new + L_consistency_old\n",
        "\n",
        "    # 任务一致性损失 (嫌疑预测与最终拒付预测的 logits 方向一致)\n",
        "    consistency_loss_suspect_new = 1 - F.cosine_similarity(new_user_suspect_prob, final_chargeback_prob, dim=0).mean()\n",
        "    consistency_loss_suspect_old = 1 - F.cosine_similarity(old_user_suspect_prob, final_chargeback_prob, dim=0).mean()\n",
        "    L_task_consistency = consistency_loss_suspect_new + consistency_loss_suspect_old\n",
        "\n",
        "\n",
        "    # 总损失\n",
        "    total_loss = L_suspect + L_chargeback + cfg.distill_weight * L_distill + cfg.consistency_weight * L_consistency + 0.1 * L_task_consistency # 增加任务一致性损失\n",
        "\n",
        "    return total_loss, L_suspect, L_chargeback, L_distill, L_consistency, L_task_consistency\n",
        "\n",
        "# 4. 模型训练函数\n",
        "def train_model(model, train_loader, optimizer, cfg, teacher_probs=None):\n",
        "    \"\"\"\n",
        "    训练模型一个 epoch。\n",
        "    Args:\n",
        "        model (nn.Module): 统一风险模型.\n",
        "        train_loader (torch.utils.data.DataLoader): 训练数据加载器.\n",
        "        optimizer (torch.optim.Optimizer): 优化器.\n",
        "        cfg (Config): 配置对象.\n",
        "        teacher_probs (dict, optional): 包含四个教师模型预测概率的字典.\n",
        "            key 为模型名称 (A, B, C, D)，value 为对应的预测概率 DataFrame。\n",
        "            默认为 None，表示不使用教师模型概率。\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        user_type = x[:, cfg.feature_dim] # 从输入特征中获取用户类型 # (batch_size,)\n",
        "        # 确保 user_type 的值在有效范围内\n",
        "        user_type = torch.clamp(user_type, 0, cfg.num_experts - 1)\n",
        "        new_user_suspect_prob, new_user_chargeback_prob, old_user_suspect_prob, old_user_chargeback_prob, final_chargeback_prob, _ = model(x, user_type)\n",
        "        loss, L_suspect, L_chargeback, L_distill, L_consistency, L_task_consistency = compute_loss(cfg, new_user_suspect_prob, new_user_chargeback_prob, old_user_suspect_prob, old_user_chargeback_prob, final_chargeback_prob, y, user_type, teacher_probs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"  Training Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# 5. 模型评估函数\n",
        "def evaluate_model(model, test_loaders):\n",
        "    \"\"\"\n",
        "    评估模型性能，计算 AUC 和 PR 曲线下的 AUC。\n",
        "    Args:\n",
        "        model (nn.Module): 统一风险模型.\n",
        "        test_loaders (dict): 包含新老用户测试集 DataLoader 的字典.\n",
        "    Returns:\n",
        "        dict: 包含新老用户 AUC 和 PR-AUC 的字典.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    for user_type_str, test_loader in test_loaders.items():\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                user_type_batch = x[:, cfg.feature_dim] # 从输入特征中获取用户类型 # (batch_size,)\n",
        "                # 确保 user_type_batch 的值在有效范围内\n",
        "                user_type_batch = torch.clamp(user_type_batch, 0, cfg.num_experts - 1).long()\n",
        "                _, _, _, _, chargeback_prob, _ = model(x, user_type_batch)\n",
        "                all_preds.extend(chargeback_prob.cpu().numpy())\n",
        "                all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        # 计算 AUC\n",
        "        auc_value = roc_auc_score(all_labels, all_preds)\n",
        "\n",
        "        # 计算 PR-AUC\n",
        "        precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
        "        pr_auc = auc(recall, precision)\n",
        "\n",
        "        results[user_type_str] = {\n",
        "            'auc': auc_value,\n",
        "            'pr_auc': pr_auc\n",
        "        }\n",
        "        print(f\"{user_type_str} AUC: {auc_value:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
        "    return results\n",
        "\n",
        "# 6. 主要训练流程\n",
        "def main(new_user_data, old_user_data, teacher_probs=None):\n",
        "    \"\"\"\n",
        "    主要训练流程。\n",
        "    Args:\n",
        "        new_user_data (pd.DataFrame): 新用户数据，包含特征和标签。\n",
        "        old_user_data (pd.DataFrame): 老用户数据，包含特征和标签。\n",
        "        teacher_probs (dict, optional): 包含四个教师模型预测概率的字典.\n",
        "            key 为模型名称 (A, B, C, D)，value 为对应的预测概率 DataFrame。\n",
        "            默认为 None，表示不使用教师模型概率。\n",
        "    Returns:\n",
        "        tuple: 包含训练集 DataLoader 和测试集 DataLoader 字典的元组。\n",
        "    \"\"\"\n",
        "    # 1. 数据准备\n",
        "    train_loader, test_loaders = prepare_data(new_user_data, old_user_data, cfg, teacher_probs)\n",
        "\n",
        "    # 2. 模型初始化\n",
        "    model = UnifiedRiskModel(cfg)\n",
        "\n",
        "    # 3. 优化器\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
        "\n",
        "    # 4. 训练循环\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{cfg.num_epochs}\")\n",
        "        train_model(model, train_loader, optimizer, cfg, teacher_probs)\n",
        "\n",
        "        # 5. 评估\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            evaluate_model(model, test_loaders)\n",
        "\n",
        "    # 在训练结束后调用evaluate_model获取评估结果\n",
        "    evaluation_results = evaluate_model(model, test_loaders)\n",
        "\n",
        "    # 6. 保存模型 (可选)\n",
        "    # torch.save(model.state_dict(), 'unified_risk_model.pth')\n",
        "    return model, evaluation_results # 返回训练好的模型和测试集loaders\n",
        "\n",
        "def prepare_data(new_user_data, old_user_data, cfg, teacher_probs=None):\n",
        "    \"\"\"\n",
        "    准备训练和测试数据。\n",
        "    Args:\n",
        "        new_user_data (pd.DataFrame): 新用户数据，包含特征和标签。\n",
        "        old_user_data (pd.DataFrame): 老用户数据，包含特征和标签。\n",
        "        cfg (Config): 配置对象.\n",
        "        teacher_probs (dict, optional): 包含四个教师模型预测概率的字典.\n",
        "            key 为模型名称 (A, B, C, D)，value 为对应的预测概率 DataFrame。\n",
        "            默认为 None，表示不使用教师模型概率。\n",
        "    Returns:\n",
        "        tuple: 包含训练集 DataLoader 和测试集 DataLoader 字典的元组。\n",
        "    \"\"\"\n",
        "    # 1. 数据预处理\n",
        "    def preprocess_data(df):\n",
        "        \"\"\"\n",
        "        对数据进行预处理，包括填充缺失值和转换为 PyTorch 张量。\n",
        "        Args:\n",
        "            df (pd.DataFrame): 输入数据 DataFrame.\n",
        "        Returns:\n",
        "            torch.Tensor: 预处理后的特征张量.\n",
        "            torch.Tensor: 标签张量.\n",
        "        \"\"\"\n",
        "        df = df.fillna(0)  # 简单地填充缺失值\n",
        "        features = torch.tensor(df.drop('label', axis=1).values, dtype=torch.float32)\n",
        "        labels = torch.tensor(df['label'].values, dtype=torch.float32)\n",
        "        return features, labels\n",
        "\n",
        "    new_user_features, new_user_labels = preprocess_data(new_user_data)\n",
        "    old_user_features, old_user_labels = preprocess_data(old_user_data)\n",
        "\n",
        "    # 添加用户类型特征\n",
        "    new_user_type = torch.zeros(new_user_features.size(0), 1)  # 0 表示新用户\n",
        "    old_user_type = torch.ones(old_user_features.size(0), 1)   # 1 表示老用户\n",
        "    new_user_features = torch.cat([new_user_features, new_user_type], dim=1)\n",
        "    old_user_features = torch.cat([old_user_features, old_user_type], dim=1)\n",
        "\n",
        "    # 如果提供了教师模型预测概率，则将其添加到特征中\n",
        "    if teacher_probs and cfg.use_teacher_probs:\n",
        "        # 将 teacher_probs 转换为 DataFrame 并对齐索引\n",
        "        teacher_probs_df = pd.concat([teacher_probs['A'], teacher_probs['B'], teacher_probs['C'], teacher_probs['D']], axis=1)\n",
        "        teacher_probs_df.columns = ['prob_A', 'prob_B',\n",
        "                                    'prob_C', 'prob_D']\n",
        "        teacher_probs_df = teacher_probs_df.reset_index(drop=True)\n",
        "\n",
        "        # 确保 new_user_features 和 old_user_features 的数量与 teacher_probs_df 的行数匹配\n",
        "        if new_user_features.size(0) + old_user_features.size(0) != teacher_probs_df.shape[0]:\n",
        "            raise ValueError(\"Total number of samples in new_user_data and old_user_data must match the number of rows in teacher_probs.\")\n",
        "\n",
        "        # 将 teacher_probs_df 转换为 tensor\n",
        "        teacher_probs_tensor = torch.tensor(teacher_probs_df.values, dtype=torch.float32)\n",
        "\n",
        "        # 将 teacher_probs 添加到特征中\n",
        "        new_user_features = torch.cat([new_user_features, teacher_probs_tensor[:new_user_features.size(0)]], dim=1)\n",
        "        old_user_features = torch.cat([old_user_features, teacher_probs_tensor[new_user_features.size(0):]], dim=1)\n",
        "\n",
        "    # 2. 划分训练集和测试集\n",
        "    new_user_train_features, new_user_test_features, new_user_train_labels, new_user_test_labels = train_test_split(\n",
        "        new_user_features, new_user_labels, test_size=cfg.test_size, random_state=42\n",
        "    )\n",
        "    old_user_train_features, old_user_test_features, old_user_train_labels, old_user_test_labels = train_test_split(\n",
        "        old_user_features, old_user_labels, test_size=cfg.test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    # 3. 创建 DataLoader\n",
        "    train_features = torch.cat([new_user_train_features, old_user_train_features], dim=0)\n",
        "    train_labels = torch.cat([new_user_train_labels, old_user_train_labels], dim=0)\n",
        "    train_dataset = TensorDataset(train_features, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
        "\n",
        "    # 创建测试集 DataLoader 字典\n",
        "    test_loaders = {\n",
        "        'new_user': DataLoader(TensorDataset(new_user_test_features, new_user_test_labels), batch_size=cfg.batch_size, shuffle=False),\n",
        "        'old_user': DataLoader(TensorDataset(old_user_test_features, old_user_test_labels), batch_size=cfg.batch_size, shuffle=False),\n",
        "    }\n",
        "\n",
        "    return train_loader, test_loaders\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 模拟数据生成 (使用 Pandas DataFrame)\n",
        "    def generate_synthetic_data(num_samples, num_features):\n",
        "        \"\"\"\n",
        "        生成模拟数据。\n",
        "        Args:\n",
        "            num_samples (int): 样本数量.\n",
        "            num_features (int): 特征数量.\n",
        "        Returns:\n",
        "        pd.DataFrame: 包含模拟数据的 DataFrame.\n",
        "        \"\"\"\n",
        "        data = np.random.rand(num_samples, num_features)\n",
        "        labels = np.random.randint(0, 2, num_samples)  # 随机生成 0 和 1 的标签\n",
        "        df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(num_features)])\n",
        "        df['label'] = labels\n",
        "        return df\n",
        "\n",
        "    # 生成新用户和老用户数据\n",
        "    num_features = 100\n",
        "    new_user_data = generate_synthetic_data(1000, num_features)\n",
        "    old_user_data = generate_synthetic_data(1500, num_features)\n",
        "\n",
        "    # 模拟教师模型预测概率 (DataFrame)\n",
        "    teacher_probs = {\n",
        "        'A': pd.DataFrame(np.random.rand(2500, 1), columns=['prob_A']),\n",
        "        'B': pd.DataFrame(np.random.rand(2500, 1), columns=['prob_B']),\n",
        "        'C': pd.DataFrame(np.random.rand(2500, 1), columns=['prob_C']),\n",
        "        'D': pd.DataFrame(np.random.rand(2500, 1), columns=['prob_D']),\n",
        "    }\n",
        "\n",
        "    # 运行主要训练流程\n",
        "    trained_model, evaluation_results = main(new_user_data, old_user_data, teacher_probs)\n",
        "\n",
        "    # 打印测试结果\n",
        "    print(\"Final Test Results:\")\n",
        "    for user_type, metrics in evaluation_results.items():\n",
        "        print(type(user_type))\n",
        "        print(f\"{user_type.capitalize()} - AUC: {metrics['auc']:.4f}, PR-AUC: {metrics['pr_auc']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIZz8F7SaNTG",
        "outputId": "cba60a2b-ac1c-476b-e854-887d3bd73516"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "  Training Loss: 1.1473\n",
            "Epoch 2/50\n",
            "  Training Loss: 1.0969\n",
            "Epoch 3/50\n",
            "  Training Loss: 1.0795\n",
            "Epoch 4/50\n",
            "  Training Loss: 1.0660\n",
            "Epoch 5/50\n",
            "  Training Loss: 1.0605\n",
            "Epoch 6/50\n",
            "  Training Loss: 1.0546\n",
            "Epoch 7/50\n",
            "  Training Loss: 1.0518\n",
            "Epoch 8/50\n",
            "  Training Loss: 1.0497\n",
            "Epoch 9/50\n",
            "  Training Loss: 1.0477\n",
            "Epoch 10/50\n",
            "  Training Loss: 1.0473\n",
            "new_user AUC: 0.5124, PR-AUC: 0.5328\n",
            "old_user AUC: 0.4760, PR-AUC: 0.4704\n",
            "Epoch 11/50\n",
            "  Training Loss: 1.0472\n",
            "Epoch 12/50\n",
            "  Training Loss: 1.0432\n",
            "Epoch 13/50\n",
            "  Training Loss: 1.0413\n",
            "Epoch 14/50\n",
            "  Training Loss: 1.0387\n",
            "Epoch 15/50\n",
            "  Training Loss: 1.0359\n",
            "Epoch 16/50\n",
            "  Training Loss: 1.0345\n",
            "Epoch 17/50\n",
            "  Training Loss: 1.0329\n",
            "Epoch 18/50\n",
            "  Training Loss: 1.0309\n",
            "Epoch 19/50\n",
            "  Training Loss: 1.0318\n",
            "Epoch 20/50\n",
            "  Training Loss: 1.0315\n",
            "new_user AUC: 0.4978, PR-AUC: 0.5214\n",
            "old_user AUC: 0.4743, PR-AUC: 0.4700\n",
            "Epoch 21/50\n",
            "  Training Loss: 1.0313\n",
            "Epoch 22/50\n",
            "  Training Loss: 1.0308\n",
            "Epoch 23/50\n",
            "  Training Loss: 1.0304\n",
            "Epoch 24/50\n",
            "  Training Loss: 1.0345\n",
            "Epoch 25/50\n",
            "  Training Loss: 1.0302\n",
            "Epoch 26/50\n",
            "  Training Loss: 1.0301\n",
            "Epoch 27/50\n",
            "  Training Loss: 1.0263\n",
            "Epoch 28/50\n",
            "  Training Loss: 1.0237\n",
            "Epoch 29/50\n",
            "  Training Loss: 1.0244\n",
            "Epoch 30/50\n",
            "  Training Loss: 1.0234\n",
            "new_user AUC: 0.5099, PR-AUC: 0.5117\n",
            "old_user AUC: 0.5251, PR-AUC: 0.5286\n",
            "Epoch 31/50\n",
            "  Training Loss: 1.0240\n",
            "Epoch 32/50\n",
            "  Training Loss: 1.0209\n",
            "Epoch 33/50\n",
            "  Training Loss: 1.0221\n",
            "Epoch 34/50\n",
            "  Training Loss: 1.0235\n",
            "Epoch 35/50\n",
            "  Training Loss: 1.0195\n",
            "Epoch 36/50\n",
            "  Training Loss: 1.0204\n",
            "Epoch 37/50\n",
            "  Training Loss: 1.0227\n",
            "Epoch 38/50\n",
            "  Training Loss: 1.0211\n",
            "Epoch 39/50\n",
            "  Training Loss: 1.0175\n",
            "Epoch 40/50\n",
            "  Training Loss: 1.0170\n",
            "new_user AUC: 0.5137, PR-AUC: 0.5152\n",
            "old_user AUC: 0.5261, PR-AUC: 0.5272\n",
            "Epoch 41/50\n",
            "  Training Loss: 1.0180\n",
            "Epoch 42/50\n",
            "  Training Loss: 1.0172\n",
            "Epoch 43/50\n",
            "  Training Loss: 1.0094\n",
            "Epoch 44/50\n",
            "  Training Loss: 1.0120\n",
            "Epoch 45/50\n",
            "  Training Loss: 1.0106\n",
            "Epoch 46/50\n",
            "  Training Loss: 1.0099\n",
            "Epoch 47/50\n",
            "  Training Loss: 1.0130\n",
            "Epoch 48/50\n",
            "  Training Loss: 1.0120\n",
            "Epoch 49/50\n",
            "  Training Loss: 1.0131\n",
            "Epoch 50/50\n",
            "  Training Loss: 1.0110\n",
            "new_user AUC: 0.5097, PR-AUC: 0.5115\n",
            "old_user AUC: 0.5250, PR-AUC: 0.5245\n",
            "new_user AUC: 0.5097, PR-AUC: 0.5115\n",
            "old_user AUC: 0.5250, PR-AUC: 0.5245\n",
            "Final Test Results:\n",
            "<class 'str'>\n",
            "New_user - AUC: 0.5097, PR-AUC: 0.5115\n",
            "<class 'str'>\n",
            "Old_user - AUC: 0.5250, PR-AUC: 0.5245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 统一模型构建方案\n",
        "\n",
        "---\n",
        "\n",
        "#### 一、架构设计：层次化多专家蒸馏网络\n",
        "**1. 输入层特征工程**\n",
        "- **基础特征**：合并新老用户特征，标准化处理：\n",
        "  - 用户类型（新/老）作为显式二值特征\n",
        "  - 动态字段映射：对存在差异的特征字段（如新用户缺少年度消费次数），使用零值填充并添加缺失标识\n",
        "- **教师信号特征**：将A/B/C/D模型的预测概率作为辅助输入特征\n",
        "\n",
        "**2. 核心网络结构**\n",
        "```mermaid\n",
        "\n",
        "graph TD\n",
        "    A[输入层] --> B[双向门控共享层\\nBi-Gated Dense Block]\n",
        "    B --> C[新用户专家通道\\nGBDT特征蒸馏]\n",
        "    B --> D[老用户专家通道\\nGBDT特征蒸馏]\n",
        "    C --> E[嫌疑预测任务头\\nSigmoid输出]\n",
        "    D --> F[嫌疑预测任务头\\nSigmoid输出]\n",
        "    E --> G[动态权重融合层\\nAttention]\n",
        "    F --> G\n",
        "    G --> H[最终拒付预测\\nSigmoid输出]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### 二、关键技术实现\n",
        "\n",
        "**1. 双向门控共享层**\n",
        "- **门控机制**：根据用户类型动态激活通道\n",
        "  ```python\n",
        "  # 伪代码示例\n",
        "  gate_new = σ(W_g * concat(输入特征, 用户类型标识))\n",
        "  shared_feature = gate_new * Dense_New(输入特征) + (1-gate_new) * Dense_Old(输入特征)\n",
        "  ```\n",
        "\n",
        "- **蒸馏约束**：添加KL散度损失，强制专家通道的 logits 匹配教师模型的 logits\n",
        "\n",
        "**2. 专家通道设计**\n",
        "- **新用户通道**：\n",
        "  - 输入：共享层输出 + 模型 A/B 预测概率\n",
        "  - 子结构：两个全连接网络，分别进行嫌疑预测和拒付预测\n",
        "  - 损失函数：交叉熵损失 （嫌疑预测）和 Focal Loss （拒付预测）\n",
        "- **老用户通道**：\n",
        "  - 输入：共享层输出 + 模型 C/D 预测概率\n",
        "  - 子结构：两个全连接网络，分别进行嫌疑预测和拒付预测\n",
        "  - 损失函数：Focal Loss（用于嫌疑预测和拒付预测）\n",
        "\n",
        "**3. 注意力融合层**\n",
        "- **跨通道注意力**：\n",
        "  ```python\n",
        "  attention_score = Softmax(MLP(concat(新用户嫌疑预测, 新用户拒付预测, 老用户嫌疑预测, 老用户拒付预测)))\n",
        "  fused_feature = attention_score[0] * 新用户预测 + attention_score[1] * 老用户预测\n",
        "  ```\n",
        "\n",
        "- **任务一致性约束**：添加余弦相似度损失项，确保新老用户的嫌疑预测与最终拒付预测的 logits 方向一致\n",
        "\n",
        "---\n",
        "\n",
        "#### 三、训练策略设计\n",
        "\n",
        "\n",
        "**1. 训练流程**\n",
        "\n",
        "端到端联合训练\n",
        "\n",
        "\n",
        "**2. 损失函数设计**\n",
        "\n",
        "$$L_{total} = \\alpha L_{suspect} + \\beta L_{chargeback} + \\gamma L_{distill} + \\lambda L_{consistency} + \\theta L_{task\\_consistency}$$\n",
        "\n",
        "- $L_{suspect}$：嫌疑预测损失（新用户 BCE / 老用户 Focal Loss）\n",
        "- $L_{chargeback}$：拒付预测损失（Focal Loss）\n",
        "- $L_{distill}$：教师模型 KL 散度损失（温度缩放 T=3）\n",
        "- $L_{consistency}$：新老用户各自的嫌疑预测和拒付预测之间的一致性损失（MSE）\n",
        "- $L_{task_consistency}$：任务一致性损失（新老用户嫌疑预测与最终拒付预测 logits 的余弦相似度损失）\n",
        "\n",
        "\n",
        "**3. 动态权重调整**\n",
        "\n",
        "使用固定的超参数：$\\alpha$、$\\beta$、$\\gamma$、$\\lambda$、$\\theta$\n",
        "\n",
        "---\n",
        "\n",
        "#### 四、数据流与特征增强\n",
        "\n",
        "**1. 影子样本生成**\n",
        "\n",
        "（代码中未使用）\n",
        "\n",
        "**2. 时间序列增强**\n",
        "\n",
        "（代码中未使用）\n",
        "\n",
        "---\n",
        "\n",
        "#### 五、部署与监控方案\n",
        "\n",
        "**1. 渐进式上线策略**\n",
        "\n",
        "（代码中未使用）\n",
        "\n",
        "**2. 实时监控看板**\n",
        "\n",
        "（代码中未使用）\n",
        "\n",
        "---\n",
        "\n",
        "#### 六、预期效果与优势\n",
        "1. **效果提升**：\n",
        "   - 新用户风险识别能力提升\n",
        "   - 老用户拒付召回率提升\n",
        "   - 模型推理效率提升\n",
        "\n",
        "2. **技术优势**：\n",
        "   - 通过门控机制实现新老用户的动态区分和特征共享\n",
        "   - 注意力融合层实现新老用户预测结果的动态加权融合\n",
        "   - 知识蒸馏从教师模型迁移知识，提升学生模型性能\n",
        "   - 一致性损失约束模型学习到更加一致的表征\n",
        "   - 任务一致性损失约束模型学习到相关性更高的特征表示\n",
        "\n",
        "该方案通过结合门控机制、注意力融合和知识蒸馏等技术，利用新老用户数据和教师模型知识，增强模型的风险识别能力和泛化性。\n"
      ],
      "metadata": {
        "id": "YoJJQYLc9Sbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 统一风险模型改进方案\n",
        "\n",
        "在现有模型基础上，通过以下维度提升模型性能和鲁棒性：\n",
        "\n",
        "---\n",
        "\n",
        "## 一、训练优化\n",
        "\n",
        "### 1. 三阶段训练流程\n",
        "| 阶段      | 训练目标                          | 数据使用策略                     | 关键操作                                                                 |\n",
        "|-----------|-----------------------------------|----------------------------------|--------------------------------------------------------------------------|\n",
        "| 预训练    | 专家通道独立初始化                | 新/老用户数据隔离                | - 分别训练新老用户专家通道<br>- 使用简单损失函数                         |\n",
        "| 联合训练  | 共享层+任务头端到端优化           | 全量数据混合，动态批次平衡       | - 混合新老用户数据<br>- 引入动态批次平衡策略<br>- 完整损失函数端到端训练 |\n",
        "| 微调      | 注意力融合层+最终层调优           | 添加拒绝推断扩展数据             | - 重点调整融合层参数<br>- 引入拒绝推断数据<br>- 小学习率精细调参         |\n",
        "\n",
        "### 2. 动态权重调整\n",
        "**自适应机制：**\n",
        "```python\n",
        "alpha = 1 - (当前epoch / max_epoch)  # 逐步降低嫌疑预测权重\n",
        "beta = 0.5 * (当前拒付召回率 / 目标召回率)\n",
        "```\n",
        "\n",
        "**参数调整策略：**\n",
        "- 早期阶段：提高蒸馏损失权重（gamma↑）\n",
        "- 后期阶段：增强一致性损失权重（lambda↑）\n",
        "- 召回率不足时：提升拒付损失权重（beta↑）\n",
        "\n",
        "---\n",
        "\n",
        "## 二、数据增强方案\n",
        "\n",
        "### 1. 影子样本生成\n",
        "**实现方法：**\n",
        "```text\n",
        "WGAN-GP生成器 → 教师模型标注 → 加入训练集\n",
        "```\n",
        "**优势：**\n",
        "- 生成新老用户边界样本\n",
        "- 利用教师模型知识自动标注\n",
        "- 提升模型鲁棒性\n",
        "\n",
        "### 2. 时序特征增强（老用户专属）\n",
        "**实施流程：**\n",
        "```\n",
        "交易时序数据 → Transformer编码 → 特征拼接\n",
        "```\n",
        "**特征维度：**\n",
        "- 30天交易频率\n",
        "- 金额波动序列\n",
        "- 历史拒付次数\n",
        "\n",
        "**技术优势：**\n",
        "- 捕捉长期行为模式\n",
        "- 增强静态特征表达能力\n",
        "\n",
        "---\n",
        "\n",
        "## 三、部署与监控\n",
        "\n",
        "### 1. 渐进式上线策略\n",
        "| 用户类型 | 阶段1权重          | 阶段2权重          | 阶段3权重          |\n",
        "|----------|--------------------|--------------------|--------------------|\n",
        "| 新用户   | 模型A:70%          | 统一模型:100%      | -                  |\n",
        "| 老用户   | 模型D:50%          | 统一模型:80%       | 统一模型:100%      |\n",
        "\n",
        "**演进特点：**\n",
        "- 分阶段流量切换\n",
        "- 异常快速回滚能力\n",
        "- 新旧模型无缝过渡\n",
        "\n",
        "### 2. 实时监控体系\n",
        "**核心指标：**\n",
        "```text\n",
        "新用户误杀率（A/B测试）\n",
        "老用户召回率提升幅度\n",
        "跨国特征PSI指数（阈值<0.15）\n",
        "```\n",
        "\n",
        "**熔断机制：**\n",
        "```python\n",
        "if 连续5批次预测方差 > 阈值:\n",
        "    自动切换至教师模型组合\n",
        "```\n",
        "\n",
        "**监控能力：**\n",
        "- 实时特征漂移检测\n",
        "- 多维度性能对比\n",
        "- 自动化异常处理\n",
        "\n",
        "---\n",
        "\n",
        "## 四、预期收益\n",
        "1. **性能提升**：通过时序特征增强预计提升老用户召回率3-5%\n",
        "2. **稳定性增强**：影子样本机制可使模型对抗性样本误判率降低40%+\n",
        "3. **部署安全**：渐进式上线策略将新模型风险暴露降低70%\n",
        "4. **可解释性**：动态注意力权重提供风险决策依据\n"
      ],
      "metadata": {
        "id": "a1s2_ekEFssR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qOP_9txi9Wtq"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}