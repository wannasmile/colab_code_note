{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9xJ0K2SW8evGbhIgZ0Xgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/Pytorch014.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzGxc8U6xeyc",
        "outputId": "c9330942-7923-48c0-cd1f-e9e68ba65401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], d_loss: 0.297884, g_loss: 2.513688 D real: 0.879271, D fake: 0.066907\n",
            "Epoch [0/100], d_loss: 0.481741, g_loss: 2.282193 D real: 0.965257, D fake: 0.295282\n",
            "Epoch [0/100], d_loss: 0.813101, g_loss: 1.823182 D real: 0.659371, D fake: 0.200225\n",
            "Epoch [0/100], d_loss: 0.924667, g_loss: 0.948327 D real: 0.549454, D fake: 0.126662\n",
            "Epoch [1/100], d_loss: 0.756264, g_loss: 2.951166 D real: 0.894572, D fake: 0.420585\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "\n",
        "if not os.path.exists('./dc_img'):\n",
        "    os.mkdir('./dc_img')\n",
        "\n",
        "\n",
        "def to_img(x):\n",
        "    out = 0.5 * (x + 1)\n",
        "    out = out.clamp(0, 1)\n",
        "    out = out.view(-1, 1, 28, 28)\n",
        "    return out\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_epoch = 100\n",
        "z_dimension = 100  # noise dimension\n",
        "\n",
        "#img_transform = transforms.Compose([\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#])\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist = datasets.MNIST('./data', transform=img_transform, download=True)\n",
        "dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True,\n",
        "                        num_workers=4)\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 5, padding=2),  # batch, 32, 28, 28\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.AvgPool2d(2, stride=2),  # batch, 32, 14, 14\n",
        "            )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 5, padding=2),  # batch, 64, 14, 14\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.AvgPool2d(2, stride=2)  # batch, 64, 7, 7\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*7*7, 1024),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1024, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: batch, width, height, channel=1\n",
        "        '''\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class generator(nn.Module):\n",
        "    def __init__(self, input_size, num_feature):\n",
        "        super(generator, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, num_feature)  # batch, 3136=1x56x56\n",
        "        self.br = nn.Sequential(\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 50, 3, stride=1, padding=1),  # batch, 50, 56, 56\n",
        "            nn.BatchNorm2d(50),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            nn.Conv2d(50, 25, 3, stride=1, padding=1),  # batch, 25, 56, 56\n",
        "            nn.BatchNorm2d(25),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            nn.Conv2d(25, 1, 2, stride=2),  # batch, 1, 28, 28\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), 1, 56, 56)\n",
        "        x = self.br(x)\n",
        "        x = self.downsample1(x)\n",
        "        x = self.downsample2(x)\n",
        "        x = self.downsample3(x)\n",
        "        return x\n",
        "\n",
        "D = discriminator()  # discriminator model\n",
        "G = generator(z_dimension, 3136)  # generator model\n",
        "if torch.cuda.is_available():\n",
        "    D = D.cuda()\n",
        "    G = G.cuda()\n",
        "\n",
        "#D = discriminator().cuda()  # discriminator model\n",
        "#G = generator(z_dimension, 3136).cuda()  # generator model\n",
        "\n",
        "criterion = nn.BCELoss()  # binary cross entropy\n",
        "\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)\n",
        "\n",
        "# train\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (img, _) in enumerate(dataloader):\n",
        "        num_img = img.size(0)\n",
        "        # =================train discriminator\n",
        "        #real_img = Variable(img).cuda()\n",
        "        #real_label = Variable(torch.ones(num_img)).cuda()\n",
        "        #fake_label = Variable(torch.zeros(num_img)).cuda()\n",
        "        real_img = Variable(img)\n",
        "        real_label = Variable(torch.ones(num_img))\n",
        "        fake_label = Variable(torch.zeros(num_img))\n",
        "        if torch.cuda.is_available():\n",
        "            real_img = real_img.cuda()\n",
        "            real_label = real_label.cuda()\n",
        "            fake_label = fake_label.cuda()\n",
        "\n",
        "\n",
        "        # compute loss of real_img\n",
        "        real_out = D(real_img)\n",
        "        #ValueError: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
        "        real_label = real_label.unsqueeze(1)\n",
        "        d_loss_real = criterion(real_out, real_label)\n",
        "        real_scores = real_out  # closer to 1 means better\n",
        "\n",
        "        # compute loss of fake_img\n",
        "        #z = Variable(torch.randn(num_img, z_dimension)).cuda()\n",
        "        z = Variable(torch.randn(num_img, z_dimension))\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "        fake_img = G(z)\n",
        "        fake_out = D(fake_img)\n",
        "        #ValueError: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
        "        fake_label = fake_label.unsqueeze(1)\n",
        "        d_loss_fake = criterion(fake_out, fake_label)\n",
        "        fake_scores = fake_out  # closer to 0 means better\n",
        "\n",
        "        # bp and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_optimizer.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # =================train generator\n",
        "        # compute loss of fake_img\n",
        "        #z = Variable(torch.randn(num_img, z_dimension)).cuda()\n",
        "        z = Variable(torch.randn(num_img, z_dimension))\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "        fake_img = G(z)\n",
        "        output = D(fake_img)\n",
        "        g_loss = criterion(output, real_label)\n",
        "\n",
        "        # bp and optimize\n",
        "        g_optimizer.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], d_loss: {:.6f}, g_loss: {:.6f} '\n",
        "                  'D real: {:.6f}, D fake: {:.6f}'\n",
        "                  #.format(epoch, num_epoch, d_loss.data[0], g_loss.data[0],\n",
        "                  .format(epoch, num_epoch, d_loss.item(), g_loss.item(),\n",
        "                          real_scores.data.mean(), fake_scores.data.mean()))\n",
        "    if epoch == 0:\n",
        "        real_images = to_img(real_img.cpu().data)\n",
        "        save_image(real_images, './dc_img/real_images.png')\n",
        "\n",
        "    fake_images = to_img(fake_img.cpu().data)\n",
        "    save_image(fake_images, './dc_img/fake_images-{}.png'.format(epoch+1))\n",
        "\n",
        "torch.save(G.state_dict(), './generator.pth')\n",
        "torch.save(D.state_dict(), './discriminator.pth')"
      ]
    }
  ]
}