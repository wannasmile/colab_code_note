{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI01CYnKZNvFSaGbA5Xbue",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/Pytorch009.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbl5YLSp1uMH",
        "outputId": "637b1d24-b1cd-4b11-9b29-b1660a42dda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "epoch 1\n",
            "Loss: 1.0968608260154724\n",
            "**********\n",
            "epoch 2\n",
            "Loss: 1.0926524996757507\n",
            "**********\n",
            "epoch 3\n",
            "Loss: 1.0884774923324585\n",
            "**********\n",
            "epoch 4\n",
            "Loss: 1.0843341946601868\n",
            "**********\n",
            "epoch 5\n",
            "Loss: 1.0802211165428162\n",
            "**********\n",
            "epoch 6\n",
            "Loss: 1.0761367678642273\n",
            "**********\n",
            "epoch 7\n",
            "Loss: 1.0720799565315247\n",
            "**********\n",
            "epoch 8\n",
            "Loss: 1.0680488348007202\n",
            "**********\n",
            "epoch 9\n",
            "Loss: 1.0640424489974976\n",
            "**********\n",
            "epoch 10\n",
            "Loss: 1.0600594282150269\n",
            "**********\n",
            "epoch 11\n",
            "Loss: 1.0560985207557678\n",
            "**********\n",
            "epoch 12\n",
            "Loss: 1.0521583557128906\n",
            "**********\n",
            "epoch 13\n",
            "Loss: 1.0482377409934998\n",
            "**********\n",
            "epoch 14\n",
            "Loss: 1.0443356037139893\n",
            "**********\n",
            "epoch 15\n",
            "Loss: 1.0404508709907532\n",
            "**********\n",
            "epoch 16\n",
            "Loss: 1.0365821719169617\n",
            "**********\n",
            "epoch 17\n",
            "Loss: 1.0327286124229431\n",
            "**********\n",
            "epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-75081d979bf4>:68: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  y = F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.0288891792297363\n",
            "**********\n",
            "epoch 19\n",
            "Loss: 1.0250626802444458\n",
            "**********\n",
            "epoch 20\n",
            "Loss: 1.0212481915950775\n",
            "**********\n",
            "epoch 21\n",
            "Loss: 1.0174448490142822\n",
            "**********\n",
            "epoch 22\n",
            "Loss: 1.0136515498161316\n",
            "**********\n",
            "epoch 23\n",
            "Loss: 1.0098674297332764\n",
            "**********\n",
            "epoch 24\n",
            "Loss: 1.0060915350914001\n",
            "**********\n",
            "epoch 25\n",
            "Loss: 1.0023231208324432\n",
            "**********\n",
            "epoch 26\n",
            "Loss: 0.9985613226890564\n",
            "**********\n",
            "epoch 27\n",
            "Loss: 0.9948052167892456\n",
            "**********\n",
            "epoch 28\n",
            "Loss: 0.9910540878772736\n",
            "**********\n",
            "epoch 29\n",
            "Loss: 0.9873071014881134\n",
            "**********\n",
            "epoch 30\n",
            "Loss: 0.9835636019706726\n",
            "**********\n",
            "epoch 31\n",
            "Loss: 0.9798227548599243\n",
            "**********\n",
            "epoch 32\n",
            "Loss: 0.9760839939117432\n",
            "**********\n",
            "epoch 33\n",
            "Loss: 0.9723464548587799\n",
            "**********\n",
            "epoch 34\n",
            "Loss: 0.9686095714569092\n",
            "**********\n",
            "epoch 35\n",
            "Loss: 0.9648728370666504\n",
            "**********\n",
            "epoch 36\n",
            "Loss: 0.9611354768276215\n",
            "**********\n",
            "epoch 37\n",
            "Loss: 0.9573970139026642\n",
            "**********\n",
            "epoch 38\n",
            "Loss: 0.9536568522453308\n",
            "**********\n",
            "epoch 39\n",
            "Loss: 0.9499145150184631\n",
            "**********\n",
            "epoch 40\n",
            "Loss: 0.9461693167686462\n",
            "**********\n",
            "epoch 41\n",
            "Loss: 0.942421019077301\n",
            "**********\n",
            "epoch 42\n",
            "Loss: 0.938668966293335\n",
            "**********\n",
            "epoch 43\n",
            "Loss: 0.9349128603935242\n",
            "**********\n",
            "epoch 44\n",
            "Loss: 0.9311522543430328\n",
            "**********\n",
            "epoch 45\n",
            "Loss: 0.927386611700058\n",
            "**********\n",
            "epoch 46\n",
            "Loss: 0.9236159026622772\n",
            "**********\n",
            "epoch 47\n",
            "Loss: 0.9198395013809204\n",
            "**********\n",
            "epoch 48\n",
            "Loss: 0.9160572588443756\n",
            "**********\n",
            "epoch 49\n",
            "Loss: 0.9122687876224518\n",
            "**********\n",
            "epoch 50\n",
            "Loss: 0.9084738492965698\n",
            "**********\n",
            "epoch 51\n",
            "Loss: 0.904672235250473\n",
            "**********\n",
            "epoch 52\n",
            "Loss: 0.9008638262748718\n",
            "**********\n",
            "epoch 53\n",
            "Loss: 0.897048145532608\n",
            "**********\n",
            "epoch 54\n",
            "Loss: 0.8932252526283264\n",
            "**********\n",
            "epoch 55\n",
            "Loss: 0.889394998550415\n",
            "**********\n",
            "epoch 56\n",
            "Loss: 0.8855571150779724\n",
            "**********\n",
            "epoch 57\n",
            "Loss: 0.8817115128040314\n",
            "**********\n",
            "epoch 58\n",
            "Loss: 0.8778582811355591\n",
            "**********\n",
            "epoch 59\n",
            "Loss: 0.8739970922470093\n",
            "**********\n",
            "epoch 60\n",
            "Loss: 0.8701279759407043\n",
            "**********\n",
            "epoch 61\n",
            "Loss: 0.8662508726119995\n",
            "**********\n",
            "epoch 62\n",
            "Loss: 0.8623658716678619\n",
            "**********\n",
            "epoch 63\n",
            "Loss: 0.8584727644920349\n",
            "**********\n",
            "epoch 64\n",
            "Loss: 0.8545717000961304\n",
            "**********\n",
            "epoch 65\n",
            "Loss: 0.8506624400615692\n",
            "**********\n",
            "epoch 66\n",
            "Loss: 0.8467452824115753\n",
            "**********\n",
            "epoch 67\n",
            "Loss: 0.8428201675415039\n",
            "**********\n",
            "epoch 68\n",
            "Loss: 0.8388870656490326\n",
            "**********\n",
            "epoch 69\n",
            "Loss: 0.8349461257457733\n",
            "**********\n",
            "epoch 70\n",
            "Loss: 0.8309971988201141\n",
            "**********\n",
            "epoch 71\n",
            "Loss: 0.8270406126976013\n",
            "**********\n",
            "epoch 72\n",
            "Loss: 0.8230762481689453\n",
            "**********\n",
            "epoch 73\n",
            "Loss: 0.8191043436527252\n",
            "**********\n",
            "epoch 74\n",
            "Loss: 0.8151247799396515\n",
            "**********\n",
            "epoch 75\n",
            "Loss: 0.8111379146575928\n",
            "**********\n",
            "epoch 76\n",
            "Loss: 0.8071436583995819\n",
            "**********\n",
            "epoch 77\n",
            "Loss: 0.8031422197818756\n",
            "**********\n",
            "epoch 78\n",
            "Loss: 0.7991336584091187\n",
            "**********\n",
            "epoch 79\n",
            "Loss: 0.795118123292923\n",
            "**********\n",
            "epoch 80\n",
            "Loss: 0.7910957932472229\n",
            "**********\n",
            "epoch 81\n",
            "Loss: 0.7870668172836304\n",
            "**********\n",
            "epoch 82\n",
            "Loss: 0.7830312848091125\n",
            "**********\n",
            "epoch 83\n",
            "Loss: 0.7789894342422485\n",
            "**********\n",
            "epoch 84\n",
            "Loss: 0.7749413251876831\n",
            "**********\n",
            "epoch 85\n",
            "Loss: 0.7708872258663177\n",
            "**********\n",
            "epoch 86\n",
            "Loss: 0.766827255487442\n",
            "**********\n",
            "epoch 87\n",
            "Loss: 0.7627617716789246\n",
            "**********\n",
            "epoch 88\n",
            "Loss: 0.758690744638443\n",
            "**********\n",
            "epoch 89\n",
            "Loss: 0.754614531993866\n",
            "**********\n",
            "epoch 90\n",
            "Loss: 0.7505332827568054\n",
            "**********\n",
            "epoch 91\n",
            "Loss: 0.7464472055435181\n",
            "**********\n",
            "epoch 92\n",
            "Loss: 0.7423566281795502\n",
            "**********\n",
            "epoch 93\n",
            "Loss: 0.7382617294788361\n",
            "**********\n",
            "epoch 94\n",
            "Loss: 0.7341627478599548\n",
            "**********\n",
            "epoch 95\n",
            "Loss: 0.7300599217414856\n",
            "**********\n",
            "epoch 96\n",
            "Loss: 0.7259535193443298\n",
            "**********\n",
            "epoch 97\n",
            "Loss: 0.7218438684940338\n",
            "**********\n",
            "epoch 98\n",
            "Loss: 0.7177311182022095\n",
            "**********\n",
            "epoch 99\n",
            "Loss: 0.7136157155036926\n",
            "**********\n",
            "epoch 100\n",
            "Loss: 0.7094977498054504\n",
            "**********\n",
            "epoch 101\n",
            "Loss: 0.7053776681423187\n",
            "**********\n",
            "epoch 102\n",
            "Loss: 0.7012557685375214\n",
            "**********\n",
            "epoch 103\n",
            "Loss: 0.6971322298049927\n",
            "**********\n",
            "epoch 104\n",
            "Loss: 0.6930074691772461\n",
            "**********\n",
            "epoch 105\n",
            "Loss: 0.6888818144798279\n",
            "**********\n",
            "epoch 106\n",
            "Loss: 0.6847554743289948\n",
            "**********\n",
            "epoch 107\n",
            "Loss: 0.6806288063526154\n",
            "**********\n",
            "epoch 108\n",
            "Loss: 0.6765023171901703\n",
            "**********\n",
            "epoch 109\n",
            "Loss: 0.6723761260509491\n",
            "**********\n",
            "epoch 110\n",
            "Loss: 0.6682506501674652\n",
            "**********\n",
            "epoch 111\n",
            "Loss: 0.6641262769699097\n",
            "**********\n",
            "epoch 112\n",
            "Loss: 0.6600033342838287\n",
            "**********\n",
            "epoch 113\n",
            "Loss: 0.6558821201324463\n",
            "**********\n",
            "epoch 114\n",
            "Loss: 0.6517630815505981\n",
            "**********\n",
            "epoch 115\n",
            "Loss: 0.6476465463638306\n",
            "**********\n",
            "epoch 116\n",
            "Loss: 0.643532782793045\n",
            "**********\n",
            "epoch 117\n",
            "Loss: 0.6394222974777222\n",
            "**********\n",
            "epoch 118\n",
            "Loss: 0.6353153884410858\n",
            "**********\n",
            "epoch 119\n",
            "Loss: 0.631212443113327\n",
            "**********\n",
            "epoch 120\n",
            "Loss: 0.6271138489246368\n",
            "**********\n",
            "epoch 121\n",
            "Loss: 0.6230199038982391\n",
            "**********\n",
            "epoch 122\n",
            "Loss: 0.6189310550689697\n",
            "**********\n",
            "epoch 123\n",
            "Loss: 0.6148476898670197\n",
            "**********\n",
            "epoch 124\n",
            "Loss: 0.6107701659202576\n",
            "**********\n",
            "epoch 125\n",
            "Loss: 0.6066988110542297\n",
            "**********\n",
            "epoch 126\n",
            "Loss: 0.6026340425014496\n",
            "**********\n",
            "epoch 127\n",
            "Loss: 0.5985762774944305\n",
            "**********\n",
            "epoch 128\n",
            "Loss: 0.594525933265686\n",
            "**********\n",
            "epoch 129\n",
            "Loss: 0.5904831886291504\n",
            "**********\n",
            "epoch 130\n",
            "Loss: 0.5864486694335938\n",
            "**********\n",
            "epoch 131\n",
            "Loss: 0.5824225842952728\n",
            "**********\n",
            "epoch 132\n",
            "Loss: 0.578405350446701\n",
            "**********\n",
            "epoch 133\n",
            "Loss: 0.5743973255157471\n",
            "**********\n",
            "epoch 134\n",
            "Loss: 0.5703989565372467\n",
            "**********\n",
            "epoch 135\n",
            "Loss: 0.5664105415344238\n",
            "**********\n",
            "epoch 136\n",
            "Loss: 0.5624325275421143\n",
            "**********\n",
            "epoch 137\n",
            "Loss: 0.5584652125835419\n",
            "**********\n",
            "epoch 138\n",
            "Loss: 0.5545089244842529\n",
            "**********\n",
            "epoch 139\n",
            "Loss: 0.5505640953779221\n",
            "**********\n",
            "epoch 140\n",
            "Loss: 0.5466310381889343\n",
            "**********\n",
            "epoch 141\n",
            "Loss: 0.5427102148532867\n",
            "**********\n",
            "epoch 142\n",
            "Loss: 0.5388018637895584\n",
            "**********\n",
            "epoch 143\n",
            "Loss: 0.5349062532186508\n",
            "**********\n",
            "epoch 144\n",
            "Loss: 0.5310239344835281\n",
            "**********\n",
            "epoch 145\n",
            "Loss: 0.5271551311016083\n",
            "**********\n",
            "epoch 146\n",
            "Loss: 0.5233001559972763\n",
            "**********\n",
            "epoch 147\n",
            "Loss: 0.5194593816995621\n",
            "**********\n",
            "epoch 148\n",
            "Loss: 0.5156331658363342\n",
            "**********\n",
            "epoch 149\n",
            "Loss: 0.5118216574192047\n",
            "**********\n",
            "epoch 150\n",
            "Loss: 0.5080254226922989\n",
            "**********\n",
            "epoch 151\n",
            "Loss: 0.5042445510625839\n",
            "**********\n",
            "epoch 152\n",
            "Loss: 0.500479519367218\n",
            "**********\n",
            "epoch 153\n",
            "Loss: 0.4967304766178131\n",
            "**********\n",
            "epoch 154\n",
            "Loss: 0.49299775063991547\n",
            "**********\n",
            "epoch 155\n",
            "Loss: 0.48928169906139374\n",
            "**********\n",
            "epoch 156\n",
            "Loss: 0.485582560300827\n",
            "**********\n",
            "epoch 157\n",
            "Loss: 0.4819005876779556\n",
            "**********\n",
            "epoch 158\n",
            "Loss: 0.47823601961135864\n",
            "**********\n",
            "epoch 159\n",
            "Loss: 0.4745892137289047\n",
            "**********\n",
            "epoch 160\n",
            "Loss: 0.4709603488445282\n",
            "**********\n",
            "epoch 161\n",
            "Loss: 0.46734970808029175\n",
            "**********\n",
            "epoch 162\n",
            "Loss: 0.4637574255466461\n",
            "**********\n",
            "epoch 163\n",
            "Loss: 0.46018391847610474\n",
            "**********\n",
            "epoch 164\n",
            "Loss: 0.45662927627563477\n",
            "**********\n",
            "epoch 165\n",
            "Loss: 0.45309367775917053\n",
            "**********\n",
            "epoch 166\n",
            "Loss: 0.44957752525806427\n",
            "**********\n",
            "epoch 167\n",
            "Loss: 0.44608086347579956\n",
            "**********\n",
            "epoch 168\n",
            "Loss: 0.4426039159297943\n",
            "**********\n",
            "epoch 169\n",
            "Loss: 0.43914687633514404\n",
            "**********\n",
            "epoch 170\n",
            "Loss: 0.43570996820926666\n",
            "**********\n",
            "epoch 171\n",
            "Loss: 0.4322933405637741\n",
            "**********\n",
            "epoch 172\n",
            "Loss: 0.4288971424102783\n",
            "**********\n",
            "epoch 173\n",
            "Loss: 0.425521582365036\n",
            "**********\n",
            "epoch 174\n",
            "Loss: 0.42216674983501434\n",
            "**********\n",
            "epoch 175\n",
            "Loss: 0.41883283853530884\n",
            "**********\n",
            "epoch 176\n",
            "Loss: 0.41551996767520905\n",
            "**********\n",
            "epoch 177\n",
            "Loss: 0.4122282713651657\n",
            "**********\n",
            "epoch 178\n",
            "Loss: 0.40895789861679077\n",
            "**********\n",
            "epoch 179\n",
            "Loss: 0.4057089239358902\n",
            "**********\n",
            "epoch 180\n",
            "Loss: 0.40248143672943115\n",
            "**********\n",
            "epoch 181\n",
            "Loss: 0.39927563071250916\n",
            "**********\n",
            "epoch 182\n",
            "Loss: 0.39609159529209137\n",
            "**********\n",
            "epoch 183\n",
            "Loss: 0.3929292857646942\n",
            "**********\n",
            "epoch 184\n",
            "Loss: 0.389788955450058\n",
            "**********\n",
            "epoch 185\n",
            "Loss: 0.3866705596446991\n",
            "**********\n",
            "epoch 186\n",
            "Loss: 0.3835742026567459\n",
            "**********\n",
            "epoch 187\n",
            "Loss: 0.3804999738931656\n",
            "**********\n",
            "epoch 188\n",
            "Loss: 0.3774479031562805\n",
            "**********\n",
            "epoch 189\n",
            "Loss: 0.3744180351495743\n",
            "**********\n",
            "epoch 190\n",
            "Loss: 0.37141041457653046\n",
            "**********\n",
            "epoch 191\n",
            "Loss: 0.36842504143714905\n",
            "**********\n",
            "epoch 192\n",
            "Loss: 0.3654620200395584\n",
            "**********\n",
            "epoch 193\n",
            "Loss: 0.36252133548259735\n",
            "**********\n",
            "epoch 194\n",
            "Loss: 0.35960301756858826\n",
            "**********\n",
            "epoch 195\n",
            "Loss: 0.35670703649520874\n",
            "**********\n",
            "epoch 196\n",
            "Loss: 0.3538334518671036\n",
            "**********\n",
            "epoch 197\n",
            "Loss: 0.35098226368427277\n",
            "**********\n",
            "epoch 198\n",
            "Loss: 0.34815338253974915\n",
            "**********\n",
            "epoch 199\n",
            "Loss: 0.34534695744514465\n",
            "**********\n",
            "epoch 200\n",
            "Loss: 0.3425627499818802\n",
            "**********\n",
            "epoch 201\n",
            "Loss: 0.3398009389638901\n",
            "**********\n",
            "epoch 202\n",
            "Loss: 0.33706140518188477\n",
            "**********\n",
            "epoch 203\n",
            "Loss: 0.33434411883354187\n",
            "**********\n",
            "epoch 204\n",
            "Loss: 0.3316490352153778\n",
            "**********\n",
            "epoch 205\n",
            "Loss: 0.3289761394262314\n",
            "**********\n",
            "epoch 206\n",
            "Loss: 0.3263253718614578\n",
            "**********\n",
            "epoch 207\n",
            "Loss: 0.32369670271873474\n",
            "**********\n",
            "epoch 208\n",
            "Loss: 0.32109005749225616\n",
            "**********\n",
            "epoch 209\n",
            "Loss: 0.31850533187389374\n",
            "**********\n",
            "epoch 210\n",
            "Loss: 0.31594257056713104\n",
            "**********\n",
            "epoch 211\n",
            "Loss: 0.31340157985687256\n",
            "**********\n",
            "epoch 212\n",
            "Loss: 0.3108823746442795\n",
            "**********\n",
            "epoch 213\n",
            "Loss: 0.30838488042354584\n",
            "**********\n",
            "epoch 214\n",
            "Loss: 0.3059089481830597\n",
            "**********\n",
            "epoch 215\n",
            "Loss: 0.30345456302165985\n",
            "**********\n",
            "epoch 216\n",
            "Loss: 0.3010215610265732\n",
            "**********\n",
            "epoch 217\n",
            "Loss: 0.2986099123954773\n",
            "**********\n",
            "epoch 218\n",
            "Loss: 0.2962195500731468\n",
            "**********\n",
            "epoch 219\n",
            "Loss: 0.2938503175973892\n",
            "**********\n",
            "epoch 220\n",
            "Loss: 0.29150212556123734\n",
            "**********\n",
            "epoch 221\n",
            "Loss: 0.2891748696565628\n",
            "**********\n",
            "epoch 222\n",
            "Loss: 0.28686849027872086\n",
            "**********\n",
            "epoch 223\n",
            "Loss: 0.28458285331726074\n",
            "**********\n",
            "epoch 224\n",
            "Loss: 0.28231778740882874\n",
            "**********\n",
            "epoch 225\n",
            "Loss: 0.28007324784994125\n",
            "**********\n",
            "epoch 226\n",
            "Loss: 0.27784913778305054\n",
            "**********\n",
            "epoch 227\n",
            "Loss: 0.27564528584480286\n",
            "**********\n",
            "epoch 228\n",
            "Loss: 0.27346158772706985\n",
            "**********\n",
            "epoch 229\n",
            "Loss: 0.2712979018688202\n",
            "**********\n",
            "epoch 230\n",
            "Loss: 0.2691541761159897\n",
            "**********\n",
            "epoch 231\n",
            "Loss: 0.2670302242040634\n",
            "**********\n",
            "epoch 232\n",
            "Loss: 0.26492589712142944\n",
            "**********\n",
            "epoch 233\n",
            "Loss: 0.2628411576151848\n",
            "**********\n",
            "epoch 234\n",
            "Loss: 0.2607758045196533\n",
            "**********\n",
            "epoch 235\n",
            "Loss: 0.2587296888232231\n",
            "**********\n",
            "epoch 236\n",
            "Loss: 0.2567027434706688\n",
            "**********\n",
            "epoch 237\n",
            "Loss: 0.2546948045492172\n",
            "**********\n",
            "epoch 238\n",
            "Loss: 0.2527056485414505\n",
            "**********\n",
            "epoch 239\n",
            "Loss: 0.2507352828979492\n",
            "**********\n",
            "epoch 240\n",
            "Loss: 0.24878349900245667\n",
            "**********\n",
            "epoch 241\n",
            "Loss: 0.24685018509626389\n",
            "**********\n",
            "epoch 242\n",
            "Loss: 0.24493518471717834\n",
            "**********\n",
            "epoch 243\n",
            "Loss: 0.2430383339524269\n",
            "**********\n",
            "epoch 244\n",
            "Loss: 0.24115952104330063\n",
            "**********\n",
            "epoch 245\n",
            "Loss: 0.23929863423109055\n",
            "**********\n",
            "epoch 246\n",
            "Loss: 0.23745545744895935\n",
            "**********\n",
            "epoch 247\n",
            "Loss: 0.2356298863887787\n",
            "**********\n",
            "epoch 248\n",
            "Loss: 0.2338217943906784\n",
            "**********\n",
            "epoch 249\n",
            "Loss: 0.23203103244304657\n",
            "**********\n",
            "epoch 250\n",
            "Loss: 0.23025742173194885\n",
            "**********\n",
            "epoch 251\n",
            "Loss: 0.2285008579492569\n",
            "**********\n",
            "epoch 252\n",
            "Loss: 0.22676114737987518\n",
            "**********\n",
            "epoch 253\n",
            "Loss: 0.22503822296857834\n",
            "**********\n",
            "epoch 254\n",
            "Loss: 0.22333189845085144\n",
            "**********\n",
            "epoch 255\n",
            "Loss: 0.22164203226566315\n",
            "**********\n",
            "epoch 256\n",
            "Loss: 0.21996843814849854\n",
            "**********\n",
            "epoch 257\n",
            "Loss: 0.21831099689006805\n",
            "**********\n",
            "epoch 258\n",
            "Loss: 0.21666959673166275\n",
            "**********\n",
            "epoch 259\n",
            "Loss: 0.21504413336515427\n",
            "**********\n",
            "epoch 260\n",
            "Loss: 0.2134343534708023\n",
            "**********\n",
            "epoch 261\n",
            "Loss: 0.21184015274047852\n",
            "**********\n",
            "epoch 262\n",
            "Loss: 0.21026144921779633\n",
            "**********\n",
            "epoch 263\n",
            "Loss: 0.20869802683591843\n",
            "**********\n",
            "epoch 264\n",
            "Loss: 0.20714975148439407\n",
            "**********\n",
            "epoch 265\n",
            "Loss: 0.2056165114045143\n",
            "**********\n",
            "epoch 266\n",
            "Loss: 0.2040981873869896\n",
            "**********\n",
            "epoch 267\n",
            "Loss: 0.20259454101324081\n",
            "**********\n",
            "epoch 268\n",
            "Loss: 0.2011055052280426\n",
            "**********\n",
            "epoch 269\n",
            "Loss: 0.1996309831738472\n",
            "**********\n",
            "epoch 270\n",
            "Loss: 0.1981707438826561\n",
            "**********\n",
            "epoch 271\n",
            "Loss: 0.19672467559576035\n",
            "**********\n",
            "epoch 272\n",
            "Loss: 0.19529267400503159\n",
            "**********\n",
            "epoch 273\n",
            "Loss: 0.19387461245059967\n",
            "**********\n",
            "epoch 274\n",
            "Loss: 0.1924702748656273\n",
            "**********\n",
            "epoch 275\n",
            "Loss: 0.19107958674430847\n",
            "**********\n",
            "epoch 276\n",
            "Loss: 0.1897023841738701\n",
            "**********\n",
            "epoch 277\n",
            "Loss: 0.18833857774734497\n",
            "**********\n",
            "epoch 278\n",
            "Loss: 0.18698801845312119\n",
            "**********\n",
            "epoch 279\n",
            "Loss: 0.18565048277378082\n",
            "**********\n",
            "epoch 280\n",
            "Loss: 0.18432597070932388\n",
            "**********\n",
            "epoch 281\n",
            "Loss: 0.18301430344581604\n",
            "**********\n",
            "epoch 282\n",
            "Loss: 0.18171532452106476\n",
            "**********\n",
            "epoch 283\n",
            "Loss: 0.18042893707752228\n",
            "**********\n",
            "epoch 284\n",
            "Loss: 0.17915502190589905\n",
            "**********\n",
            "epoch 285\n",
            "Loss: 0.17789337784051895\n",
            "**********\n",
            "epoch 286\n",
            "Loss: 0.176643967628479\n",
            "**********\n",
            "epoch 287\n",
            "Loss: 0.17540659755468369\n",
            "**********\n",
            "epoch 288\n",
            "Loss: 0.17418118566274643\n",
            "**********\n",
            "epoch 289\n",
            "Loss: 0.17296762019395828\n",
            "**********\n",
            "epoch 290\n",
            "Loss: 0.1717657670378685\n",
            "**********\n",
            "epoch 291\n",
            "Loss: 0.17057541757822037\n",
            "**********\n",
            "epoch 292\n",
            "Loss: 0.16939660906791687\n",
            "**********\n",
            "epoch 293\n",
            "Loss: 0.1682290881872177\n",
            "**********\n",
            "epoch 294\n",
            "Loss: 0.16707276552915573\n",
            "**********\n",
            "epoch 295\n",
            "Loss: 0.16592758148908615\n",
            "**********\n",
            "epoch 296\n",
            "Loss: 0.16479339450597763\n",
            "**********\n",
            "epoch 297\n",
            "Loss: 0.16367009282112122\n",
            "**********\n",
            "epoch 298\n",
            "Loss: 0.16255752742290497\n",
            "**********\n",
            "epoch 299\n",
            "Loss: 0.16145556047558784\n",
            "**********\n",
            "epoch 300\n",
            "Loss: 0.16036413237452507\n",
            "\n",
            "tensor([[-3.0469, -0.1043, -2.9647],\n",
            "        [-2.1871, -1.9356, -0.2965],\n",
            "        [-0.2375, -2.1075, -2.4097],\n",
            "        [-2.4545, -0.1260, -3.4261]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "training_data = [(\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])]\n",
        "\n",
        "word_to_idx = {}\n",
        "tag_to_idx = {}\n",
        "for context, tag in training_data:\n",
        "    for word in context:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "    for label in tag:\n",
        "        if label not in tag_to_idx:\n",
        "            tag_to_idx[label] = len(tag_to_idx)\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "character_to_idx = {}\n",
        "for i in range(len(alphabet)):\n",
        "    character_to_idx[alphabet[i]] = i\n",
        "\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, n_char, char_dim, char_hidden):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.char_embedding = nn.Embedding(n_char, char_dim)\n",
        "        self.char_lstm = nn.LSTM(char_dim, char_hidden, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.char_embedding(x)\n",
        "        _, h = self.char_lstm(x)\n",
        "        return h[0]\n",
        "\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden, n_hidden,\n",
        "                 n_tag):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(n_word, n_dim)\n",
        "        self.char_lstm = CharLSTM(n_char, char_dim, char_hidden)\n",
        "        self.lstm = nn.LSTM(n_dim + char_hidden, n_hidden, batch_first=True)\n",
        "        self.linear1 = nn.Linear(n_hidden, n_tag)\n",
        "\n",
        "    def forward(self, x, word):\n",
        "        char = torch.FloatTensor()\n",
        "        for each in word:\n",
        "            char_list = []\n",
        "            for letter in each:\n",
        "                char_list.append(character_to_idx[letter.lower()])\n",
        "            char_list = torch.LongTensor(char_list)\n",
        "            char_list = char_list.unsqueeze(0)\n",
        "            if torch.cuda.is_available():\n",
        "                tempchar = self.char_lstm(Variable(char_list).cuda())\n",
        "            else:\n",
        "                tempchar = self.char_lstm(Variable(char_list))\n",
        "            tempchar = tempchar.squeeze(0)\n",
        "            char = torch.cat((char, tempchar.cpu().data), 0)\n",
        "        if torch.cuda.is_available():\n",
        "            char = char.cuda()\n",
        "        char = Variable(char)\n",
        "        x = self.word_embedding(x)\n",
        "        x = torch.cat((x, char), 1)\n",
        "        x = x.unsqueeze(0)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x.squeeze(0)\n",
        "        x = self.linear1(x)\n",
        "        y = F.log_softmax(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "model = LSTMTagger(len(word_to_idx), len(character_to_idx), 10, 100, 50, 128, len(tag_to_idx))\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "def make_sequence(x, dic):\n",
        "    idx = [dic[i] for i in x]\n",
        "    idx = Variable(torch.LongTensor(idx))\n",
        "    return idx\n",
        "\n",
        "\n",
        "for epoch in range(300):\n",
        "    print('*' * 10)\n",
        "    print('epoch {}'.format(epoch + 1))\n",
        "    running_loss = 0\n",
        "    for data in training_data:\n",
        "        word, tag = data\n",
        "        word_list = make_sequence(word, word_to_idx)\n",
        "        tag = make_sequence(tag, tag_to_idx)\n",
        "        if torch.cuda.is_available():\n",
        "            word_list = word_list.cuda()\n",
        "            tag = tag.cuda()\n",
        "        # forward\n",
        "        out = model(word_list, word)\n",
        "        loss = criterion(out, tag)\n",
        "        #running_loss += loss.data[0]\n",
        "        running_loss += loss.item()\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print('Loss: {}'.format(running_loss / len(data)))\n",
        "print()\n",
        "input = make_sequence(\"Everybody ate the apple\".split(), word_to_idx)\n",
        "if torch.cuda.is_available():\n",
        "    input = input.cuda()\n",
        "\n",
        "out = model(input, \"Everybody ate the apple\".split())\n",
        "print(out)"
      ]
    }
  ]
}