{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3iGp+lDqYspyf4tI2qvC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/Pytorch015.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "soGZbiYIiN0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 输入嵌入\n",
        "\n",
        "    输入嵌入将输入序列中的每个token转换为一个向量。这可以通过使用预训练的词向量（如Word2Vec或GloVe）或从头开始训练嵌入层来实现。\n"
      ],
      "metadata": {
        "id": "0Gwh4jQQnQJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将词嵌入向量乘以根号d_model，以缩小数值范围，这有助于在训练过程中稳定梯度。\n",
        "        return self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))"
      ],
      "metadata": {
        "id": "aZ44jD_Hijhd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将词嵌入向量乘以根号d_model，以缩小数值范围，这有助于在训练过程中稳定梯度。\n",
        "        return self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "# 1. 定义超参数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "vocab_size = 10000  # 词汇表大小\n",
        "\n",
        "# 2. 创建 InputEmbeddings 实例\n",
        "input_embeddings = InputEmbeddings(d_model, vocab_size)\n",
        "\n",
        "# 3. 创建一个随机输入张量\n",
        "# 假设我们有一个形状为 (batch_size, seq_len) 的输入序列，其中每个元素都是词汇表中的一个索引。\n",
        "batch_size = 32  # 批大小\n",
        "seq_len = 64  # 序列长度\n",
        "input_tensor = torch.randint(0, vocab_size, (batch_size, seq_len))  # 生成 [0, vocab_size) 之间的随机整数\n",
        "\n",
        "# 4. 将输入张量传递给 InputEmbeddings 实例\n",
        "output_tensor = input_embeddings(input_tensor)\n",
        "\n",
        "# 5. 打印输入和输出的形状\n",
        "print(\"Input tensor shape:\", input_tensor.shape)  # 输出：torch.Size([32, 64])\n",
        "print(\"Output tensor shape:\", output_tensor.shape)  # 输出：torch.Size([32, 64, 512])\n",
        "\n",
        "# 6. 打印输入张量的内容\n",
        "print(\"Input tensor:\", input_tensor)\n",
        "\n",
        "# 7. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxVKC2svjXO5",
        "outputId": "13d261db-527f-4823-8ab8-202db8a82880"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([32, 64])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Input tensor: tensor([[8297, 8897, 6965,  ..., 3620, 6531,   93],\n",
            "        [1742, 2056, 1572,  ..., 5202, 4388,  615],\n",
            "        [6934, 6253, 7875,  ..., 1671, 1969, 7301],\n",
            "        ...,\n",
            "        [5477, 3773, 3561,  ..., 6790, 6976, 7585],\n",
            "        [4731, 3778, 7213,  ..., 8537, 1309, 7942],\n",
            "        [4134, 1730, 8429,  ..., 7028, 1556, 8555]])\n",
            "Output tensor: tensor([[[ 22.1121, -52.7231, -19.3767,  ...,  -3.3033,  15.5561,  32.3309],\n",
            "         [-12.7012,  47.2343,  -1.4278,  ...,  -3.6128, -53.0922, -12.4165],\n",
            "         [ 16.6640,  -7.8519,  55.3293,  ...,  21.8677,  10.0091, -11.9508],\n",
            "         ...,\n",
            "         [ -7.4874,  33.0481, -14.0697,  ..., -23.7683,  -1.8711, -10.5100],\n",
            "         [ -9.2349, -20.4624,  19.2936,  ...,  16.5013, -23.3660, -24.7568],\n",
            "         [-21.7927,  39.1972,   4.7612,  ..., -16.4555, -16.7975, -14.8032]],\n",
            "\n",
            "        [[ -0.3361, -25.8222,   3.0963,  ...,  37.2056, -43.5989, -23.8237],\n",
            "         [ 52.8753,  14.1144, -62.1190,  ..., -37.3091, -13.9546,  -2.2486],\n",
            "         [-22.0434,   2.1723, -12.5998,  ..., -38.4454,  -4.0522,   5.1107],\n",
            "         ...,\n",
            "         [ -9.1876,  20.5056,  23.4167,  ..., -17.7775, -11.6148, -15.8315],\n",
            "         [ -6.9693, -12.7993,  37.6109,  ...,  12.7832, -18.4981, -24.8754],\n",
            "         [ 16.1391,  -0.9195, -20.4139,  ...,  15.8384, -10.5088,   4.7613]],\n",
            "\n",
            "        [[-14.2848, -36.3534, -19.5416,  ...,  17.9292,  47.3966,   0.1689],\n",
            "         [ 75.0369,  28.3906, -22.0229,  ...,  11.6363,  -3.7708, -38.7658],\n",
            "         [-22.3294,  37.4660,  31.9726,  ..., -10.3723,  27.2834,  34.1337],\n",
            "         ...,\n",
            "         [ 16.5386, -23.5276, -12.6550,  ...,  -4.0729,  13.4632, -30.3462],\n",
            "         [-34.0231,  -8.1940, -43.4819,  ..., -12.8437,   9.9992,   2.8693],\n",
            "         [ -7.8119, -25.9410,  13.8743,  ...,   1.1336,  24.8897,  -4.9346]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-22.1782,  17.5838,  43.3713,  ...,  13.2384,  17.4349, -14.0976],\n",
            "         [ 36.4556, -23.0060,  37.7078,  ...,  -5.5530,  -8.7502, -11.5962],\n",
            "         [ 17.6707, -53.0904, -10.0616,  ..., -28.1235,  25.7634, -13.7312],\n",
            "         ...,\n",
            "         [-18.6840, -19.5942,  -0.3531,  ...,  19.6892,   3.8007, -24.6709],\n",
            "         [ 40.9003, -27.2360, -16.3494,  ..., -13.5524, -43.2841,  18.7783],\n",
            "         [ -6.8447,  -8.5431, -16.4048,  ...,  -9.6865,   2.4406,  21.7507]],\n",
            "\n",
            "        [[-13.0841, -27.4076,  -6.5953,  ...,  13.8227,  35.1910, -33.8849],\n",
            "         [-39.1637,  -2.7728,  11.9713,  ..., -15.4341, -10.3292, -17.6962],\n",
            "         [  1.2176,   6.7548,  12.5784,  ...,  10.9992,   2.3861,  -1.5492],\n",
            "         ...,\n",
            "         [ 17.7874,  41.0086,  -6.2332,  ...,   1.9437,  -9.7949,  44.0337],\n",
            "         [ 15.5212, -53.6516,  12.9338,  ..., -18.9035,  -4.0448,   1.5280],\n",
            "         [  3.1779, -34.4994,   1.8864,  ...,  28.1249, -33.7581, -41.5325]],\n",
            "\n",
            "        [[ 25.6259, -33.4528,  13.7338,  ..., -40.6353, -17.3339, -10.1748],\n",
            "         [ 23.5419, -14.3937, -32.2195,  ..., -23.7629,  50.1980,  -3.3187],\n",
            "         [  2.0403,  -1.4209,   6.5954,  ..., -11.4397,   0.7705, -30.8070],\n",
            "         ...,\n",
            "         [  6.8157, -24.4554, -15.8850,  ...,  20.0387, -20.8277,  47.8494],\n",
            "         [  1.9496, -21.4758,  -5.7025,  ..., -11.6304, -17.8727,  30.7041],\n",
            "         [-36.3629, -31.0190, -22.4090,  ..., -25.3534, -16.6656,  30.1358]]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "这段代码首先定义了 InputEmbeddings 类，然后创建了一个实例。接着，它生成了一个随机的输入张量，模拟一个包含词汇表中索引的序列。最后，它将输入张量传递给 InputEmbeddings 实例，并打印输入和输出张量的形状和内容。\n",
        "\n",
        "通过运行这段代码，你应该能看到输入张量和输出张量的形状和内容。输入张量的形状是 (batch_size, seq_len)，表示一个包含 batch_size 个序列的批次，每个序列的长度为 seq_len。输出张量的形状是 (batch_size, seq_len, d_model)，表示输入序列中的每个词都被转换成了一个 d_model 维的嵌入向量。"
      ],
      "metadata": {
        "id": "-J1WaRSvk9Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. 位置编码\n",
        "    \n",
        "    由于Transformer模型没有循环神经网络（RNN）那样的循环结构，它无法捕捉序列中token的位置信息。为了解决这个问题，我们使用位置编码将每个token的位置信息添加到其嵌入向量中。\n",
        "\n"
      ],
      "metadata": {
        "id": "-f5h-kRynXVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # 创建一个形状为 (max_len, d_model) 的矩阵，用于保存位置编码\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # 创建一个形状为 (max_len, 1) 的向量，包含位置信息\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        # 计算位置除以 (10000 ** (2i / d_model)) 的值，用于后续计算\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        # 计算偶数位置的正弦值\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # 计算奇数位置的余弦值\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # 将 pe 的形状变为 (1, max_len, d_model)，以便与嵌入向量相加\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将输入 x 的词嵌入向量与对应位置的位置编码相加\n",
        "        # x.shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "6UOxooF7kM2z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # 创建一个形状为 (max_len, d_model) 的矩阵，用于保存位置编码\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # 创建一个形状为 (max_len, 1) 的向量，包含位置信息\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        # 计算位置除以 (10000 ** (2i / d_model)) 的值，用于后续计算\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        # 计算偶数位置的正弦值\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # 计算奇数位置的余弦值\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # 将 pe 的形状变为 (1, max_len, d_model)，以便与嵌入向量相加\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将输入 x 的词嵌入向量与对应位置的位置编码相加\n",
        "        # x.shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# 1. 定义超参数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "max_len = 100  # 最大序列长度\n",
        "batch_size = 32\n",
        "seq_len = 64\n",
        "\n",
        "# 2. 创建 PositionalEncoding 实例\n",
        "positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "# 3. 创建一个随机输入张量\n",
        "# 假设我们有一个形状为 (batch_size, seq_len, d_model) 的输入张量，其中每个元素都是一个嵌入向量。\n",
        "input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 4. 将输入张量传递给 PositionalEncoding 实例\n",
        "output_tensor = positional_encoding(input_tensor)\n",
        "\n",
        "# 5. 打印输入和输出的形状\n",
        "print(\"Input tensor shape:\", input_tensor.shape)\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n",
        "# 6. 打印输入张量的内容\n",
        "print(\"Input tensor:\", input_tensor)\n",
        "\n",
        "# 7. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFmC9ffmks50",
        "outputId": "b02b1278-ca88-475c-9dfd-081e4b907dc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([32, 64, 512])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Input tensor: tensor([[[ 0.3855, -0.7737, -0.9055,  ..., -0.4490,  1.1517,  2.8363],\n",
            "         [ 0.3909,  0.6545, -0.7708,  ..., -0.2495,  0.1720,  0.1927],\n",
            "         [ 0.4242,  0.2979, -0.3946,  ..., -0.6007,  1.1717,  0.4679],\n",
            "         ...,\n",
            "         [ 0.2149,  1.1373,  0.7938,  ..., -1.8803, -0.1359, -1.1605],\n",
            "         [-1.3158, -1.1940,  0.8334,  ..., -0.0222,  0.4556,  0.0550],\n",
            "         [-1.4073,  0.4470,  0.9207,  ...,  1.0289, -0.9649, -1.6743]],\n",
            "\n",
            "        [[ 1.0479,  1.3762,  0.5304,  ...,  0.0620, -0.3535,  0.4402],\n",
            "         [-0.4265,  1.0887, -0.0819,  ..., -3.7146,  0.1422,  0.4274],\n",
            "         [ 0.8953,  0.4231,  0.9791,  ..., -1.9236,  0.3852,  1.7296],\n",
            "         ...,\n",
            "         [ 0.4766, -0.8314, -1.3835,  ...,  2.0222,  1.9621,  1.6243],\n",
            "         [-1.1289, -0.0748,  0.4628,  ..., -0.5372, -0.0684, -1.7344],\n",
            "         [-0.1339,  0.1877, -1.1375,  ...,  0.3897,  0.4767, -0.9352]],\n",
            "\n",
            "        [[ 0.0059, -2.0589,  0.4385,  ...,  0.6499,  0.5573, -0.3327],\n",
            "         [ 0.0358,  1.1081,  1.1193,  ..., -0.6551,  1.7157,  0.4061],\n",
            "         [ 0.0530, -1.2635, -0.7505,  ...,  0.8604, -0.2692,  0.2410],\n",
            "         ...,\n",
            "         [-0.4781, -0.4207,  0.2155,  ..., -0.8240,  1.5217, -0.1023],\n",
            "         [ 0.0465, -1.0978,  1.6744,  ..., -0.3947, -1.6183, -1.6869],\n",
            "         [ 0.5381, -0.5543,  0.0894,  ..., -1.3809,  1.0816,  0.5255]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.7054,  0.0887, -0.3469,  ...,  0.7413,  1.7612, -1.6237],\n",
            "         [ 1.0473,  0.1082,  0.6002,  ...,  0.1374, -1.1665, -1.2621],\n",
            "         [-0.5391,  0.6312, -0.3062,  ...,  0.0057, -2.2892, -1.2208],\n",
            "         ...,\n",
            "         [ 1.1311, -1.4337,  0.2365,  ...,  1.2445,  0.3795, -0.0329],\n",
            "         [ 0.2410, -0.1246,  0.3554,  ...,  1.7115,  0.1272,  0.8348],\n",
            "         [ 0.1032, -2.4835, -0.6625,  ..., -0.3520,  2.0061, -0.7823]],\n",
            "\n",
            "        [[ 1.0187,  1.5694, -0.5498,  ..., -0.0246,  1.0240, -0.9384],\n",
            "         [-0.0289, -1.6786, -0.2257,  ..., -0.4620,  0.2485, -0.0237],\n",
            "         [ 1.4860,  0.2030,  0.7188,  ...,  0.5197,  0.3030,  0.0757],\n",
            "         ...,\n",
            "         [-1.8567,  2.3643,  0.6300,  ...,  1.5808,  1.0893, -0.6189],\n",
            "         [-0.5399, -0.4846, -1.9687,  ...,  1.4656, -0.1680,  0.1654],\n",
            "         [-0.6860,  0.5017, -0.2006,  ..., -0.8381,  0.9041,  0.4956]],\n",
            "\n",
            "        [[ 1.7719,  0.7655, -0.9880,  ..., -0.1834,  0.1459,  0.0860],\n",
            "         [-3.1861,  0.5172,  0.6485,  ...,  0.2449, -0.8796,  0.6129],\n",
            "         [-1.6990,  2.3463,  2.1265,  ...,  0.6305, -1.2344,  1.9489],\n",
            "         ...,\n",
            "         [-0.7602, -0.0503, -0.6580,  ...,  0.0715,  0.9847, -0.6241],\n",
            "         [-0.3319, -0.5354,  0.1091,  ...,  1.3748, -1.5588,  0.1390],\n",
            "         [-0.4009, -0.3469, -0.4322,  ..., -1.2878, -0.4161,  1.1380]]])\n",
            "Output tensor: tensor([[[ 0.3855,  0.2263, -0.9055,  ...,  0.5510,  1.1517,  3.8363],\n",
            "         [ 1.2324,  1.1948,  0.0510,  ...,  0.7505,  0.1722,  1.1927],\n",
            "         [ 1.3335, -0.1183,  0.5419,  ...,  0.3993,  1.1719,  1.4679],\n",
            "         ...,\n",
            "         [-0.7512,  0.8792,  1.5424,  ..., -0.8803, -0.1296, -0.1605],\n",
            "         [-2.0550, -0.5205,  0.7150,  ...,  0.9777,  0.4620,  1.0550],\n",
            "         [-1.2399,  1.4329,  0.0371,  ...,  2.0289, -0.9584, -0.6744]],\n",
            "\n",
            "        [[ 1.0479,  2.3762,  0.5304,  ...,  1.0620, -0.3535,  1.4402],\n",
            "         [ 0.4150,  1.6290,  0.7399,  ..., -2.7146,  0.1423,  1.4274],\n",
            "         [ 1.8046,  0.0070,  1.9155,  ..., -0.9236,  0.3854,  2.7296],\n",
            "         ...,\n",
            "         [-0.4895, -1.0895, -0.6350,  ...,  3.0222,  1.9684,  2.6243],\n",
            "         [-1.8680,  0.5987,  0.3443,  ...,  0.4628, -0.0620, -0.7344],\n",
            "         [ 0.0334,  1.1736, -2.0210,  ...,  1.3896,  0.4832,  0.0648]],\n",
            "\n",
            "        [[ 0.0059, -1.0589,  0.4385,  ...,  1.6499,  0.5573,  0.6673],\n",
            "         [ 0.8772,  1.6484,  1.9412,  ...,  0.3449,  1.7158,  1.4061],\n",
            "         [ 0.9623, -1.6796,  0.1860,  ...,  1.8604, -0.2690,  1.2410],\n",
            "         ...,\n",
            "         [-1.4442, -0.6788,  0.9641,  ...,  0.1760,  1.5281,  0.8977],\n",
            "         [-0.6926, -0.4243,  1.5559,  ...,  0.6053, -1.6118, -0.6869],\n",
            "         [ 0.7055,  0.4316, -0.7942,  ..., -0.3809,  1.0881,  1.5255]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.7054,  1.0887, -0.3469,  ...,  1.7413,  1.7612, -0.6237],\n",
            "         [ 1.8888,  0.6485,  1.4221,  ...,  1.1374, -1.1664, -0.2621],\n",
            "         [ 0.3702,  0.2151,  0.6302,  ...,  1.0057, -2.2890, -0.2208],\n",
            "         ...,\n",
            "         [ 0.1650, -1.6918,  0.9851,  ...,  2.2445,  0.3858,  0.9670],\n",
            "         [-0.4982,  0.5489,  0.2369,  ...,  2.7115,  0.1337,  1.8348],\n",
            "         [ 0.2706, -1.4976, -1.5461,  ...,  0.6480,  2.0126,  0.2176]],\n",
            "\n",
            "        [[ 1.0187,  2.5694, -0.5498,  ...,  0.9754,  1.0240,  0.0616],\n",
            "         [ 0.8126, -1.1383,  0.5962,  ...,  0.5380,  0.2486,  0.9763],\n",
            "         [ 2.3953, -0.2132,  1.6552,  ...,  1.5197,  0.3032,  1.0757],\n",
            "         ...,\n",
            "         [-2.8228,  2.1062,  1.3786,  ...,  2.5808,  1.0956,  0.3811],\n",
            "         [-1.2791,  0.1889, -2.0872,  ...,  2.4656, -0.1616,  1.1654],\n",
            "         [-0.5186,  1.4875, -1.0842,  ...,  0.1618,  0.9107,  1.4956]],\n",
            "\n",
            "        [[ 1.7719,  1.7655, -0.9880,  ...,  0.8166,  0.1459,  1.0860],\n",
            "         [-2.3447,  1.0575,  1.4703,  ...,  1.2449, -0.8795,  1.6129],\n",
            "         [-0.7897,  1.9301,  3.0629,  ...,  1.6305, -1.2342,  2.9489],\n",
            "         ...,\n",
            "         [-1.7263, -0.3084,  0.0906,  ...,  1.0715,  0.9910,  0.3759],\n",
            "         [-1.0711,  0.1381, -0.0094,  ...,  2.3748, -1.5524,  1.1390],\n",
            "         [-0.2335,  0.6390, -1.3158,  ..., -0.2878, -0.4096,  2.1379]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码首先定义了 PositionalEncoding 类，然后创建了一个实例。接着，它生成了一个随机的输入张量，模拟一个包含嵌入向量的序列。最后，它将输入张量传递给 PositionalEncoding 实例，并打印输入和输出张量的形状和内容。\n",
        "\n",
        "通过运行这段代码，你应该能看到输入张量和输出张量的形状和内容。输入张量的形状是 (batch_size, seq_len, d_model)，输出张量的形状也是 (batch_size, seq_len, d_model)。输出张量的内容是输入张量和位置编码的逐元素相加的结果。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2Bkc4grZmDqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer模型中的自注意力机制本身无法感知序列的顺序，因此需要位置编码来注入位置信息。位置编码的设计需要满足两个关键需求：每个位置应有唯一的编码，并且能捕捉位置间的相对关系。以下是详细解释：\n",
        "\n",
        "---\n",
        "\n",
        "### **1. 为什么用位置除以一个数？**\n",
        "位置编码公式中，使用位置除以不同频率的缩放因子（如`10000^(2i/d_model)`），目的是让不同维度对应不同频率的正弦/余弦函数：\n",
        "- **高频维度（低i）**：分母较小，频率高，能捕捉近距离的位置变化。\n",
        "- **低频维度（高i）**：分母较大，频率低，能捕捉远距离的位置依赖。\n",
        "\n",
        "例如，当`d_model=512`时，不同维度的波长从`2π`到`2π*10000`，覆盖了从局部到全局的位置关系。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. 为什么分奇偶位置并用sin和cos？**\n",
        "在代码中，偶数维度用正弦函数，奇数维度用余弦函数。这是为了利用三角函数的性质：\n",
        "- **相对位置的可学习性**：通过正弦和余弦的线性组合，模型能轻松学到相对位置关系。例如，位置`pos+k`的编码可通过`pos`的编码线性变换得到。\n",
        "- **唯一性与正交性**：正弦和余弦函数在不同频率下具有正交性，确保每个位置的编码唯一，同时避免不同维度间的信息冗余。\n",
        "\n",
        "公式形式化如下：\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\quad\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. 代码实现解析**\n",
        "```python\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        \n",
        "        # 计算频率缩放因子：10000^(2i/d_model)的倒数\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() *\n",
        "            (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        \n",
        "        # 交替填充sin和cos\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "```\n",
        "\n",
        "- **`div_term`的作用**：通过指数和对数转换，高效计算频率缩放因子，避免直接计算大数的幂次。\n",
        "- **奇偶交替填充**：相邻维度使用相同频率的正弦和余弦，增强位置关系的表达能力。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. 位置编码的优势**\n",
        "- **绝对位置**：每个位置有唯一编码。\n",
        "- **相对位置**：通过三角恒等式（如$\\sin(a+b)=\\sin a \\cos b + \\cos a \\sin b$），模型能学习到位置间的偏移。\n",
        "- **泛化性**：固定的编码方式使模型能处理超出训练时最大长度的序列。\n",
        "\n",
        "---\n",
        "\n",
        "### **总结**\n",
        "位置编码通过不同频率的正弦/余弦函数，为序列中的每个位置生成独特的嵌入。奇偶维度的交替设计利用三角函数的线性性质，使模型能同时捕捉绝对位置和相对位置信息，而频率缩放则让不同维度关注不同范围的位置关系。这种设计兼顾了效率与表达能力，是Transformer成功的关键之一。"
      ],
      "metadata": {
        "id": "66i4FefGn-BJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 自注意力机制\n",
        "\n",
        "    自注意力机制允许模型计算序列中不同位置之间的关系。对于序列中的每个token，自注意力机制会计算其与其他所有token之间的注意力权重，并使用这些权重来加权求和这些token的向量表示。\n",
        "\n",
        "\n",
        "    * 没有自注意力机制，这个词就是“孤立化”的表示，独立于语境，与周围词没建立联系\n",
        "\n",
        "    * 有了自注意力机制，这个词就是“社会化”的表示，融入到语境，与周围词建立了联系\n"
      ],
      "metadata": {
        "id": "VpsIlXzWoUg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len = Q.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "yZiy2sDXmEWy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len = Q.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "# 1. 定义超参数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "n_heads = 8  # 注意力头的数量\n",
        "batch_size = 32\n",
        "seq_len = 64\n",
        "\n",
        "# 2. 创建 SelfAttention 实例\n",
        "self_attention = SelfAttention(d_model, n_heads)\n",
        "\n",
        "# 3. 创建随机输入张量 Q, K, V\n",
        "# 假设我们有形状为 (batch_size, seq_len, d_model) 的输入张量\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 4. 创建一个随机mask\n",
        "# mask 的形状应该是 (batch_size, 1, seq_len, seq_len)\n",
        "mask = torch.randint(0, 2, (batch_size, 1, seq_len, seq_len)).bool()  # 生成随机的布尔mask\n",
        "\n",
        "# 5. 将输入张量传递给 SelfAttention 实例\n",
        "output_tensor = self_attention(Q, K, V, mask)\n",
        "\n",
        "# 6. 打印输入和输出的形状\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"K shape:\", K.shape)\n",
        "print(\"V shape:\", V.shape)\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n",
        "# 7. 打印输入张量的内容\n",
        "print(\"Q:\", Q)\n",
        "print(\"K:\", K)\n",
        "print(\"V:\", V)\n",
        "print(\"Mask:\", mask)\n",
        "\n",
        "# 8. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJcmRRP2per-",
        "outputId": "7062c5b7-1e5f-4ccb-a8b2-53b4396aa32c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([32, 64, 512])\n",
            "K shape: torch.Size([32, 64, 512])\n",
            "V shape: torch.Size([32, 64, 512])\n",
            "Mask shape: torch.Size([32, 1, 64, 64])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Q: tensor([[[ 1.3543e+00,  2.1051e+00,  9.9099e-01,  ...,  8.9917e-01,\n",
            "           7.3496e-02, -2.0353e-01],\n",
            "         [ 1.0431e-01,  4.5047e-01, -3.2936e-01,  ...,  1.6953e-01,\n",
            "           4.1585e-01,  4.9047e-01],\n",
            "         [-5.7413e-01,  3.0500e-01,  2.2931e-01,  ...,  1.5950e-01,\n",
            "          -1.6138e+00, -4.5554e-01],\n",
            "         ...,\n",
            "         [-9.9080e-01,  1.2162e+00,  5.1583e-02,  ...,  2.9905e-01,\n",
            "          -9.5179e-01, -3.0786e-01],\n",
            "         [ 5.8014e-01, -1.2049e+00,  1.7700e-01,  ..., -1.9418e+00,\n",
            "          -1.5116e+00, -1.5093e-02],\n",
            "         [-1.1372e+00,  1.7667e+00,  1.3713e+00,  ..., -8.5653e-03,\n",
            "           1.5630e+00,  6.8038e-01]],\n",
            "\n",
            "        [[-5.0720e-01,  4.6156e-01,  6.4562e-01,  ..., -2.7373e-02,\n",
            "          -1.4242e+00,  4.2019e-01],\n",
            "         [ 1.0535e+00, -6.9789e-01,  1.4447e+00,  ...,  5.6531e-01,\n",
            "           9.2296e-01, -1.0155e-01],\n",
            "         [-4.8094e-01, -9.5967e-01,  8.7197e-01,  ..., -7.7932e-01,\n",
            "          -5.4811e-01,  1.4079e+00],\n",
            "         ...,\n",
            "         [-1.3392e+00, -5.7243e-01,  1.5021e+00,  ...,  5.5690e-01,\n",
            "          -1.4223e+00,  5.1971e-01],\n",
            "         [ 1.3714e+00, -3.7708e-01,  5.3747e-01,  ..., -9.7564e-01,\n",
            "          -4.1508e-01, -6.7367e-01],\n",
            "         [ 9.5603e-01, -5.3646e-01, -4.6820e-01,  ..., -1.2422e-01,\n",
            "           6.2360e-01,  4.5616e-01]],\n",
            "\n",
            "        [[-4.8645e-01,  1.3913e+00,  2.5421e+00,  ...,  1.4543e+00,\n",
            "           8.7397e-03,  3.4363e-01],\n",
            "         [ 6.6780e-01, -3.7285e-01,  9.3490e-01,  ...,  1.2498e+00,\n",
            "          -3.1322e-01, -1.3917e+00],\n",
            "         [-5.3040e-01,  2.5256e-03, -1.4553e+00,  ..., -3.3901e-01,\n",
            "          -4.0673e-01,  3.4349e-02],\n",
            "         ...,\n",
            "         [ 7.3106e-01,  1.9811e+00,  9.6095e-01,  ...,  2.3002e-01,\n",
            "          -8.6643e-01, -5.5581e-01],\n",
            "         [-4.2885e-01,  2.1762e-01,  1.2541e+00,  ...,  1.9177e+00,\n",
            "          -8.5144e-01,  5.4057e-01],\n",
            "         [-4.3525e-01, -1.3039e+00, -2.1138e-01,  ..., -1.3980e-01,\n",
            "          -9.8924e-01, -5.5236e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4516e+00,  1.3173e+00,  3.5388e-01,  ..., -6.3463e-01,\n",
            "          -3.2107e-01,  1.7513e-01],\n",
            "         [ 1.8546e+00,  5.6512e-01, -1.7038e+00,  ...,  2.5059e-01,\n",
            "           8.1658e-01, -1.3121e+00],\n",
            "         [ 2.7411e+00, -5.2421e-01, -6.8740e-01,  ..., -1.1987e+00,\n",
            "          -1.0860e+00, -3.2618e-01],\n",
            "         ...,\n",
            "         [-5.0978e-01, -8.6306e-01,  4.5013e-01,  ..., -6.9891e-01,\n",
            "           5.9768e-01,  1.2551e+00],\n",
            "         [-2.2017e-02, -3.2289e-01, -1.6175e-02,  ..., -1.0826e+00,\n",
            "           1.1153e+00,  3.6434e-01],\n",
            "         [-1.5624e+00, -3.0338e-01, -3.8077e-01,  ...,  3.9322e-01,\n",
            "           6.2059e-01,  1.3562e+00]],\n",
            "\n",
            "        [[-8.0424e-01,  9.4420e-02,  2.9850e-01,  ..., -4.9693e-01,\n",
            "          -4.4170e-01, -1.5735e+00],\n",
            "         [ 9.3608e-01, -5.7566e-01, -6.6422e-01,  ..., -1.1862e-01,\n",
            "           4.9819e-01, -5.3751e-03],\n",
            "         [-2.7525e-01, -8.5696e-01,  3.5746e-01,  ...,  7.0171e-01,\n",
            "           1.2851e+00, -2.4487e-01],\n",
            "         ...,\n",
            "         [-1.1739e+00,  7.9789e-01,  1.8132e-01,  ...,  2.9811e-01,\n",
            "          -4.3605e-02,  8.6199e-01],\n",
            "         [-5.0274e-01, -1.3047e+00, -1.4420e+00,  ..., -3.4360e-01,\n",
            "           1.7943e-01, -1.0550e+00],\n",
            "         [-4.0726e-01, -3.7452e-01, -8.3661e-01,  ...,  1.2752e-01,\n",
            "          -6.9156e-01, -2.8212e+00]],\n",
            "\n",
            "        [[-6.9307e-01, -9.3573e-01,  1.0717e+00,  ..., -1.0542e+00,\n",
            "          -1.1014e-01, -2.4935e+00],\n",
            "         [-6.1125e-01, -1.4501e+00,  6.6614e-01,  ..., -7.3232e-01,\n",
            "           3.0398e+00, -2.0952e+00],\n",
            "         [-7.9283e-01, -1.2816e+00, -1.0802e+00,  ...,  1.0918e+00,\n",
            "           7.5462e-01,  8.3899e-02],\n",
            "         ...,\n",
            "         [-2.3021e+00, -2.9369e-01,  1.5160e-01,  ..., -4.2619e-01,\n",
            "          -1.0113e+00,  4.1321e-01],\n",
            "         [-8.9199e-01,  1.7307e+00, -9.2420e-01,  ..., -7.3438e-01,\n",
            "           6.2170e-01, -6.6289e-01],\n",
            "         [ 1.4426e-02,  1.2129e+00, -6.9151e-01,  ..., -7.9857e-01,\n",
            "          -1.0234e+00, -9.6180e-01]]])\n",
            "K: tensor([[[ 8.5553e-01,  4.4520e-01,  8.6630e-02,  ...,  7.3338e-01,\n",
            "           1.7632e+00, -9.8814e-01],\n",
            "         [ 4.3309e-01, -1.0599e+00, -2.0607e-01,  ..., -1.1384e+00,\n",
            "           2.7438e-01,  7.2553e-01],\n",
            "         [ 1.4661e+00, -5.1552e-01,  3.7996e-01,  ..., -4.8531e-01,\n",
            "           6.2041e-01,  1.0106e+00],\n",
            "         ...,\n",
            "         [ 1.9222e+00,  6.0411e-01,  6.6046e-01,  ...,  1.4391e+00,\n",
            "          -5.5767e-03, -1.2758e-01],\n",
            "         [ 1.0839e+00,  4.5728e-01, -7.5691e-01,  ..., -2.2668e-01,\n",
            "          -1.0337e+00, -7.7930e-01],\n",
            "         [ 3.6984e-01, -1.1263e+00, -5.9298e-02,  ...,  7.7054e-01,\n",
            "           1.1242e+00, -1.5844e-01]],\n",
            "\n",
            "        [[ 1.8841e+00, -1.6088e-01, -6.2591e-01,  ..., -1.0956e+00,\n",
            "          -1.0635e+00,  7.6396e-01],\n",
            "         [ 1.6502e+00,  1.0718e-03, -1.1851e+00,  ..., -2.4151e+00,\n",
            "           4.6880e-01, -1.3054e+00],\n",
            "         [ 3.6447e-01, -7.9027e-01,  2.4058e-01,  ..., -2.0707e+00,\n",
            "          -7.1289e-01,  1.3913e+00],\n",
            "         ...,\n",
            "         [-1.5026e+00,  7.2613e-01, -2.0167e-01,  ..., -5.9458e-01,\n",
            "           8.0568e-01,  2.9484e-01],\n",
            "         [ 1.1357e+00,  8.7527e-02,  8.6756e-01,  ...,  1.4666e-01,\n",
            "           2.8957e-01, -2.6477e-01],\n",
            "         [-3.8206e-01,  4.1025e-01, -1.5585e+00,  ...,  4.7343e-01,\n",
            "           1.6089e+00, -3.7573e-01]],\n",
            "\n",
            "        [[-3.5075e-01, -1.1755e-01, -2.1907e-01,  ...,  9.2441e-01,\n",
            "           1.4758e+00,  3.4938e-01],\n",
            "         [-2.7998e+00, -9.3419e-01, -1.1252e-01,  ...,  4.2242e-01,\n",
            "           1.4118e+00, -4.2516e-01],\n",
            "         [-1.2120e+00,  1.2689e+00,  1.2199e+00,  ...,  5.7304e-01,\n",
            "           4.5055e-01,  1.1582e+00],\n",
            "         ...,\n",
            "         [-1.3154e+00,  7.8877e-01,  1.1931e+00,  ...,  7.0065e-01,\n",
            "           2.2675e-01, -7.8244e-01],\n",
            "         [ 5.1268e-01,  2.6805e-01, -4.9124e-01,  ...,  5.3927e-01,\n",
            "           1.2587e+00, -3.9174e-01],\n",
            "         [ 4.6080e-01, -2.3066e+00, -9.1482e-01,  ..., -5.6390e-01,\n",
            "          -7.3902e-01, -5.3460e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.0140e-01, -4.3106e-01, -5.3102e-01,  ...,  1.2903e+00,\n",
            "          -2.3900e-01,  6.4776e-01],\n",
            "         [-1.2977e+00,  8.3920e-01, -1.2176e+00,  ..., -2.3150e+00,\n",
            "          -4.5155e-01,  7.8530e-02],\n",
            "         [-8.8688e-01, -8.3272e-01, -1.0016e+00,  ...,  4.9409e-01,\n",
            "           7.5163e-01,  1.4337e-01],\n",
            "         ...,\n",
            "         [-1.1846e+00,  4.5357e-01,  1.3425e-01,  ...,  9.7154e-01,\n",
            "           4.2615e-01, -2.2655e+00],\n",
            "         [ 5.1899e-01,  7.8763e-02,  1.2029e+00,  ..., -1.7908e+00,\n",
            "           6.6815e-01,  1.1842e+00],\n",
            "         [ 6.8443e-01, -4.1924e-01, -2.4208e-01,  ...,  6.9994e-01,\n",
            "          -3.2517e-01,  9.2617e-01]],\n",
            "\n",
            "        [[-4.9330e-01,  6.1903e-01,  9.1750e-01,  ..., -9.2682e-01,\n",
            "          -1.5580e+00, -4.6966e-01],\n",
            "         [ 1.5250e+00,  1.0645e+00,  1.4122e+00,  ...,  9.0921e-01,\n",
            "          -9.7040e-01, -5.9277e-01],\n",
            "         [ 2.6241e-01,  1.2728e+00,  4.0163e-01,  ..., -8.4898e-01,\n",
            "           7.8886e-01, -2.8268e-01],\n",
            "         ...,\n",
            "         [-5.1640e-01, -1.9707e+00,  1.1491e+00,  ...,  1.6796e+00,\n",
            "           6.7241e-01, -1.8956e+00],\n",
            "         [-1.1130e-02, -9.3449e-01,  9.2443e-01,  ..., -7.2089e-01,\n",
            "          -9.0335e-02,  2.3715e-01],\n",
            "         [-4.1270e-01,  2.2560e+00,  1.8148e+00,  ..., -8.0820e-02,\n",
            "          -9.4451e-01,  1.0981e+00]],\n",
            "\n",
            "        [[-5.4380e-01, -4.8266e-01,  3.7601e-01,  ..., -3.0494e-01,\n",
            "          -6.1694e-01,  3.7332e-01],\n",
            "         [ 7.0534e-01,  4.1837e-01,  4.9962e-01,  ..., -1.0355e+00,\n",
            "          -5.0812e-01, -1.4069e+00],\n",
            "         [ 1.2907e+00, -7.9097e-01, -7.8197e-02,  ..., -5.3020e-01,\n",
            "          -5.8915e-01,  4.4013e-01],\n",
            "         ...,\n",
            "         [-6.5510e-01,  8.7379e-01, -3.3362e-01,  ..., -1.9001e-01,\n",
            "          -2.5755e-01,  9.1684e-01],\n",
            "         [-4.1892e-01,  2.0699e+00, -6.2707e-01,  ..., -9.0189e-01,\n",
            "           1.7036e-01,  1.2165e+00],\n",
            "         [ 8.4268e-01, -6.4219e-01,  1.6947e+00,  ...,  1.6442e+00,\n",
            "           5.0178e-01, -1.5032e-01]]])\n",
            "V: tensor([[[-1.1010, -0.3600,  0.6227,  ...,  1.2444,  0.0562,  0.4082],\n",
            "         [ 1.3921, -0.0329, -1.8259,  ...,  0.0845, -1.0731, -0.4049],\n",
            "         [ 0.7476,  1.1564, -0.2029,  ..., -0.8522, -0.0933, -0.7525],\n",
            "         ...,\n",
            "         [ 0.5154, -1.0109, -0.4355,  ...,  1.1164,  1.2020,  1.9980],\n",
            "         [ 1.0985, -0.2365,  1.0678,  ..., -1.0245, -1.0246,  0.1324],\n",
            "         [ 2.4422, -1.1878, -0.7866,  ...,  0.0756,  0.8117,  0.6862]],\n",
            "\n",
            "        [[-0.5738, -0.3589, -0.2979,  ...,  1.1819, -1.4519, -1.8443],\n",
            "         [ 0.3299,  0.2911,  0.5773,  ...,  0.2472, -0.3915, -0.2148],\n",
            "         [ 0.4685, -0.7297,  0.3634,  ..., -0.1112,  0.6672,  0.2191],\n",
            "         ...,\n",
            "         [-0.2757, -0.8833, -0.3439,  ..., -0.0611, -0.8411, -0.5775],\n",
            "         [-0.6127, -0.8751, -1.1981,  ...,  0.2550, -1.1276, -1.0285],\n",
            "         [-0.1095, -1.5344, -0.8515,  ...,  0.6042,  0.1920,  1.0927]],\n",
            "\n",
            "        [[-0.5746,  0.2584, -0.8822,  ..., -0.4581, -0.1249, -0.2813],\n",
            "         [ 1.3088,  1.1457, -1.7928,  ..., -0.1880,  0.4180, -1.3391],\n",
            "         [-1.6380, -1.4720, -0.9575,  ...,  0.7478, -0.8386,  1.2803],\n",
            "         ...,\n",
            "         [-2.0473, -0.1183, -0.3499,  ..., -1.8228, -0.3375,  1.6271],\n",
            "         [ 0.3414, -0.0293, -0.8761,  ...,  0.0979, -0.6685,  0.8206],\n",
            "         [-0.7565, -0.3627,  0.8583,  ...,  0.4397,  1.2071, -1.2290]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.2688,  0.2655, -1.3356,  ...,  0.2413,  0.5608,  0.2069],\n",
            "         [-0.5462,  0.1933,  1.7466,  ...,  1.2621,  0.4460, -1.9567],\n",
            "         [-0.3922, -0.9850, -0.6847,  ..., -0.6391, -2.7699, -0.2873],\n",
            "         ...,\n",
            "         [ 0.9326, -0.7406, -0.1303,  ...,  0.8246,  1.0351,  0.3592],\n",
            "         [-0.2183,  1.0665, -1.4192,  ..., -0.1690,  0.6595, -1.4551],\n",
            "         [ 0.1980, -1.1749, -1.8338,  ..., -1.0676,  0.0086,  0.1691]],\n",
            "\n",
            "        [[ 1.0909,  0.3883, -0.7807,  ...,  2.1109, -0.1966, -1.4278],\n",
            "         [-0.6538, -0.0585, -0.7536,  ...,  0.3672, -1.1116,  0.0142],\n",
            "         [-1.0728,  0.8989,  1.9734,  ...,  0.0193,  0.5947,  2.0829],\n",
            "         ...,\n",
            "         [ 0.2214, -0.5406, -1.5155,  ..., -0.7706, -1.0529,  0.1534],\n",
            "         [ 0.1630,  0.3649,  0.1622,  ...,  2.5819,  0.4170,  0.7568],\n",
            "         [-0.0448,  0.7982, -0.5339,  ..., -0.2406,  0.8773, -1.2314]],\n",
            "\n",
            "        [[-0.8017,  0.3415, -0.6104,  ...,  0.8809, -0.8205, -0.2609],\n",
            "         [-1.0361,  0.2411,  0.1187,  ...,  0.7047, -0.2020,  0.1711],\n",
            "         [ 0.4604, -0.2047,  0.6722,  ..., -0.7683, -2.8833, -0.3812],\n",
            "         ...,\n",
            "         [-0.2541, -0.5586, -1.0036,  ..., -0.0152, -1.4281,  0.1512],\n",
            "         [-0.1116, -1.5869,  1.9705,  ...,  0.4635,  1.6025,  0.2330],\n",
            "         [-0.5332,  0.8285,  0.3352,  ...,  0.2562,  1.0431,  0.7847]]])\n",
            "Mask: tensor([[[[ True, False,  True,  ..., False,  True, False],\n",
            "          [ True, False,  True,  ..., False, False, False],\n",
            "          [False, False,  True,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [False, False,  True,  ..., False,  True, False],\n",
            "          [False, False, False,  ...,  True, False, False],\n",
            "          [ True, False,  True,  ...,  True, False, False]]],\n",
            "\n",
            "\n",
            "        [[[False, False, False,  ...,  True,  True, False],\n",
            "          [False,  True, False,  ..., False, False, False],\n",
            "          [False, False,  True,  ...,  True,  True,  True],\n",
            "          ...,\n",
            "          [ True,  True,  True,  ...,  True, False, False],\n",
            "          [False, False, False,  ...,  True, False, False],\n",
            "          [False, False, False,  ...,  True, False, False]]],\n",
            "\n",
            "\n",
            "        [[[ True, False, False,  ..., False,  True, False],\n",
            "          [False, False,  True,  ..., False,  True,  True],\n",
            "          [False,  True,  True,  ..., False, False,  True],\n",
            "          ...,\n",
            "          [False,  True,  True,  ...,  True,  True, False],\n",
            "          [False, False, False,  ..., False, False,  True],\n",
            "          [ True, False,  True,  ..., False,  True,  True]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ True, False,  True,  ..., False, False, False],\n",
            "          [False,  True, False,  ..., False,  True,  True],\n",
            "          [ True, False, False,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [ True,  True, False,  ..., False,  True, False],\n",
            "          [False,  True, False,  ..., False,  True, False],\n",
            "          [False,  True, False,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False, False, False,  ...,  True,  True, False],\n",
            "          [ True, False,  True,  ...,  True, False, False],\n",
            "          [False,  True, False,  ..., False,  True,  True],\n",
            "          ...,\n",
            "          [False, False, False,  ...,  True, False, False],\n",
            "          [False, False, False,  ..., False, False,  True],\n",
            "          [ True, False,  True,  ..., False,  True, False]]],\n",
            "\n",
            "\n",
            "        [[[ True, False,  True,  ...,  True, False, False],\n",
            "          [ True, False,  True,  ..., False,  True, False],\n",
            "          [ True,  True,  True,  ...,  True, False,  True],\n",
            "          ...,\n",
            "          [False,  True, False,  ...,  True, False,  True],\n",
            "          [False, False, False,  ...,  True,  True,  True],\n",
            "          [False,  True,  True,  ..., False, False,  True]]]])\n",
            "Output tensor: tensor([[[-4.5831e-03, -1.8719e-01,  9.7283e-02,  ..., -1.2008e-02,\n",
            "           1.5218e-03,  6.2145e-02],\n",
            "         [-1.4889e-02, -9.9540e-02,  5.7697e-02,  ..., -7.0899e-02,\n",
            "           8.0923e-02,  4.5848e-02],\n",
            "         [ 1.6391e-02, -8.5918e-02,  5.7493e-02,  ...,  1.0838e-02,\n",
            "           4.7430e-02,  1.0997e-01],\n",
            "         ...,\n",
            "         [-5.0162e-02, -8.5107e-02,  6.7954e-02,  ..., -3.3149e-02,\n",
            "           2.0866e-02,  1.1626e-01],\n",
            "         [-5.0331e-02, -1.2523e-01,  5.0304e-02,  ...,  2.3168e-03,\n",
            "           4.1984e-02,  7.5042e-02],\n",
            "         [ 5.7353e-03, -6.3013e-02,  8.8685e-02,  ...,  2.3346e-02,\n",
            "           5.1090e-03,  3.2470e-02]],\n",
            "\n",
            "        [[-4.7813e-02,  6.7325e-03,  1.0250e-01,  ...,  6.7893e-02,\n",
            "          -4.2056e-02, -2.6477e-02],\n",
            "         [-4.8266e-02, -7.0237e-02,  1.1161e-01,  ..., -6.9618e-02,\n",
            "          -2.1408e-02,  4.8215e-02],\n",
            "         [ 5.9940e-06,  4.8704e-03,  1.4512e-01,  ..., -2.0513e-03,\n",
            "          -3.2544e-02, -5.7085e-02],\n",
            "         ...,\n",
            "         [ 3.4336e-03, -4.2689e-02,  8.4573e-02,  ..., -1.1086e-02,\n",
            "          -5.7560e-02, -3.1134e-02],\n",
            "         [-4.8054e-02, -5.2775e-02,  1.3747e-01,  ..., -6.0173e-02,\n",
            "          -3.0585e-02,  1.4346e-02],\n",
            "         [ 5.0070e-02, -8.2048e-02,  6.6707e-02,  ..., -5.2677e-02,\n",
            "          -1.1878e-01, -5.2767e-02]],\n",
            "\n",
            "        [[ 7.5224e-03, -1.3022e-01,  8.1304e-02,  ..., -1.1298e-01,\n",
            "           8.5573e-02, -1.3567e-02],\n",
            "         [-2.1308e-02, -3.7466e-02, -6.6417e-03,  ..., -4.5932e-02,\n",
            "           3.7223e-02,  3.9190e-02],\n",
            "         [ 1.0290e-01, -1.4017e-01,  4.6116e-02,  ..., -8.8341e-02,\n",
            "           9.9986e-02, -3.4896e-02],\n",
            "         ...,\n",
            "         [ 5.9121e-02, -1.5731e-01,  4.9619e-02,  ..., -7.8219e-02,\n",
            "          -2.2619e-03, -1.1485e-02],\n",
            "         [ 4.6492e-02, -9.1903e-02,  1.2843e-01,  ..., -1.0396e-01,\n",
            "           8.8489e-02, -3.5865e-03],\n",
            "         [ 1.4518e-02, -4.7758e-02,  4.8561e-03,  ..., -4.6569e-02,\n",
            "           5.3037e-02,  4.1207e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.3460e-02,  4.6081e-02,  6.6246e-02,  ..., -5.4718e-02,\n",
            "           6.0593e-02,  1.4535e-02],\n",
            "         [-3.2011e-02,  7.4673e-03, -1.9343e-02,  ..., -1.0993e-01,\n",
            "           1.1240e-01,  1.8015e-03],\n",
            "         [-3.2941e-02, -6.1699e-03,  4.4221e-02,  ..., -4.0099e-02,\n",
            "          -1.9777e-03, -1.2042e-02],\n",
            "         ...,\n",
            "         [ 4.1309e-02,  8.3265e-02,  6.5490e-02,  ..., -7.2971e-02,\n",
            "           8.4711e-02,  3.4593e-02],\n",
            "         [ 8.7966e-02, -4.4433e-02, -5.3891e-02,  ..., -3.8431e-02,\n",
            "           7.3698e-02, -4.3197e-02],\n",
            "         [ 7.5261e-02, -1.2358e-01, -1.1149e-02,  ..., -1.0280e-01,\n",
            "          -1.2891e-02, -9.5806e-02]],\n",
            "\n",
            "        [[-3.0049e-02, -1.2321e-01,  1.8406e-02,  ..., -2.7615e-02,\n",
            "          -2.5171e-02,  1.8927e-02],\n",
            "         [-6.1368e-03, -1.2864e-02,  6.1727e-03,  ..., -3.0281e-02,\n",
            "          -8.0705e-02,  4.1676e-02],\n",
            "         [-4.0713e-02, -6.9076e-03, -2.2570e-02,  ..., -1.3944e-03,\n",
            "           4.9754e-03, -3.8080e-02],\n",
            "         ...,\n",
            "         [-3.2285e-02, -2.1796e-01, -1.7952e-04,  ..., -6.0650e-02,\n",
            "          -7.1562e-02,  6.0007e-03],\n",
            "         [-3.6998e-02, -2.0306e-02,  1.2127e-01,  ..., -2.9536e-03,\n",
            "          -1.4444e-01,  9.4206e-03],\n",
            "         [ 2.5524e-02, -3.0922e-02,  6.6471e-02,  ..., -4.3342e-02,\n",
            "           5.3108e-02,  3.4584e-02]],\n",
            "\n",
            "        [[ 8.5485e-03,  8.6957e-02,  1.0353e-01,  ...,  1.3554e-02,\n",
            "           4.0702e-02, -3.9704e-02],\n",
            "         [ 2.8298e-02,  1.2773e-02,  1.2512e-01,  ...,  6.8867e-02,\n",
            "          -6.8820e-02,  7.2938e-02],\n",
            "         [-3.6364e-02,  3.3642e-02, -4.0281e-02,  ...,  1.6951e-02,\n",
            "           1.8454e-02,  4.6891e-02],\n",
            "         ...,\n",
            "         [ 5.3868e-02, -4.3993e-03,  5.9276e-02,  ..., -2.3145e-02,\n",
            "          -5.6078e-02,  2.3493e-02],\n",
            "         [ 5.6487e-02, -4.7371e-02,  5.0316e-02,  ...,  1.8996e-02,\n",
            "          -6.1197e-03, -2.3316e-02],\n",
            "         [-3.6689e-02,  4.5230e-02,  1.8199e-01,  ...,  5.6052e-02,\n",
            "           2.3495e-02, -4.6631e-02]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **自注意力机制：让词语学会“社交”的魔法**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. 直观理解：从“孤立”到“社交”**\n",
        "想象你在一场聚会中：\n",
        "- **没有自注意力**：每个人只能自言自语，不知道周围人在说什么。就像词语被孤立，每个词只能表达自己的意思，无法理解上下文。\n",
        "- **有自注意力**：每个人开始观察周围人的表情和动作，调整自己的发言。词语之间开始“交流”，每个词的含义会根据上下文动态变化。\n",
        "\n",
        "**例如**：\n",
        "- **孤立**：句子 *\"苹果股价上涨了\"* 中的“苹果”可能指水果或公司。\n",
        "- **社交**：通过自注意力，“苹果”会关注“股价”和“上涨”，从而确定自己指代公司。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. 自注意力机制的核心思想**\n",
        "自注意力机制让每个词主动做三件事：\n",
        "1. **提问（Query）**：当前词提出一个问题（例如：“我应该关注哪些词？”）\n",
        "2. **回答（Key）**：所有词给出答案（例如：“我的关键词是什么？”）\n",
        "3. **整合（Value）**：根据问题和答案的匹配程度，汇总所有词的信息\n",
        "\n",
        "---\n",
        "\n",
        "### **3. 代码分步解析**\n",
        "\n",
        "```python\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # 输入维度（如512）\n",
        "        self.n_heads = n_heads  # 多头注意力头数（如8）\n",
        "        self.d_k = d_model // n_heads  # 每头的维度（如64）\n",
        "\n",
        "        # 定义线性变换矩阵\n",
        "        self.W_Q = nn.Linear(d_model, d_model)  # 生成Query\n",
        "        self.W_K = nn.Linear(d_model, d_model)  # 生成Key\n",
        "        self.W_V = nn.Linear(d_model, d_model)  # 生成Value\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出变换\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 1：生成Q/K/V（提问、回答、内容）**\n",
        "- **输入**：每个词的嵌入向量（含语义信息）\n",
        "- **操作**：通过线性变换生成三个角色\n",
        "  ```python\n",
        "  Q = self.W_Q(x)  # Query：当前词提出的问题\n",
        "  K = self.W_K(x)  # Key：其他词提供的答案关键词\n",
        "  V = self.W_V(x)  # Value：其他词的实际内容\n",
        "  ```\n",
        "- **意义**：让每个词既能主动提问，也能被动回答问题。\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 2：多头分割（多视角观察）**\n",
        "```python\n",
        "Q = Q.view(batch_size, seq_len, n_heads, d_k).transpose(1, 2)\n",
        "```\n",
        "- **原理**：将向量拆分为多个“头”（类似多组独立的眼睛），每组关注不同的语义关系。\n",
        "- **示例**：\n",
        "  - 头1关注“语法关系”（如主谓宾）\n",
        "  - 头2关注“语义关系”（如同义词）\n",
        "  - 头3关注“指代关系”（如代词指向）\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 3：计算注意力分数（社交匹配度）**\n",
        "```python\n",
        "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)\n",
        "```\n",
        "- **数学意义**：通过点积计算Query和Key的相似度。\n",
        "- **直观解释**：\n",
        "  - 点积越大 → 问题与答案越相关\n",
        "  - 除以`sqrt(d_k)`：防止维度过高导致点积过大，让梯度更稳定\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 4：Softmax归一化（分配注意力权重）**\n",
        "```python\n",
        "attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "```\n",
        "- **作用**：将分数转化为概率分布，权重总和为1。\n",
        "- **示例**：\n",
        "  ```\n",
        "  \"猫吃鱼\"中，“吃”的注意力权重可能是：\n",
        "  猫: 0.7, 吃: 0.1, 鱼: 0.2\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 5：加权聚合（信息整合）**\n",
        "```python\n",
        "attn_output = torch.matmul(attn_weights, V)\n",
        "```\n",
        "- **意义**：用注意力权重对Value加权求和，得到当前词的新表示。\n",
        "- **效果**：\n",
        "  - 若“苹果”关注“股价”→ 新表示包含公司信息\n",
        "  - 若“苹果”关注“红色”→ 新表示包含水果信息\n",
        "\n",
        "---\n",
        "\n",
        "#### **步骤 6：多头拼接（综合多视角信息）**\n",
        "```python\n",
        "attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "```\n",
        "- **原理**：将多个头的结果拼接，恢复原始维度，综合不同视角的信息。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. 自注意力的独特优势**\n",
        "| 特性                | 说明                          | 类比                      |\n",
        "|---------------------|-----------------------------|-------------------------|\n",
        "| **动态权重**          | 每个位置的关注权重实时计算          | 实时调整社交圈               |\n",
        "| **并行计算**          | 所有位置的关系可同时计算            | 多人同时交谈                 |\n",
        "| **长距离依赖**        | 直接捕捉任意距离的词语关系          | 跨房间对话                  |\n",
        "| **可解释性**          | 注意力权重可视化显示词语关联        | 社交关系图谱                |\n",
        "\n",
        "---\n",
        "\n",
        "### **5. 对比传统方法**\n",
        "|                  | RNN                        | CNN                | Self-Attention         |\n",
        "|------------------|----------------------------|--------------------|------------------------|\n",
        "| **计算方式**      | 顺序计算（依赖前一时刻）          | 局部窗口滑动         | 全连接并行计算            |\n",
        "| **长距离依赖**    | 容易衰减（梯度消失）             | 需要多层堆叠         | 直接捕捉（一步计算）        |\n",
        "| **可并行性**      | 差                         | 中等               | 优                     |\n",
        "| **示例**         | “I am from China”→处理到“China”时可能已忘记“I” | 只能看到局部词语组合 | 让“China”直接关注“I”    |\n",
        "\n",
        "---\n",
        "\n",
        "### **6. 终极总结**\n",
        "自注意力机制通过三个关键设计：\n",
        "1. **Q/K/V角色分离**：实现“提问-回答-整合”的社交逻辑\n",
        "2. **多头机制**：多视角观察上下文关系\n",
        "3. **动态权重**：根据语境灵活调整关注重点\n",
        "\n",
        "让每个词语从“孤立个体”变成“社交达人”，最终使模型理解：\n",
        "- *“bank”在“河边”和“金融”的不同含义*\n",
        "- *“他”指的是前文提到的哪个名词*\n",
        "- *“虽然...但是...”之间的转折关系*\n",
        "\n",
        "这正是Transformer模型超越传统方法的核心魔法！ 🧙♂️"
      ],
      "metadata": {
        "id": "EgCV-xfJq1ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. 多头注意力机制\n",
        "\n",
        "    多头注意力机制是自注意力机制的扩展，它允许模型同时关注序列中不同的关系。具体来说，多头注意力机制将输入序列分成多个“头”，并在每个头上独立计算自注意力，然后将所有头的结果拼接起来。\n",
        "\n"
      ],
      "metadata": {
        "id": "Ol0nkhwQsTLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **多头拼接：像“侦探团队破案”一样整合信息**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. 通俗比喻：侦探团队协作**\n",
        "想象你是一个侦探队长，调查一起复杂案件：\n",
        "- **单头注意力**：你独自分析所有线索，可能因为视角单一忽略关键细节。\n",
        "- **多头注意力**：你组建了一个侦探小队，每个侦探专注不同方向：\n",
        "  - **侦探A**：研究时间线（关注词语出现的顺序）\n",
        "  - **侦探B**：分析人物关系（关注代词指向）\n",
        "  - **侦探C**：观察情感倾向（关注褒贬义词）\n",
        "  - **侦探D**：梳理逻辑结构（关注连词和转折词）\n",
        "\n",
        "**最终破案**：每个侦探提交自己的调查报告，你把这些报告的关键结论拼接起来，得到完整真相。\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 代码拆解：如何实现“侦探团队协作”**\n",
        "```python\n",
        "# 假设输入向量维度d_model=512，头数n_heads=8\n",
        "# 每个头的维度d_k = 512 / 8 = 64\n",
        "\n",
        "# 步骤1：分割多头（组建侦探团队）\n",
        "Q = Q.view(batch_size, seq_len, 8, 64).transpose(1, 2)  # 形状变为 (batch_size, 8, seq_len, 64)\n",
        "# 现在Q被拆分成8个侦探（头），每个侦探携带64维的观察工具\n",
        "\n",
        "# 步骤2：各头独立工作（侦探各自调查）\n",
        "attn_output = ...  # 每个头独立计算得到 (batch_size, 8, seq_len, 64)\n",
        "\n",
        "# 步骤3：拼接结果（汇总调查报告）\n",
        "attn_output = attn_output.transpose(1, 2)  # 形状变为 (batch_size, seq_len, 8, 64)\n",
        "attn_output = attn_output.contiguous().view(batch_size, seq_len, 512)  # 拼接后恢复512维\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. 关键设计：为什么不是简单相加？**\n",
        "假设每个侦探的调查报告是64页纸：\n",
        "- **直接相加**：把8份64页的报告叠加，会得到一堆混乱的重复信息。\n",
        "- **拼接后线性变换**：把8份报告首尾相连（共8×64=512页），然后用一个智能文件柜（`W_O`线性层）重新整理：\n",
        "  - 合并相似结论（如多个侦探都提到时间矛盾）\n",
        "  - 过滤冗余信息（如重复的现场描述）\n",
        "  - 突出关键线索（如某个侦探发现的指纹细节）\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. 实例解析：处理句子 \"他打了那个骂他朋友的人\"**\n",
        "- **侦探A（语法分析头）**：\n",
        "  - 发现“打”是动词，关注施动者“他”和受动者“人”\n",
        "  \n",
        "- **侦探B（指代分析头）**：\n",
        "  - 确定第一个“他”和第二个“他”指同一人\n",
        "  - “朋友”属于第一个“他”\n",
        "\n",
        "- **侦探C（逻辑关系头）**：\n",
        "  - 分析“骂”是“打”的原因\n",
        "  \n",
        "- **拼接后**：\n",
        "  ```python\n",
        "  # 每个头输出的64维向量可能包含：\n",
        "  [侦探A的语法结论] + [侦探B的指代结论] + [侦探C的逻辑结论] + ...其他侦探结论\n",
        "  ```\n",
        "  - 最终通过`W_O`线性层，模型理解：“他因为朋友被骂而打人”\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. 多头机制的三大优势**\n",
        "| 优势                | 解释                          | 类比                      |\n",
        "|---------------------|-----------------------------|-------------------------|\n",
        "| **信息多样性**       | 不同头捕获不同类型的语义关系        | 多学科专家会诊              |\n",
        "| **抗干扰能力**       | 即使某个头失效，其他头仍可补救       | 团队中有人犯错不影响最终结论     |\n",
        "| **高效并行**         | 各头独立计算，充分利用GPU并行能力    | 侦探们同时调查不同线索          |\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. 和单头注意力的直观对比**\n",
        "- **单头效果**：\n",
        "  ```python\n",
        "  # 假设所有头合并成一个512维的大头\n",
        "  attn_output = [单一侦探的512页冗长报告]\n",
        "  ```\n",
        "  - 容易陷入局部观察（例如只关注语法忽略情感）\n",
        "  - 处理复杂句子时准确率下降\n",
        "\n",
        "- **多头效果**：\n",
        "  ```python\n",
        "  # 8个头各司其职\n",
        "  attn_output = [语法报告64页] + [指代报告64页] + [情感报告64页] + ...\n",
        "  ```\n",
        "  - 各领域专家分工协作\n",
        "  - 对歧义句子的处理更精准（如“我喜欢苹果”中识别是水果还是品牌）\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. 可视化理解：拼接就像乐高积木**\n",
        "- **每个头**：一块特定形状的乐高积木（如齿轮、车轮、窗户）\n",
        "- **拼接**：把不同形状的积木首尾相连，组合成一辆完整的乐高汽车\n",
        "- **线性层（`W_O`）**：调整积木位置和角度，让车轮在底部、窗户在顶部，最终成为可行驶的汽车\n",
        "\n",
        "---\n",
        "\n",
        "### **终极总结**\n",
        "多头拼接的精髓在于：\n",
        "1. **分工**：让不同的“子专家”（头）关注不同维度的信息\n",
        "2. **协作**：通过拼接和线性变换整合多视角观察结果\n",
        "3. **升华**：最终输出比任何单头更全面、更准确的语义表示\n",
        "\n",
        "就像通过多棱镜观察白光会分解出七彩颜色，多头机制让模型从多个角度“折射”语义，最终合成更丰富的理解！ 🌈"
      ],
      "metadata": {
        "id": "RawXLiVzs5Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(d_model, n_heads)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 多头自注意力的输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算自注意力\n",
        "        attn_output = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        # dropout + 残差连接 + layer norm\n",
        "        output = self.dropout(attn_output) + Q\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "g24Svh7pq4VS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len = Q.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(d_model, n_heads)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 多头自注意力的输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算自注意力\n",
        "        attn_output = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        # dropout + 残差连接 + layer norm\n",
        "        output = self.dropout(attn_output) + Q\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 1. 定义超参数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "n_heads = 8  # 注意力头的数量\n",
        "batch_size = 32\n",
        "seq_len = 64\n",
        "\n",
        "# 2. 创建 MultiHeadAttention 实例\n",
        "multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "\n",
        "# 3. 创建随机输入张量 Q, K, V\n",
        "# 假设我们有形状为 (batch_size, seq_len, d_model) 的输入张量\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "mask = torch.randint(0, 2, (batch_size, 1, seq_len, seq_len)).bool()  # 生成随机的布尔mask\n",
        "\n",
        "# 4. 将输入张量传递给 MultiHeadAttention 实例\n",
        "output_tensor = multi_head_attention(Q, K, V, mask)\n",
        "\n",
        "# 5. 打印输入和输出的形状\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"K shape:\", K.shape)\n",
        "print(\"V shape:\", V.shape)\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n",
        "# 6. 打印输入张量的内容\n",
        "print(\"Q:\", Q)\n",
        "print(\"K:\", K)\n",
        "print(\"V:\", V)\n",
        "print(\"Mask:\", mask)\n",
        "\n",
        "# 7. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MIGvfP9tzI1",
        "outputId": "cbefde1f-344a-4797-d2b8-5544be80066c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([32, 64, 512])\n",
            "K shape: torch.Size([32, 64, 512])\n",
            "V shape: torch.Size([32, 64, 512])\n",
            "Mask shape: torch.Size([32, 1, 64, 64])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Q: tensor([[[ 4.0043e-01,  1.2000e-01, -1.2407e+00,  ...,  8.2515e-01,\n",
            "           1.7532e+00,  1.9152e-01],\n",
            "         [-5.8081e-01,  9.4857e-01, -9.8336e-01,  ..., -1.5724e-01,\n",
            "           4.0996e-01, -7.3585e-01],\n",
            "         [-2.5012e-01,  1.5524e-01, -1.0508e+00,  ..., -2.0908e+00,\n",
            "          -3.1061e-01, -1.6027e+00],\n",
            "         ...,\n",
            "         [-1.9709e+00,  4.1245e-01, -2.2308e-01,  ...,  4.2023e-01,\n",
            "           1.8654e+00, -1.3326e-01],\n",
            "         [-9.2165e-02,  9.5081e-01,  7.0895e-01,  ..., -2.7355e-01,\n",
            "           5.6614e-01, -1.2911e+00],\n",
            "         [ 1.2471e+00,  3.7250e-01, -2.5672e+00,  ..., -2.6930e+00,\n",
            "          -3.6622e-01,  1.1147e-01]],\n",
            "\n",
            "        [[ 8.2169e-01,  1.1999e+00, -1.8431e+00,  ..., -1.1390e-01,\n",
            "          -3.1040e-01, -5.0482e-01],\n",
            "         [-9.4894e-01,  1.0505e+00,  5.8862e-01,  ..., -2.2605e+00,\n",
            "           1.1448e+00,  1.8109e-01],\n",
            "         [-6.5853e-01,  1.0601e+00, -2.5214e-01,  ...,  1.1045e+00,\n",
            "          -1.9693e+00,  1.7230e-01],\n",
            "         ...,\n",
            "         [ 6.1115e-01, -2.2477e-01, -7.4464e-01,  ...,  1.3715e-01,\n",
            "           8.7666e-01,  5.2393e-01],\n",
            "         [ 1.0230e+00, -1.8287e-01,  2.6706e-01,  ..., -1.0939e+00,\n",
            "          -7.8705e-01,  8.0550e-01],\n",
            "         [-2.7602e-01,  1.7633e-01, -1.0356e+00,  ..., -1.7116e+00,\n",
            "           7.7563e-01, -1.4424e-02]],\n",
            "\n",
            "        [[ 6.1808e-01, -7.7306e-01, -2.5720e-01,  ...,  7.7136e-01,\n",
            "           8.6811e-01,  1.4129e+00],\n",
            "         [ 1.3108e-01, -1.6071e+00, -4.2144e-01,  ...,  5.7542e-01,\n",
            "          -1.0175e+00, -1.4508e+00],\n",
            "         [-2.5724e-03, -9.4662e-01,  1.0135e+00,  ..., -1.4367e+00,\n",
            "           1.1421e-01, -3.1277e-01],\n",
            "         ...,\n",
            "         [ 3.8429e-01,  1.3638e+00, -4.2164e-01,  ...,  6.0798e-01,\n",
            "          -2.4670e-01, -1.1906e+00],\n",
            "         [ 6.0171e-02, -7.3732e-01,  3.2662e-03,  ..., -3.1611e-01,\n",
            "          -5.1592e-01,  7.6529e-01],\n",
            "         [ 3.1506e-01, -1.8283e-01,  5.9583e-01,  ..., -1.0476e+00,\n",
            "           5.6508e-01, -4.6210e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.7946e-01, -3.9611e-01,  2.1876e+00,  ..., -1.6656e-01,\n",
            "          -1.0071e+00,  4.4650e-01],\n",
            "         [ 9.2927e-01,  5.5162e-01,  5.4734e-01,  ..., -1.2247e-01,\n",
            "          -2.1374e+00, -9.1859e-01],\n",
            "         [-6.4739e-01,  1.1199e+00,  1.1990e+00,  ..., -5.2637e-01,\n",
            "          -2.1159e+00, -7.0756e-01],\n",
            "         ...,\n",
            "         [ 8.8240e-01,  8.9620e-01,  1.8081e-01,  ..., -1.3111e+00,\n",
            "          -8.8803e-01, -4.7788e-01],\n",
            "         [ 9.5819e-01,  6.9596e-01,  7.5444e-01,  ...,  2.5713e-01,\n",
            "          -2.5715e-01, -2.6134e+00],\n",
            "         [-6.0232e-01,  6.2449e-01,  1.4722e+00,  ...,  1.9474e-01,\n",
            "           2.2499e-01, -2.0174e+00]],\n",
            "\n",
            "        [[ 1.9533e-01, -9.4186e-01,  9.6473e-01,  ...,  2.6385e-01,\n",
            "          -2.7405e-01, -1.6158e+00],\n",
            "         [-2.8306e-01, -6.0110e-01, -1.0913e+00,  ..., -6.4083e-01,\n",
            "          -7.9755e-01, -1.3652e+00],\n",
            "         [ 4.4169e-02, -5.9013e-01, -1.6729e+00,  ..., -1.3820e+00,\n",
            "          -1.0057e+00,  1.8582e-01],\n",
            "         ...,\n",
            "         [-9.3459e-01, -8.9034e-01, -1.5318e+00,  ...,  9.3273e-01,\n",
            "          -7.8417e-01,  1.9361e-01],\n",
            "         [-1.4279e+00, -2.0537e+00, -4.2001e-01,  ..., -1.0733e+00,\n",
            "           9.1634e-01, -1.0950e+00],\n",
            "         [-1.2504e+00, -5.3675e-02, -1.3467e+00,  ...,  2.5043e-01,\n",
            "          -1.4477e+00, -2.1050e-01]],\n",
            "\n",
            "        [[ 4.7054e-01, -1.7713e+00,  2.5526e-01,  ...,  1.1734e+00,\n",
            "           1.1004e+00,  8.6809e-01],\n",
            "         [-7.2959e-01,  1.1614e-01, -5.8152e-01,  ..., -1.4641e+00,\n",
            "           1.5802e+00, -4.2481e-01],\n",
            "         [ 1.1981e+00, -2.7103e-02, -7.8439e-01,  ..., -1.5093e+00,\n",
            "          -5.1100e-01, -2.7642e-01],\n",
            "         ...,\n",
            "         [ 5.3259e-01,  1.1746e+00, -1.7549e+00,  ...,  2.4264e+00,\n",
            "           1.7469e+00,  7.6577e-01],\n",
            "         [ 1.8130e-01, -1.5057e+00, -1.6249e+00,  ..., -4.7096e-01,\n",
            "          -9.2607e-01,  1.3812e+00],\n",
            "         [-6.6221e-01,  6.9302e-01, -2.2876e-02,  ...,  9.3791e-01,\n",
            "           4.1438e-01,  1.0837e-02]]])\n",
            "K: tensor([[[ 1.3567,  0.3170,  0.6033,  ...,  0.7244, -1.4029, -0.8390],\n",
            "         [-1.3380, -1.3849, -1.3817,  ..., -0.0437,  0.2094,  0.6641],\n",
            "         [-0.0123,  0.9498,  0.0819,  ...,  0.2850,  0.4545,  0.3494],\n",
            "         ...,\n",
            "         [ 0.4864, -0.5633,  0.8985,  ..., -0.0237, -0.6293,  2.4386],\n",
            "         [-0.8044,  0.8000, -0.5001,  ...,  2.1887, -1.1960, -0.4056],\n",
            "         [-1.6975, -0.1495, -2.0734,  ...,  0.6487,  0.4424,  0.2848]],\n",
            "\n",
            "        [[ 0.9230, -0.8072,  0.0822,  ...,  0.8131, -0.6680,  0.0744],\n",
            "         [-0.4427,  0.0536,  0.5238,  ...,  0.4638,  0.8359, -0.6724],\n",
            "         [ 0.6115,  1.0054, -0.2709,  ..., -0.0813, -0.1205,  1.2834],\n",
            "         ...,\n",
            "         [ 1.9442,  2.0719,  0.5976,  ...,  0.7088,  1.9267,  0.2215],\n",
            "         [-0.5868, -0.5678,  2.8784,  ...,  0.4956, -0.3375, -0.1626],\n",
            "         [ 1.0209,  1.5003, -0.4658,  ...,  0.4349,  0.1180, -1.1588]],\n",
            "\n",
            "        [[ 0.4078,  0.6293,  0.2838,  ...,  1.4471, -0.6257,  1.5900],\n",
            "         [ 1.6613, -1.0190,  1.1743,  ...,  0.6830, -2.0168,  0.1059],\n",
            "         [-1.3215,  1.4311,  0.8605,  ...,  1.5520,  0.3879, -0.1973],\n",
            "         ...,\n",
            "         [-0.1697,  0.7587, -0.7133,  ..., -0.5546,  2.1145,  0.7913],\n",
            "         [-1.0198, -0.7633,  0.4897,  ..., -0.0211,  0.4631,  0.5498],\n",
            "         [ 0.6148, -0.3924,  0.9033,  ...,  0.2721, -0.4206, -0.0196]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4337, -1.2444, -0.3100,  ...,  0.2153, -1.0315,  0.1994],\n",
            "         [ 0.7206,  0.6203, -0.0998,  ...,  0.7854,  0.5563,  0.3423],\n",
            "         [-0.4116, -0.7265, -0.8585,  ...,  0.4760,  1.1460,  0.8690],\n",
            "         ...,\n",
            "         [-0.4943,  0.6215, -0.4233,  ...,  0.4325, -0.1668, -0.0260],\n",
            "         [-1.1376,  0.2922,  0.6282,  ...,  0.1262, -0.7060, -1.4729],\n",
            "         [ 1.5015,  0.6096,  1.1407,  ...,  0.0865,  0.2374, -1.6533]],\n",
            "\n",
            "        [[ 0.9907, -0.0114, -0.2323,  ...,  0.7015,  0.3967, -1.5932],\n",
            "         [-0.6089,  0.3023,  0.7282,  ...,  0.1969,  0.4626,  0.6219],\n",
            "         [ 0.2035, -0.3966,  0.0073,  ..., -0.5479,  1.3460,  0.2572],\n",
            "         ...,\n",
            "         [ 0.3662, -0.9642,  0.4974,  ..., -2.7262, -0.0969,  0.2418],\n",
            "         [-0.1497,  1.8754, -0.2480,  ...,  0.2305,  0.1858,  0.8190],\n",
            "         [-0.7312,  0.8038,  0.2865,  ...,  0.2645,  0.6305, -1.7406]],\n",
            "\n",
            "        [[-0.7947,  0.2142,  0.9832,  ...,  0.5291, -1.4194, -1.5685],\n",
            "         [-1.7303,  1.1687, -1.0514,  ...,  0.0228, -1.3293,  0.5375],\n",
            "         [-0.6708,  0.0358, -0.7072,  ..., -1.1628,  0.5450,  1.1842],\n",
            "         ...,\n",
            "         [ 0.7134,  0.8373,  0.9533,  ..., -1.3560, -0.8280, -1.6074],\n",
            "         [-2.3301, -0.3021,  1.0546,  ...,  1.5641,  0.3032,  1.5813],\n",
            "         [ 0.3580, -0.7092,  0.0383,  ..., -2.1285, -1.1555,  0.9192]]])\n",
            "V: tensor([[[ 0.2185, -0.1988, -0.3499,  ...,  1.7556, -1.8297, -0.6424],\n",
            "         [-0.6851,  0.5837,  1.0750,  ...,  1.8819, -0.1125, -0.4461],\n",
            "         [-0.2311,  2.1259,  0.5467,  ..., -0.6277, -1.2541, -0.6931],\n",
            "         ...,\n",
            "         [ 0.2819,  0.3182,  0.6855,  ..., -0.8766, -0.1851,  0.8528],\n",
            "         [ 0.3205, -0.6810,  0.4009,  ..., -0.2609, -0.3787, -0.9526],\n",
            "         [-0.6170, -0.5553, -0.5112,  ...,  1.6684,  1.9822, -1.8975]],\n",
            "\n",
            "        [[ 2.2862,  0.7739, -0.8201,  ...,  0.8780,  0.0911,  0.3832],\n",
            "         [ 1.3930,  0.3524,  1.7911,  ..., -0.1423,  0.4275, -1.1694],\n",
            "         [-0.4544, -2.9722, -1.0419,  ..., -0.3736,  0.5872,  0.4091],\n",
            "         ...,\n",
            "         [ 0.4769, -0.6331, -0.5370,  ...,  1.1734,  1.7088,  0.9718],\n",
            "         [ 0.2499, -0.6814, -2.0829,  ..., -0.1855, -1.1468,  1.0019],\n",
            "         [ 0.1133, -0.1834,  1.0237,  ...,  0.7818, -1.0322,  0.6761]],\n",
            "\n",
            "        [[-0.9685, -0.4727, -0.1046,  ..., -1.6343,  1.2923,  0.5816],\n",
            "         [-0.1873,  0.7102,  1.3027,  ..., -0.4151, -0.0111, -0.1707],\n",
            "         [ 0.6378, -0.3541, -1.1051,  ...,  1.1329, -0.8553, -0.4469],\n",
            "         ...,\n",
            "         [-0.0061,  0.0732, -1.1133,  ...,  2.1188,  0.8759, -0.0259],\n",
            "         [ 0.1900, -1.9353, -0.8281,  ..., -0.9693,  1.1549,  0.0099],\n",
            "         [ 1.2773, -0.6319, -1.2761,  ...,  2.3250, -0.4231,  0.7063]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.0212,  1.5142,  1.2698,  ...,  0.5984,  0.6104,  1.8875],\n",
            "         [-1.0174, -1.2696, -0.2279,  ..., -0.1399, -0.1829, -1.0565],\n",
            "         [-0.4061, -0.2713, -0.8154,  ...,  0.0199, -0.9955,  1.6395],\n",
            "         ...,\n",
            "         [-0.7492,  0.0126,  0.8961,  ..., -1.0024,  0.4162, -1.1100],\n",
            "         [ 0.3195,  0.9740,  1.6826,  ..., -0.0710, -0.8718,  0.8793],\n",
            "         [ 1.4981, -1.0230, -0.3889,  ..., -1.1267, -1.0063,  0.3717]],\n",
            "\n",
            "        [[ 1.1071, -0.1071, -1.2613,  ...,  0.8823, -0.9590,  2.2077],\n",
            "         [-0.1793, -0.8685,  0.7407,  ..., -0.1507, -0.4305, -0.9316],\n",
            "         [ 0.5771, -1.0010, -1.5254,  ..., -0.0723, -0.9062, -2.5203],\n",
            "         ...,\n",
            "         [ 0.5336, -0.6252, -1.3380,  ..., -1.1501, -2.1911,  0.7425],\n",
            "         [ 1.2763,  0.4906, -0.4745,  ..., -0.5281,  0.1694, -0.5869],\n",
            "         [-0.2117, -0.7070, -0.6125,  ...,  0.4599, -0.7445, -0.0165]],\n",
            "\n",
            "        [[-0.2133, -0.4953,  1.1243,  ..., -0.3054, -0.6902, -1.1002],\n",
            "         [ 0.8986, -0.5019, -0.8444,  ...,  0.9653, -1.5524, -1.3574],\n",
            "         [-1.9650, -0.1254,  1.3210,  ...,  0.2682, -0.1831, -0.3801],\n",
            "         ...,\n",
            "         [-0.7343,  0.4713,  1.1538,  ..., -2.8486,  0.7067, -2.6538],\n",
            "         [ 0.6348,  0.1307,  0.8992,  ...,  0.6310, -0.3845, -0.0405],\n",
            "         [-0.9054, -0.4357, -0.5417,  ..., -1.1858,  2.3032,  0.4454]]])\n",
            "Mask: tensor([[[[ True, False,  True,  ...,  True,  True, False],\n",
            "          [ True,  True,  True,  ..., False, False, False],\n",
            "          [ True,  True,  True,  ...,  True,  True,  True],\n",
            "          ...,\n",
            "          [ True, False,  True,  ...,  True,  True,  True],\n",
            "          [False,  True, False,  ...,  True, False,  True],\n",
            "          [False, False, False,  ..., False,  True,  True]]],\n",
            "\n",
            "\n",
            "        [[[ True, False, False,  ..., False,  True, False],\n",
            "          [False,  True, False,  ..., False, False, False],\n",
            "          [ True,  True,  True,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [ True, False, False,  ..., False,  True,  True],\n",
            "          [False, False,  True,  ..., False,  True,  True],\n",
            "          [False, False,  True,  ..., False, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False, False, False,  ...,  True,  True,  True],\n",
            "          [ True,  True,  True,  ..., False, False,  True],\n",
            "          [False,  True, False,  ..., False, False,  True],\n",
            "          ...,\n",
            "          [ True,  True, False,  ...,  True,  True,  True],\n",
            "          [ True,  True, False,  ..., False, False, False],\n",
            "          [ True,  True, False,  ..., False,  True,  True]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[False,  True,  True,  ..., False, False, False],\n",
            "          [ True, False, False,  ..., False, False,  True],\n",
            "          [False,  True,  True,  ...,  True,  True, False],\n",
            "          ...,\n",
            "          [ True, False,  True,  ..., False, False,  True],\n",
            "          [ True,  True,  True,  ..., False,  True, False],\n",
            "          [False,  True, False,  ...,  True, False, False]]],\n",
            "\n",
            "\n",
            "        [[[False,  True, False,  ..., False, False, False],\n",
            "          [ True, False,  True,  ...,  True, False, False],\n",
            "          [ True,  True,  True,  ..., False, False, False],\n",
            "          ...,\n",
            "          [ True,  True, False,  ...,  True,  True,  True],\n",
            "          [False, False,  True,  ...,  True, False, False],\n",
            "          [ True,  True, False,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False,  True, False,  ..., False,  True, False],\n",
            "          [False, False,  True,  ..., False, False,  True],\n",
            "          [False,  True,  True,  ...,  True, False, False],\n",
            "          ...,\n",
            "          [ True,  True,  True,  ..., False, False, False],\n",
            "          [False, False,  True,  ..., False,  True, False],\n",
            "          [ True, False, False,  ..., False, False,  True]]]])\n",
            "Output tensor: tensor([[[ 4.0606e-01,  1.2599e-01, -1.3136e+00,  ...,  8.8883e-01,\n",
            "           1.7545e+00,  2.0180e-01],\n",
            "         [-5.2127e-01,  1.0875e+00, -1.0655e+00,  ..., -1.5268e-01,\n",
            "           4.2818e-01, -7.4090e-01],\n",
            "         [-2.9961e-01,  6.6762e-02, -1.1455e+00,  ..., -1.9533e+00,\n",
            "          -3.0267e-01, -1.5339e+00],\n",
            "         ...,\n",
            "         [-2.0067e+00,  4.3661e-01, -2.1048e-01,  ...,  5.1459e-01,\n",
            "           1.8558e+00, -8.4968e-02],\n",
            "         [-1.0812e-01,  9.2255e-01,  7.0127e-01,  ..., -1.9044e-01,\n",
            "           5.2835e-01, -1.2098e+00],\n",
            "         [ 1.2410e+00,  3.7678e-01, -2.5530e+00,  ..., -2.6010e+00,\n",
            "          -3.4015e-01,  2.0809e-01]],\n",
            "\n",
            "        [[ 7.9111e-01,  1.3055e+00, -1.9869e+00,  ..., -1.8175e-01,\n",
            "          -3.3512e-01, -4.6879e-01],\n",
            "         [-8.9278e-01,  1.0709e+00,  5.6577e-01,  ..., -2.1466e+00,\n",
            "           1.0850e+00,  2.2147e-01],\n",
            "         [-6.2564e-01,  1.1529e+00, -2.0038e-01,  ...,  1.0638e+00,\n",
            "          -1.9518e+00,  2.1083e-01],\n",
            "         ...,\n",
            "         [ 6.2343e-01, -1.1647e-01, -5.9900e-01,  ...,  1.6485e-01,\n",
            "           1.0216e+00,  6.8809e-01],\n",
            "         [ 1.0053e+00, -1.1566e-01,  2.9358e-01,  ..., -1.1135e+00,\n",
            "          -6.5075e-01,  9.5419e-01],\n",
            "         [-2.6931e-01,  2.5856e-01, -9.2093e-01,  ..., -1.7265e+00,\n",
            "           8.3211e-01,  1.9387e-02]],\n",
            "\n",
            "        [[ 6.1969e-01, -8.5818e-01, -1.6398e-01,  ...,  5.6731e-01,\n",
            "           8.5788e-01,  1.3290e+00],\n",
            "         [ 1.8459e-01, -1.4320e+00, -1.9584e-01,  ...,  6.7971e-01,\n",
            "          -8.9295e-01, -1.3907e+00],\n",
            "         [-4.9037e-02, -9.4964e-01,  1.0692e+00,  ..., -1.4618e+00,\n",
            "           1.2820e-01, -3.5225e-01],\n",
            "         ...,\n",
            "         [ 4.9231e-01,  1.3913e+00, -3.9442e-01,  ...,  5.3274e-01,\n",
            "          -1.0238e-01, -1.2605e+00],\n",
            "         [ 1.2220e-01, -7.1991e-01,  6.8249e-02,  ..., -3.3142e-01,\n",
            "          -4.7006e-01,  7.5737e-01],\n",
            "         [ 3.2830e-01, -1.3496e-01,  7.0225e-01,  ..., -1.0962e+00,\n",
            "           6.0975e-01, -4.9893e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.0644e-01, -4.7030e-01,  2.1741e+00,  ..., -3.1356e-01,\n",
            "          -1.0448e+00,  4.7191e-01],\n",
            "         [ 9.3858e-01,  4.7313e-01,  6.8201e-01,  ..., -2.0613e-01,\n",
            "          -2.0885e+00, -8.7329e-01],\n",
            "         [-6.5562e-01,  1.1269e+00,  1.2358e+00,  ..., -6.0059e-01,\n",
            "          -2.1007e+00, -5.8162e-01],\n",
            "         ...,\n",
            "         [ 9.0895e-01,  9.7657e-01,  1.9477e-01,  ..., -1.5050e+00,\n",
            "          -8.4780e-01, -4.0655e-01],\n",
            "         [ 1.1042e+00,  8.1994e-01,  9.2872e-01,  ...,  3.6969e-01,\n",
            "          -1.4755e-01, -2.5248e+00],\n",
            "         [-6.6884e-01,  6.8104e-01,  1.7089e+00,  ...,  1.9762e-01,\n",
            "           2.2921e-01, -2.0826e+00]],\n",
            "\n",
            "        [[ 2.6923e-01, -8.7302e-01,  1.0625e+00,  ...,  2.5443e-01,\n",
            "          -2.7188e-01, -1.5898e+00],\n",
            "         [-3.3426e-01, -6.1184e-01, -1.0000e+00,  ..., -7.5400e-01,\n",
            "          -8.0790e-01, -1.2717e+00],\n",
            "         [ 9.1633e-02, -6.2508e-01, -1.5619e+00,  ..., -1.3444e+00,\n",
            "          -9.5893e-01,  3.6298e-01],\n",
            "         ...,\n",
            "         [-1.0286e+00, -9.4180e-01, -1.5343e+00,  ...,  8.1726e-01,\n",
            "          -8.2715e-01,  1.5502e-01],\n",
            "         [-1.4984e+00, -1.9555e+00, -4.0147e-01,  ..., -1.1048e+00,\n",
            "           1.0426e+00, -9.8863e-01],\n",
            "         [-1.3531e+00, -8.0180e-02, -1.4013e+00,  ...,  2.8571e-01,\n",
            "          -1.4055e+00, -1.8092e-01]],\n",
            "\n",
            "        [[ 6.7181e-01, -1.6489e+00,  2.5085e-01,  ...,  1.2634e+00,\n",
            "           1.1536e+00,  9.3499e-01],\n",
            "         [-6.5539e-01,  6.0509e-02, -5.4354e-01,  ..., -1.4273e+00,\n",
            "           1.4942e+00, -3.7298e-01],\n",
            "         [ 1.2579e+00, -2.5064e-04, -7.3243e-01,  ..., -1.4724e+00,\n",
            "          -5.7961e-01, -2.6560e-01],\n",
            "         ...,\n",
            "         [ 5.5023e-01,  1.1620e+00, -1.6843e+00,  ...,  2.3411e+00,\n",
            "           1.6863e+00,  6.7107e-01],\n",
            "         [ 2.1205e-01, -1.4906e+00, -1.5757e+00,  ..., -4.1112e-01,\n",
            "          -9.1773e-01,  1.3436e+00],\n",
            "         [-6.6411e-01,  7.4857e-01,  8.0606e-02,  ...,  9.9883e-01,\n",
            "           4.5522e-01,  2.8299e-02]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "这段代码首先定义了 MultiHeadAttention 类，然后创建了一个实例。接着，它生成了随机的输入张量 Q, K, 和 V，以及一个随机的 mask。最后，它将这些张量传递给 MultiHeadAttention 实例，并打印输入和输出张量的形状和内容。\n",
        "\n",
        "通过运行这段代码，你应该能看到输入张量 Q, K, 和 V 的形状和内容，以及输出张量的形状和内容。所有输入张量的形状都是 (batch_size, seq_len, d_model)，输出张量的形状也是 (batch_size, seq_len, d_model)。\n",
        "\n"
      ],
      "metadata": {
        "id": "D1hEQRgdxh_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 前馈神经网络\n",
        "\n",
        "    前馈神经网络（FFN）用于对每个位置的向量进行非线性变换。它由两个线性层组成，中间有一个ReLU激活函数。\n"
      ],
      "metadata": {
        "id": "3MZQhjUguN7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 线性层 + ReLU + 线性层\n",
        "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
        "        output = self.dropout(output) + x  # dropout + 残差连接\n",
        "        output = self.layer_norm(output)  # layer norm\n",
        "        return output"
      ],
      "metadata": {
        "id": "JCpH5N2XuOb3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 线性层 + ReLU + 线性层\n",
        "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
        "        output = self.dropout(output) + x  # dropout + 残差连接\n",
        "        output = self.layer_norm(output)  # layer norm\n",
        "        return output\n",
        "\n",
        "# 1. 定义超参数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "d_ff = 2048  # 前馈神经网络的隐藏层维度\n",
        "batch_size = 32\n",
        "seq_len = 64\n",
        "\n",
        "# 2. 创建 FeedForward 实例\n",
        "feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "# 3. 创建一个随机输入张量\n",
        "# 假设我们有一个形状为 (batch_size, seq_len, d_model) 的输入张量\n",
        "input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 4. 将输入张量传递给 FeedForward 实例\n",
        "output_tensor = feed_forward(input_tensor)\n",
        "\n",
        "# 5. 打印输入和输出的形状\n",
        "print(\"Input tensor shape:\", input_tensor.shape)\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n",
        "# 6. 打印输入张量的内容\n",
        "print(\"Input tensor:\", input_tensor)\n",
        "\n",
        "# 7. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wskzmIsxl99",
        "outputId": "8db3ae9e-e5f6-459d-a8ea-84c406e99a58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([32, 64, 512])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Input tensor: tensor([[[ 0.1017,  1.6102, -1.0562,  ..., -0.9411, -1.3324, -0.1248],\n",
            "         [-0.1562, -0.4736, -0.1512,  ..., -0.0599, -1.6264, -0.1323],\n",
            "         [ 0.7724,  1.8818, -0.3402,  ..., -2.8729,  1.4256, -0.1110],\n",
            "         ...,\n",
            "         [ 0.8633,  0.4225, -1.0925,  ..., -1.1562, -0.8201,  0.5502],\n",
            "         [ 1.4233, -0.9024, -1.3541,  ..., -0.3882,  0.9496, -0.6593],\n",
            "         [ 0.1632, -0.6885,  0.6971,  ...,  1.1138, -0.6352, -0.4801]],\n",
            "\n",
            "        [[-0.6802, -0.2314,  0.2467,  ..., -0.1419, -1.1976,  0.0106],\n",
            "         [-0.7010, -1.3776, -0.8094,  ..., -1.0240, -0.3757, -1.0994],\n",
            "         [ 1.2871, -0.5674,  0.0573,  ..., -0.0879,  0.7120, -2.0591],\n",
            "         ...,\n",
            "         [ 0.3913, -0.4572,  0.5571,  ..., -0.9398,  0.3013,  0.8204],\n",
            "         [ 0.3829, -1.7627, -0.5273,  ..., -1.4363, -0.4933,  1.3425],\n",
            "         [ 0.4029,  1.1732, -0.6661,  ...,  0.2858, -0.5831,  0.2284]],\n",
            "\n",
            "        [[-1.1209,  0.5313, -0.5590,  ...,  0.6048,  1.7480,  0.3853],\n",
            "         [-1.1080,  0.2320, -1.6051,  ..., -0.8423, -0.1877,  0.4053],\n",
            "         [-1.5405,  0.4884, -0.3568,  ...,  0.3018, -0.5108, -0.2754],\n",
            "         ...,\n",
            "         [-0.4018,  1.6936,  1.3448,  ...,  0.4798, -0.5870, -0.4111],\n",
            "         [-0.1225, -0.3630,  0.1534,  ..., -0.8241, -0.6539, -1.7370],\n",
            "         [ 0.3058,  0.3792, -0.7074,  ...,  0.2896,  0.6918, -0.0171]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.8493, -0.4382,  0.3140,  ..., -0.4941,  1.8171,  1.7775],\n",
            "         [ 0.4045, -0.8648, -1.3253,  ..., -2.5158, -1.2421,  1.2107],\n",
            "         [-0.0489,  0.1114, -0.3630,  ..., -1.4116,  0.6596,  1.3034],\n",
            "         ...,\n",
            "         [-0.5503,  1.8578,  0.7758,  ..., -0.9021,  0.7331, -0.3614],\n",
            "         [-1.2986, -1.4383,  1.0447,  ..., -1.4063,  0.1467,  1.3918],\n",
            "         [-1.3417, -0.2955, -0.5402,  ..., -0.9213, -0.1508,  1.2575]],\n",
            "\n",
            "        [[ 0.4908,  1.2008,  0.4595,  ...,  0.1289,  0.3614,  1.4302],\n",
            "         [ 1.7713,  0.4820,  1.1801,  ...,  0.3151, -1.1452,  1.1856],\n",
            "         [ 0.0845, -0.1664, -0.3768,  ..., -0.1982,  0.2652, -0.7430],\n",
            "         ...,\n",
            "         [ 0.4066,  0.9340,  1.2300,  ..., -1.4598,  0.3796,  1.0695],\n",
            "         [-0.4907,  0.4001, -0.5689,  ...,  0.9219, -0.0849,  0.9972],\n",
            "         [ 1.3646, -0.6694,  0.3904,  ..., -0.8166, -0.4566,  0.9904]],\n",
            "\n",
            "        [[-0.3186, -0.4756, -0.6915,  ...,  0.9999, -0.6504, -0.6230],\n",
            "         [-0.7344, -0.5236, -0.4954,  ..., -0.7985, -0.7155,  1.0394],\n",
            "         [ 1.5403,  0.0262,  0.0831,  ..., -1.1722,  0.1489, -0.3941],\n",
            "         ...,\n",
            "         [ 0.5845,  0.1236, -1.2159,  ...,  2.2812, -0.1393, -0.7872],\n",
            "         [ 0.9358,  1.3953,  0.4986,  ...,  0.3087,  0.1024, -0.2178],\n",
            "         [ 1.6239,  1.8535, -0.3071,  ..., -2.2476,  0.3555,  0.4748]]])\n",
            "Output tensor: tensor([[[-1.6601e-01,  1.6249e+00, -1.3081e+00,  ..., -8.1474e-01,\n",
            "          -1.6659e+00, -3.2397e-01],\n",
            "         [ 4.7652e-02, -1.1154e-01, -5.4203e-01,  ..., -3.4706e-01,\n",
            "          -1.9077e+00, -8.9166e-02],\n",
            "         [ 3.2996e-01,  2.3470e+00, -3.9003e-01,  ..., -2.6900e+00,\n",
            "           1.1564e+00, -3.9512e-01],\n",
            "         ...,\n",
            "         [ 6.3778e-01,  7.4950e-01, -1.4235e+00,  ..., -1.6635e+00,\n",
            "          -1.0584e+00,  9.9543e-02],\n",
            "         [ 1.3122e+00, -6.2080e-01, -1.8072e+00,  ..., -3.1534e-01,\n",
            "           6.5972e-01, -1.2778e+00],\n",
            "         [-9.0531e-02, -5.0965e-01,  8.5517e-01,  ...,  8.7850e-01,\n",
            "          -9.0052e-01, -1.0880e+00]],\n",
            "\n",
            "        [[-6.5307e-01, -8.4132e-02,  3.3688e-01,  ...,  1.8253e-01,\n",
            "          -1.8091e+00, -1.7624e-01],\n",
            "         [-8.9078e-01, -1.0286e+00, -5.7192e-01,  ..., -1.1622e+00,\n",
            "          -4.3442e-01, -1.6983e+00],\n",
            "         [ 1.1657e+00, -6.4102e-01, -2.2939e-01,  ..., -2.0661e-01,\n",
            "           6.5511e-01, -2.4922e+00],\n",
            "         ...,\n",
            "         [-1.5140e-03, -1.0797e-01,  6.8549e-01,  ..., -5.0280e-01,\n",
            "           2.7418e-01, -1.0655e-01],\n",
            "         [ 3.8101e-01, -9.4830e-01, -1.4357e-01,  ..., -1.4146e+00,\n",
            "          -5.7823e-01,  4.2659e-01],\n",
            "         [ 2.5676e-01,  1.2340e+00, -7.1733e-01,  ...,  2.3526e-01,\n",
            "          -9.0152e-01, -2.2729e-01]],\n",
            "\n",
            "        [[-1.1331e+00,  4.6740e-01, -3.6032e-01,  ...,  6.4006e-01,\n",
            "           1.2555e+00, -1.8385e-01],\n",
            "         [-1.0014e+00,  8.4542e-01, -1.4552e+00,  ..., -9.4071e-01,\n",
            "          -1.8408e-01, -3.7074e-01],\n",
            "         [-1.6810e+00,  1.2590e+00, -1.9540e-01,  ...,  5.3364e-01,\n",
            "          -7.3130e-01, -5.7235e-01],\n",
            "         ...,\n",
            "         [-3.6034e-01,  2.2614e+00,  1.1407e+00,  ...,  4.9138e-01,\n",
            "          -4.5405e-01, -8.2966e-01],\n",
            "         [-3.0063e-01, -1.3115e-01,  3.0884e-01,  ..., -7.2290e-01,\n",
            "          -1.1109e+00, -2.6149e+00],\n",
            "         [ 5.0475e-01,  4.3311e-01, -8.4864e-01,  ...,  3.7704e-02,\n",
            "           3.7173e-01, -6.4637e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.9970e+00, -1.3532e-01,  1.3510e-01,  ..., -7.5859e-02,\n",
            "           1.3127e+00,  1.6447e+00],\n",
            "         [-8.5189e-03, -8.5614e-01, -1.2361e+00,  ..., -2.4337e+00,\n",
            "          -1.6021e+00,  9.2215e-01],\n",
            "         [-2.1350e-01,  1.8802e-01, -2.9388e-02,  ..., -1.3821e+00,\n",
            "           4.4662e-01,  9.9606e-01],\n",
            "         ...,\n",
            "         [-7.5452e-01,  2.2062e+00,  8.3485e-01,  ..., -7.1657e-01,\n",
            "           3.3011e-01, -4.9515e-01],\n",
            "         [-1.2882e+00, -1.1644e+00,  1.1305e+00,  ..., -1.3911e+00,\n",
            "          -1.7361e-01,  8.9270e-01],\n",
            "         [-1.2512e+00,  2.8053e-01, -1.6158e-01,  ..., -8.6389e-01,\n",
            "          -9.2135e-02,  7.7138e-01]],\n",
            "\n",
            "        [[ 1.3561e-01,  1.0800e+00, -6.7151e-02,  ...,  9.3888e-02,\n",
            "           2.0884e-01,  5.6604e-01],\n",
            "         [ 1.4787e+00,  3.8577e-01,  1.0588e+00,  ...,  1.6579e-01,\n",
            "          -1.5427e+00,  4.8155e-01],\n",
            "         [-3.9883e-01, -1.5952e-01, -1.8096e-01,  ..., -2.0476e-01,\n",
            "          -1.8038e-01, -1.1509e+00],\n",
            "         ...,\n",
            "         [ 3.5608e-02,  1.1381e+00,  1.3135e+00,  ..., -1.1178e+00,\n",
            "          -4.1444e-02,  2.2811e-01],\n",
            "         [-5.2215e-01,  6.8425e-01, -8.7161e-01,  ...,  8.8524e-01,\n",
            "          -1.3944e-01,  5.0014e-01],\n",
            "         [ 9.8817e-01, -9.0017e-02,  6.3859e-01,  ..., -7.3494e-01,\n",
            "          -5.0320e-01,  1.3666e-01]],\n",
            "\n",
            "        [[-4.8609e-01, -5.0793e-01, -7.2345e-01,  ...,  1.3322e+00,\n",
            "          -9.7938e-01, -1.3107e+00],\n",
            "         [-7.2792e-01, -4.2778e-01, -5.3960e-01,  ..., -7.5853e-01,\n",
            "          -8.9699e-01,  2.6135e-02],\n",
            "         [ 1.3524e+00,  5.1498e-01, -8.6406e-02,  ..., -1.2394e+00,\n",
            "          -2.6227e-01, -3.9790e-01],\n",
            "         ...,\n",
            "         [ 1.4481e-01,  1.5583e-01, -1.1098e+00,  ...,  2.1651e+00,\n",
            "          -3.2640e-01, -1.3656e+00],\n",
            "         [ 9.8841e-01,  1.4368e+00,  4.1044e-01,  ...,  3.9441e-01,\n",
            "          -1.2914e-01, -9.6188e-01],\n",
            "         [ 1.5974e+00,  2.2786e+00, -1.4949e-01,  ..., -2.4446e+00,\n",
            "           3.3308e-01, -3.6484e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "这段代码首先定义了 FeedForward 类，然后创建了一个实例。接着，它生成了一个随机的输入张量，模拟一个包含嵌入向量的序列。最后，它将输入张量传递给 FeedForward 实例，并打印输入和输出张量的形状和内容。\n",
        "\n",
        "通过运行这段代码，你应该能看到输入张量和输出张量的形状和内容。输入张量的形状是 (batch_size, seq_len, d_model)，输出张量的形状也是 (batch_size, seq_len, d_model)。输出张量的内容是输入张量经过前馈神经网络处理后的结果。\n",
        "\n"
      ],
      "metadata": {
        "id": "BgRDQwe-xtsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **FeedForward与残差连接：像“滚雪球”一样积累知识**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. 通俗比喻：知识迭代升级**\n",
        "想象你在写一篇论文：\n",
        "- **无残差连接**：每修改一版都要从头重写，之前的思考全部丢弃。\n",
        "- **有残差连接**：在上一版基础上修改，保留有价值的内容，只调整需要改进的部分。\n",
        "\n",
        "**例如**：\n",
        "- 初稿：*“Transformer是一种强大的模型”*\n",
        "- 修改稿（残差思想）：*“Transformer是一种通过自注意力机制实现长距离依赖捕捉的强大模型”*\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 代码拆解：知识如何“滚雪球”**\n",
        "```python\n",
        "class FeedForward(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # 原始输入x：当前知识状态\n",
        "        new_knowledge = self.linear_2(self.relu(self.linear_1(x)))  # 学习新知识\n",
        "        output = self.dropout(new_knowledge) + x  # 新知识叠加到旧知识上（残差连接）\n",
        "        output = self.layer_norm(output)  # 知识标准化\n",
        "        return output\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. 残差连接的四大核心作用**\n",
        "| 作用                | 解释                          | 类比                      |\n",
        "|---------------------|-----------------------------|-------------------------|\n",
        "| **防知识丢失**       | 确保底层重要信息不被覆盖           | 论文保留历史版本             |\n",
        "| **梯度高速公路**     | 允许梯度直接回流到底层             | 建立作者-审稿人的直达沟通通道    |\n",
        "| **渐进式学习**       | 每次只学习需要调整的部分            | 逐步完善论文而非推翻重写        |\n",
        "| **抗过拟合**         | Dropout只作用于新增部分，保留主干   | 修改时只调整局部，保持整体稳定性   |\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. 实例解析：处理句子 \"Deep Learning is fascinating\"**\n",
        "- **输入x**：经过自注意力后的词向量\n",
        "- **FeedForward过程**：\n",
        "  1. 线性层1：提取高阶特征（如将\"fascinating\"关联到\"neural networks\"）\n",
        "  2. ReLU：过滤负值（保留关键特征）\n",
        "  3. 线性层2：降维恢复原始维度\n",
        "  4. **残差相加**：保留自注意力层捕捉的词语关系，叠加FFN提取的语义特征\n",
        "  5. LayerNorm：平衡特征尺度\n",
        "\n",
        "**最终效果**：\n",
        "- 既保留了*自注意力层发现的\"Deep Learning\"与\"fascinating\"的关系*\n",
        "- 又增加了*FFN层理解的\"fascinating\"隐含的\"complex mathematical models\"信息*\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. 残差连接 vs 普通连接**\n",
        "| 场景               | 普通连接                         | 残差连接                          |\n",
        "|--------------------|--------------------------------|---------------------------------|\n",
        "| **梯度传播**        | 梯度逐层衰减，底层更新困难             | 梯度可直达底层，缓解消失问题            |\n",
        "| **深层网络表现**    | 超过20层后准确率下降                 | 100+层仍保持稳定（如GPT-3有96层）       |\n",
        "| **信息保留**        | 逐层覆盖原始信息                    | 像版本控制系统保留所有重要修改           |\n",
        "| **物理意义**        | 强制模型学习绝对变换                 | 让模型专注学习相对调整（残差）           |\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. 可视化理解：残差如同“传送带”**\n",
        "- **传统网络**：货物（数据）每经过一个加工站（网络层），都要完全重新包装\n",
        "  ![](https://example.com/normal_network.gif)\n",
        "- **残差网络**：传送带直接贯通所有加工站，工人（网络参数）只需在传送带上添加新零件\n",
        "  ![](https://example.com/residual_network.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### **终极总结**\n",
        "残差连接的精妙之处在于：\n",
        "1. **加法设计**：`output = F(x) + x` 让网络只需学习差值\n",
        "2. **常态保留**：原始信息始终在场，避免\"一错到底\"\n",
        "3. **动态平衡**：LayerNorm在残差后实施，既规范输出又不压制原始信号\n",
        "\n",
        "就像优秀的作家不断在原有稿子上精修，而不是每次重写，残差连接让神经网络得以在深层结构中稳定积累知识，最终成就了Transformer这类大模型的强大能力！ ✨"
      ],
      "metadata": {
        "id": "DE-j4H8XyxOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 编码器层\n",
        "\n",
        "    编码器层是Transformer编码器的基本组成部分。它由一个多头自注意力子层和一个前馈神经网络子层组成。\n"
      ],
      "metadata": {
        "id": "hLanIcH408S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        output = self.multi_head_attention(x, x, x, mask)\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "RWbtnTQbxzum"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 编码器\n",
        "\n",
        "    编码器由多个相同的编码器层堆叠而成。它用于处理输入序列，并将其转换为一个中间表示，该中间表示将用于解码器生成输出序列。\n"
      ],
      "metadata": {
        "id": "GsMv6zyv2PQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M1EyDdij2P4B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len = Q.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(d_model, n_heads)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 多头自注意力的输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算自注意力\n",
        "        attn_output = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        # dropout + 残差连接 + layer norm\n",
        "        output = self.dropout(attn_output) + Q\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 线性层 + ReLU + 线性层\n",
        "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
        "        output = self.dropout(output) + x  # dropout + 残差连接\n",
        "        output = self.layer_norm(output)  # layer norm\n",
        "        return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        output = self.multi_head_attention(x, x, x, mask)\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# 1. 定义超参数\n",
        "n_layers = 6  # 编码器层数\n",
        "d_model = 512  # 嵌入向量的维度\n",
        "n_heads = 8  # 注意力头的数量\n",
        "d_ff = 2048  # 前馈神经网络的隐藏层维度\n",
        "batch_size = 32\n",
        "seq_len = 64\n",
        "\n",
        "# 2. 创建 Encoder 实例\n",
        "encoder = Encoder(n_layers, d_model, n_heads, d_ff)\n",
        "\n",
        "# 3. 创建一个随机输入张量\n",
        "# 假设我们有一个形状为 (batch_size, seq_len, d_model) 的输入张量\n",
        "input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# 4. 创建一个随机mask\n",
        "# mask 的形状应该是 (batch_size, 1, seq_len, seq_len)\n",
        "mask = torch.randint(0, 2, (batch_size, 1, seq_len, seq_len)).bool()  # 生成随机的布尔mask\n",
        "\n",
        "# 5. 将输入张量传递给 Encoder 实例\n",
        "output_tensor = encoder(input_tensor, mask)\n",
        "\n",
        "# 6. 打印输入和输出的形状\n",
        "print(\"Input tensor shape:\", input_tensor.shape)\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "print(\"Output tensor shape:\", output_tensor.shape)\n",
        "\n",
        "# 7. 打印输入张量的内容\n",
        "print(\"Input tensor:\", input_tensor)\n",
        "print(\"Mask:\", mask)\n",
        "\n",
        "# 8. 打印输出张量的内容\n",
        "print(\"Output tensor:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TTzAN_m1mX8",
        "outputId": "077d6130-fd8c-4ac9-ce94-df4e31abaf1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([32, 64, 512])\n",
            "Mask shape: torch.Size([32, 1, 64, 64])\n",
            "Output tensor shape: torch.Size([32, 64, 512])\n",
            "Input tensor: tensor([[[-1.3209e+00,  4.9295e-01, -4.9741e-01,  ..., -4.9661e-01,\n",
            "          -7.5814e-01, -3.2700e-01],\n",
            "         [ 8.1219e-01, -1.4553e+00,  1.0710e-01,  ...,  1.0356e+00,\n",
            "           4.2538e-01,  1.3326e+00],\n",
            "         [-5.7732e-01, -2.0885e+00, -6.4024e-02,  ..., -2.8878e+00,\n",
            "          -1.9811e-01,  9.6770e-01],\n",
            "         ...,\n",
            "         [ 1.4377e+00,  7.8260e-01,  1.3218e+00,  ..., -7.3485e-01,\n",
            "          -6.6175e-01,  9.2765e-01],\n",
            "         [ 1.5755e+00, -3.2009e-01, -3.8572e-01,  ...,  1.1086e+00,\n",
            "           7.7100e-01,  1.7608e-01],\n",
            "         [-5.9777e-01, -5.1152e-01, -2.3108e-01,  ...,  9.3014e-02,\n",
            "           1.4436e+00,  2.6183e-01]],\n",
            "\n",
            "        [[-2.8499e-01, -5.1307e-01,  1.7653e-01,  ..., -2.4935e-01,\n",
            "           4.8257e-01, -8.5938e-01],\n",
            "         [-6.4622e-01, -1.8943e-02, -3.1036e-01,  ..., -5.1551e-01,\n",
            "           6.1017e-01,  1.2147e+00],\n",
            "         [-1.8190e+00,  1.3590e-01, -1.0278e+00,  ..., -3.0630e-01,\n",
            "          -4.7277e-01,  3.5560e-01],\n",
            "         ...,\n",
            "         [-8.6076e-01, -8.5028e-01, -2.3607e-01,  ..., -6.4566e-01,\n",
            "           1.2620e-01,  2.3971e-01],\n",
            "         [-2.8541e-01,  1.2767e-01,  1.1789e+00,  ...,  1.9657e-01,\n",
            "           5.7476e-02,  1.3552e+00],\n",
            "         [ 7.7624e-02, -1.2493e+00, -3.3598e-02,  ..., -6.1114e-01,\n",
            "           3.1185e-02,  7.8994e-01]],\n",
            "\n",
            "        [[-2.7991e+00, -1.0226e+00,  6.2142e-01,  ...,  6.1084e-01,\n",
            "          -1.3285e-02, -1.3589e-01],\n",
            "         [-1.2275e+00, -6.5146e-01,  1.0495e+00,  ...,  1.1897e+00,\n",
            "           1.1212e+00,  4.4534e-01],\n",
            "         [ 5.5909e-01,  1.4028e+00, -7.7668e-01,  ...,  1.3071e-01,\n",
            "          -6.0804e-01, -6.3694e-01],\n",
            "         ...,\n",
            "         [ 6.1477e-02, -4.4561e-01,  4.3630e-02,  ..., -8.2381e-01,\n",
            "           1.2083e-01,  1.3678e+00],\n",
            "         [-3.5409e-01, -1.3658e+00,  3.0600e-01,  ..., -6.8579e-01,\n",
            "          -7.7028e-01, -3.1599e-01],\n",
            "         [ 2.5673e-02,  1.3088e+00,  6.4708e-01,  ...,  9.4893e-02,\n",
            "           1.3638e+00, -8.5872e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 8.4047e-01,  9.7974e-01, -1.9154e+00,  ..., -1.9570e+00,\n",
            "          -1.1930e+00,  7.6301e-02],\n",
            "         [-8.3166e-01,  7.3757e-01,  1.5266e+00,  ..., -5.8073e-01,\n",
            "           1.4321e+00, -1.9905e-02],\n",
            "         [ 8.0543e-01, -1.0406e+00, -7.5326e-01,  ...,  1.3973e+00,\n",
            "          -4.1070e-01, -2.3521e-01],\n",
            "         ...,\n",
            "         [ 3.8616e-01, -2.9066e-01, -1.0033e+00,  ..., -3.7555e-01,\n",
            "           4.6426e-01,  8.6800e-01],\n",
            "         [ 5.8528e-01,  2.6967e-01,  3.4257e-01,  ...,  5.1022e-01,\n",
            "           1.2575e+00, -8.9567e-01],\n",
            "         [ 8.4141e-01, -1.1850e+00,  1.4135e+00,  ...,  6.4629e-01,\n",
            "           7.3686e-01,  5.0114e-01]],\n",
            "\n",
            "        [[ 6.9904e-01, -9.8310e-01, -9.6281e-01,  ..., -1.3031e+00,\n",
            "           4.8637e-01,  1.6278e+00],\n",
            "         [ 3.2961e-01,  1.1596e+00,  1.2128e+00,  ..., -4.5554e-01,\n",
            "           2.6159e-02,  4.2778e-01],\n",
            "         [ 7.5030e-01,  8.2646e-01, -8.9954e-01,  ..., -1.1333e-01,\n",
            "           4.1546e-01, -9.0850e-01],\n",
            "         ...,\n",
            "         [-2.5348e+00, -1.3726e+00,  7.3816e-01,  ..., -4.7712e-01,\n",
            "          -1.6899e-01, -6.2636e-01],\n",
            "         [ 5.7321e-01, -6.9559e-01, -3.8997e-01,  ...,  6.0864e-01,\n",
            "          -8.5759e-01, -2.9315e-01],\n",
            "         [ 1.9711e+00,  3.6197e-02,  1.3507e+00,  ..., -4.9037e-01,\n",
            "           2.2806e+00,  4.5293e-01]],\n",
            "\n",
            "        [[ 6.0059e-01,  1.6826e-03, -1.6291e-01,  ...,  5.1351e-01,\n",
            "           6.3795e-02, -2.5943e-01],\n",
            "         [-1.1037e+00, -2.9253e-01,  1.9364e-01,  ..., -1.1543e+00,\n",
            "           1.4847e-01, -2.3667e-01],\n",
            "         [ 3.3755e-01, -1.2908e+00,  8.6958e-01,  ...,  6.8538e-01,\n",
            "           2.0241e+00, -1.6874e+00],\n",
            "         ...,\n",
            "         [ 8.8370e-01, -3.3006e-01,  3.2779e-02,  ..., -3.8643e-01,\n",
            "          -9.0792e-03,  8.7755e-01],\n",
            "         [ 3.0222e-01, -1.0177e+00, -1.6684e+00,  ..., -8.3818e-01,\n",
            "           1.6122e+00,  4.9577e-01],\n",
            "         [-1.2174e+00,  8.2486e-01, -1.5836e+00,  ...,  1.6195e+00,\n",
            "           2.2395e-01,  5.8833e-01]]])\n",
            "Mask: tensor([[[[ True,  True,  True,  ...,  True,  True,  True],\n",
            "          [False,  True,  True,  ..., False, False, False],\n",
            "          [ True,  True,  True,  ...,  True,  True,  True],\n",
            "          ...,\n",
            "          [False, False,  True,  ...,  True,  True,  True],\n",
            "          [False, False, False,  ..., False, False, False],\n",
            "          [ True, False,  True,  ..., False,  True,  True]]],\n",
            "\n",
            "\n",
            "        [[[ True, False,  True,  ..., False, False,  True],\n",
            "          [False,  True,  True,  ...,  True,  True,  True],\n",
            "          [ True,  True,  True,  ...,  True,  True, False],\n",
            "          ...,\n",
            "          [False, False, False,  ...,  True, False, False],\n",
            "          [ True, False,  True,  ..., False,  True,  True],\n",
            "          [False,  True, False,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[ True, False, False,  ..., False,  True,  True],\n",
            "          [False,  True,  True,  ..., False,  True, False],\n",
            "          [ True, False, False,  ..., False, False,  True],\n",
            "          ...,\n",
            "          [False,  True,  True,  ..., False,  True,  True],\n",
            "          [ True,  True, False,  ..., False, False,  True],\n",
            "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[False,  True,  True,  ...,  True,  True,  True],\n",
            "          [ True,  True, False,  ...,  True,  True,  True],\n",
            "          [False, False, False,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [False,  True, False,  ..., False,  True,  True],\n",
            "          [ True, False, False,  ..., False, False,  True],\n",
            "          [ True, False, False,  ..., False, False, False]]],\n",
            "\n",
            "\n",
            "        [[[False,  True,  True,  ..., False,  True, False],\n",
            "          [ True,  True,  True,  ...,  True,  True,  True],\n",
            "          [ True,  True,  True,  ...,  True, False, False],\n",
            "          ...,\n",
            "          [False, False, False,  ...,  True,  True, False],\n",
            "          [ True,  True,  True,  ...,  True,  True, False],\n",
            "          [ True, False,  True,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False, False,  True,  ..., False, False,  True],\n",
            "          [False,  True, False,  ...,  True,  True, False],\n",
            "          [ True, False,  True,  ..., False, False, False],\n",
            "          ...,\n",
            "          [ True, False, False,  ...,  True,  True, False],\n",
            "          [ True,  True, False,  ..., False,  True, False],\n",
            "          [False,  True, False,  ...,  True, False,  True]]]])\n",
            "Output tensor: tensor([[[-1.4195, -0.3752, -0.0112,  ..., -1.3258, -0.4991, -1.2466],\n",
            "         [ 0.6808, -1.5952,  1.5475,  ..., -0.5666,  0.8810,  0.8157],\n",
            "         [ 0.1713, -1.5291,  0.6560,  ..., -2.1557,  0.6096, -0.2644],\n",
            "         ...,\n",
            "         [ 0.7631, -0.0942,  0.6829,  ..., -0.5260, -0.4226, -0.5573],\n",
            "         [ 1.0168, -0.9365, -0.4949,  ...,  1.3740,  0.3907, -0.8518],\n",
            "         [-0.2312, -0.7530,  0.4947,  ..., -0.1509,  0.9691, -1.2857]],\n",
            "\n",
            "        [[-0.1053, -1.1236,  0.1582,  ..., -0.6404, -0.6202, -1.9677],\n",
            "         [-1.0347, -1.0534, -0.1362,  ..., -0.0137, -0.0324, -0.8160],\n",
            "         [-1.1017, -1.2810, -0.2158,  ..., -0.6858, -0.6531, -0.7390],\n",
            "         ...,\n",
            "         [-0.5878, -1.0902, -0.4161,  ..., -0.8987,  0.4604, -0.2810],\n",
            "         [-0.1342, -0.3914,  1.4356,  ..., -0.4596,  0.5245,  1.0312],\n",
            "         [ 0.1424, -2.0018,  0.1701,  ..., -0.7091, -0.1677, -0.7314]],\n",
            "\n",
            "        [[-3.3183, -1.8625,  0.6874,  ..., -0.2821, -0.2469, -0.5887],\n",
            "         [-1.3506, -1.2154,  1.1120,  ...,  2.1030,  0.6846, -0.7031],\n",
            "         [ 0.0501, -0.4170, -0.2749,  ..., -1.0221, -0.9392, -1.2907],\n",
            "         ...,\n",
            "         [-0.3863, -1.0060, -0.4745,  ..., -1.3831, -0.4386,  0.6406],\n",
            "         [-0.2531, -1.9475,  0.7529,  ..., -1.6078, -0.7035, -1.4615],\n",
            "         [-1.1139, -0.1099, -0.2205,  ...,  0.6304,  0.9001, -1.3675]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0779,  0.5393, -2.2624,  ..., -1.3616, -1.7973, -0.7689],\n",
            "         [-0.6883, -0.0940,  0.5809,  ...,  0.4686,  0.5606, -0.3304],\n",
            "         [ 0.6029, -1.3691, -0.8689,  ...,  1.3200,  0.1432, -0.5011],\n",
            "         ...,\n",
            "         [ 0.0629, -0.0784, -0.3542,  ..., -0.2581,  0.8453,  0.2986],\n",
            "         [ 0.1132,  0.6897, -0.1646,  ..., -0.5501,  1.6541, -1.3766],\n",
            "         [ 0.6050, -0.9746,  0.6291,  ..., -0.0907,  2.2153,  0.5480]],\n",
            "\n",
            "        [[ 0.2391, -1.4135, -0.3649,  ..., -0.6986,  0.8721,  0.3227],\n",
            "         [-0.9654,  0.2958,  0.5438,  ..., -1.0150,  0.1184, -0.6920],\n",
            "         [-0.2792, -0.2612, -0.7415,  ..., -1.1926,  0.3749, -1.8118],\n",
            "         ...,\n",
            "         [-1.9249, -0.9757,  0.5187,  ..., -0.6843,  0.7887, -1.1647],\n",
            "         [-0.2129, -2.1276, -0.4319,  ...,  0.6715, -0.3980, -1.1025],\n",
            "         [ 0.7077, -0.9758,  1.0920,  ..., -0.2294,  2.8285, -0.6468]],\n",
            "\n",
            "        [[-0.4431, -1.3220,  0.5509,  ..., -0.1509,  0.2300, -1.9469],\n",
            "         [-1.5850, -0.1340,  0.1190,  ..., -1.9036,  1.2336, -1.0642],\n",
            "         [ 0.0287, -1.9625,  0.8274,  ..., -0.1492,  1.9002, -2.0694],\n",
            "         ...,\n",
            "         [-0.0872, -0.2816,  0.4103,  ..., -1.3527,  0.3518, -0.4970],\n",
            "         [ 0.1194, -1.0157, -1.0538,  ..., -0.5646,  1.9981, -0.4231],\n",
            "         [-2.0518, -0.2650, -0.3304,  ...,  0.6029, -0.0101, -0.9289]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码首先定义了 Encoder 类及其依赖的 EncoderLayer、MultiHeadAttention 和 FeedForward 类。然后，它创建了一个 Encoder 实例。接着，它生成了一个随机的输入张量和一个随机的 mask。最后，它将输入张量和 mask 传递给 Encoder 实例，并打印输入和输出张量的形状和内容。\n",
        "\n",
        "通过运行这段代码，你应该能看到输入张量、mask 和输出张量的形状和内容。输入张量的形状是 (batch_size, seq_len, d_model)，mask 的形状是 (batch_size, 1, seq_len, seq_len)，输出张量的形状也是 (batch_size, seq_len, d_model)。输出张量的内容是输入张量经过编码器处理后的结果。"
      ],
      "metadata": {
        "id": "1DubVH_b1lj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoder：像“多层精炼工厂”一样理解文本**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. 直观比喻：文本理解的流水线**\n",
        "想象一家矿石精炼厂的处理流程：\n",
        "- **原始矿石**：输入的文本序列（如\"I love natural language processing\"）\n",
        "- **第1层处理**：破碎矿石 → 识别基本语法结构（主谓宾）\n",
        "- **第2层处理**：分离金属 → 捕捉语义关系（\"processing\"与\"language\"的修饰关系）\n",
        "- **第3层处理**：提纯冶炼 → 理解深层含义（这句话表达对NLP领域的热情）\n",
        "- **最终产品**：高度抽象的语义表示，可供下游任务（翻译/分类）直接使用\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 代码拆解：知识精炼流水线**\n",
        "```python\n",
        "class EncoderLayer(nn.Module):\n",
        "    def forward(self, x, mask=None):\n",
        "        # 第一车间：自注意力车间（建立全局关联）\n",
        "        x = self.multi_head_attention(x, x, x, mask)  # 每个词与其他词对话\n",
        "        \n",
        "        # 第二车间：前馈神经网络车间（深度加工特征）\n",
        "        x = self.feed_forward(x)  # 每个词独立思考\n",
        "        \n",
        "        return x  # 精炼后的特征\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def forward(self, x, mask=None):\n",
        "        # 经过N层精炼（典型6层）\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)  # 每层在前一层基础上继续加工\n",
        "        return x\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Encoder的三大核心思想**\n",
        "| 思想                | 解释                          | 类比                      |\n",
        "|---------------------|-----------------------------|-------------------------|\n",
        "| **渐进式抽象**       | 每层逐步提取更高阶的特征            | 绘画：线稿→底色→细节           |\n",
        "| **参数共享架构**     | 所有层结构相同但参数独立              | 工厂每层设备相同但调节参数不同      |\n",
        "| **信息无损传递**     | 残差连接确保底层信息直达高层           | 文件修订保留所有历史批注          |\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. 逐层工作原理示例**\n",
        "处理句子：\"The animal didn't cross the street because it was too tired\"\n",
        "\n",
        "- **第1层**：\n",
        "  - 建立基础关联：\"it\"初步关联到\"animal\"和\"street\"\n",
        "  - 识别否定结构：\"didn't\"与\"cross\"的关系\n",
        "\n",
        "- **第3层**：\n",
        "  - 确定指代关系：\"it\"明确指向\"animal\"\n",
        "  - 理解因果逻辑：\"because\"连接结果与原因\n",
        "\n",
        "- **第6层**：\n",
        "  - 捕捉深层语义：整个句子的隐含重点是\"动物的状态\"\n",
        "  - 准备适用于下游任务的特征表示\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. 与CNN/RNN的对比**\n",
        "|                  | CNN                        | RNN                | Transformer Encoder |\n",
        "|------------------|----------------------------|--------------------|---------------------|\n",
        "| **特征提取方式**  | 局部窗口卷积                 | 顺序递归处理         | 全局自注意力           |\n",
        "| **长距离依赖**    | 需要多层堆叠                 | 容易遗忘早期信息      | 单层即可捕捉           |\n",
        "| **并行性**        | 高                         | 低                 | 极高                 |\n",
        "| **典型应用**      | 图像分类                    | 短文本生成           | 需要深度理解的任务       |\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. 可视化理解：特征演变动画**\n",
        "假设用颜色表示特征变化：\n",
        "- **输入层**：每个词是纯色块（红/蓝/绿...）\n",
        "- **第1层**：颜色开始混合（红+蓝→紫色表示动词关系）\n",
        "- **第3层**：出现金属光泽（指代关系用反光效果表现）\n",
        "- **输出层**：所有词呈现统一的金色（完成语义融合）\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Encoder的独特设计哲学**\n",
        "1. **横向对称性**：所有Encoder层结构完全相同\n",
        "   - 优势：方便扩展（如BERT-base有12层，large有24层）\n",
        "   - 实现：通过`nn.ModuleList`统一管理\n",
        "\n",
        "2. **纵向差异性**：每层通过独立参数学习不同模式\n",
        "   - 低层：关注语法/词性等基础特征\n",
        "   - 高层：捕捉语义/逻辑等抽象特征\n",
        "\n",
        "3. **稳定化设计**：\n",
        "   - 残差连接：防止深层网络退化\n",
        "   - LayerNorm：控制特征尺度\n",
        "   - Dropout：增强泛化能力\n",
        "\n",
        "---\n",
        "\n",
        "### **终极总结**\n",
        "Encoder就像一组智能滤网：\n",
        "1. **多层筛滤**：每层过滤掉无关信息，保留关键特征\n",
        "2. **逐步提纯**：从表层语法到深层语义的渐进理解\n",
        "3. **稳定传导**：通过残差连接确保信息无损传递\n",
        "\n",
        "这种设计使得Transformer能够像人类一样：\n",
        "- 第一遍阅读抓主旨\n",
        "- 第二遍阅读理逻辑\n",
        "- 第三遍阅读悟深意\n",
        "\n",
        "最终将原始文本转化为蕴含丰富语义的特征表示，为后续的解码器（Decoder）提供强大的理解基础！ 🚀"
      ],
      "metadata": {
        "id": "MPGsk58p2lYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "8. 解码器层\n",
        "\n",
        "    解码器层是Transformer解码器的基本组成部分。它由三个子层组成：一个多头自注意力子层、一个编码器-解码器注意力子层和一个前馈神经网络子层。\n"
      ],
      "metadata": {
        "id": "o2LGe4Gu2CJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.masked_multi_head_attention = MultiHeadAttention(d_model, n_heads)  # Masked Self Attention\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads)  # Encoder-Decoder Attention\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Masked self attention\n",
        "        output = self.masked_multi_head_attention(x, x, x, tgt_mask)\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        output = self.encoder_decoder_attention(output, encoder_output, encoder_output, src_mask)\n",
        "\n",
        "        # Feed forward\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "-mbuvav12CmO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "9. 解码器\n",
        "\n",
        "    解码器由多个相同的解码器层堆叠而成。它用于生成输出序列，一次生成一个token。\n"
      ],
      "metadata": {
        "id": "q7Y0AgTB2Vdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-sdKR6hd2WCv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len = Q.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(d_model, n_heads)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 多头自注意力的输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算自注意力\n",
        "        attn_output = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        # dropout + 残差连接 + layer norm\n",
        "        output = self.dropout(attn_output) + Q\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 线性层 + ReLU + 线性层\n",
        "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
        "        output = self.dropout(output) + x  # dropout + 残差连接\n",
        "        output = self.layer_norm(output)  # layer norm\n",
        "        return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        output = self.multi_head_attention(x, x, x, mask)\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.masked_multi_head_attention = MultiHeadAttention(d_model, n_heads)  # Masked Self Attention\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads)  # Encoder-Decoder Attention\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Masked self attention\n",
        "        output = self.masked_multi_head_attention(x, x, x, tgt_mask)\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        output = self.encoder_decoder_attention(output, encoder_output, encoder_output, src_mask)\n",
        "\n",
        "        # Feed forward\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "# 1. 定义超参数\n",
        "n_layers = 6\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "batch_size = 32\n",
        "src_seq_len = 64\n",
        "tgt_seq_len = 64\n",
        "\n",
        "# 2. 创建 Encoder 和 Decoder 实例\n",
        "encoder = Encoder(n_layers, d_model, n_heads, d_ff)\n",
        "decoder = Decoder(n_layers, d_model, n_heads, d_ff)\n",
        "\n",
        "# 3. 创建随机输入张量\n",
        "# 假设我们有形状为 (batch_size, src_seq_len, d_model) 的源序列输入和形状为 (batch_size, tgt_seq_len, d_model) 的目标序列输入。\n",
        "encoder_input_tensor = torch.randn(batch_size, src_seq_len, d_model)\n",
        "decoder_input_tensor = torch.randn(batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "# 4. 创建随机 mask\n",
        "src_mask = torch.randint(0, 2, (batch_size, 1, 1, src_seq_len)).bool()\n",
        "tgt_mask = torch.randint(0, 2, (batch_size, 1, tgt_seq_len, tgt_seq_len)).bool()\n",
        "\n",
        "# 5. 将输入张量传递给 Encoder 和 Decoder 实例\n",
        "encoder_output_tensor = encoder(encoder_input_tensor, src_mask)\n",
        "decoder_output_tensor = decoder(decoder_input_tensor, encoder_output_tensor, src_mask, tgt_mask)\n",
        "\n",
        "# 6. 打印输入和输出的形状\n",
        "print(\"Encoder Input tensor shape:\", encoder_input_tensor.shape)\n",
        "print(\"Encoder Output tensor shape:\", encoder_output_tensor.shape)\n",
        "print(\"Decoder Input tensor shape:\", decoder_input_tensor.shape)\n",
        "print(\"Decoder Output tensor shape:\", decoder_output_tensor.shape)\n",
        "print(\"Source Mask shape:\", src_mask.shape)\n",
        "print(\"Target Mask shape:\", tgt_mask.shape)\n",
        "\n",
        "# 7. 打印输入张量的内容\n",
        "print(\"Encoder Input tensor:\", encoder_input_tensor)\n",
        "print(\"Encoder Output tensor:\", encoder_output_tensor)\n",
        "print(\"Decoder Input tensor:\", decoder_input_tensor)\n",
        "print(\"Decoder Output tensor:\", decoder_output_tensor)\n",
        "print(\"Source Mask:\", src_mask)\n",
        "print(\"Target Mask:\", tgt_mask)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztCttl5y2Z0X",
        "outputId": "65dd81c0-686c-4ae9-8e34-d5a2995b86cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Input tensor shape: torch.Size([32, 64, 512])\n",
            "Encoder Output tensor shape: torch.Size([32, 64, 512])\n",
            "Decoder Input tensor shape: torch.Size([32, 64, 512])\n",
            "Decoder Output tensor shape: torch.Size([32, 64, 512])\n",
            "Source Mask shape: torch.Size([32, 1, 1, 64])\n",
            "Target Mask shape: torch.Size([32, 1, 64, 64])\n",
            "Encoder Input tensor: tensor([[[-3.7422e-02,  4.8560e-01, -1.9716e-01,  ...,  1.7757e+00,\n",
            "          -1.1385e-01,  6.8083e-02],\n",
            "         [ 9.9102e-01,  2.3063e-01, -2.3128e+00,  ..., -1.0813e-01,\n",
            "           1.2448e+00,  1.0214e+00],\n",
            "         [-1.8463e+00,  6.8184e-01,  7.4153e-01,  ...,  2.0625e+00,\n",
            "           3.7722e-01,  5.8482e-01],\n",
            "         ...,\n",
            "         [-3.3154e-01, -4.4327e-01, -3.0454e-01,  ...,  1.0890e-01,\n",
            "           1.6019e+00, -3.8015e-02],\n",
            "         [-1.2105e+00,  7.9613e-01, -6.6962e-01,  ..., -5.8293e-01,\n",
            "          -1.8930e+00, -1.2643e+00],\n",
            "         [-8.1272e-01,  5.7420e-02, -1.1916e+00,  ..., -9.7657e-01,\n",
            "          -7.3459e-01, -2.4755e+00]],\n",
            "\n",
            "        [[-1.3749e+00, -1.2282e+00,  1.7792e+00,  ...,  1.6325e+00,\n",
            "          -1.9100e+00, -1.1347e+00],\n",
            "         [ 5.7049e-01, -9.2971e-01,  8.0235e-01,  ..., -5.2722e-01,\n",
            "           4.8937e-01,  4.3419e-01],\n",
            "         [-1.2228e+00,  6.5914e-01,  1.2564e+00,  ...,  6.4280e-01,\n",
            "          -8.8788e-01, -6.3503e-01],\n",
            "         ...,\n",
            "         [ 8.1138e-01,  2.7964e-01,  9.2027e-01,  ...,  1.9356e-03,\n",
            "          -9.0123e-01,  1.0795e+00],\n",
            "         [-4.8238e-01, -1.8153e+00, -4.7443e-01,  ...,  3.5327e-01,\n",
            "           1.8338e+00,  2.3118e-01],\n",
            "         [ 2.8615e-01,  6.0619e-01,  1.7432e+00,  ...,  9.6988e-01,\n",
            "          -1.8321e+00, -7.0826e-01]],\n",
            "\n",
            "        [[ 1.6866e-01,  1.7149e+00, -1.9576e-01,  ..., -1.2483e+00,\n",
            "          -4.4629e-01,  3.1747e-01],\n",
            "         [-1.1800e+00,  1.6260e+00, -1.6628e-01,  ..., -1.4677e+00,\n",
            "           1.2381e+00,  4.7404e-01],\n",
            "         [-4.2507e-01,  1.2114e+00, -7.6387e-01,  ..., -6.6375e-01,\n",
            "          -1.0593e+00, -7.1881e-01],\n",
            "         ...,\n",
            "         [ 7.5605e-01,  3.2942e-01,  1.4330e+00,  ..., -4.8065e-01,\n",
            "          -1.1093e-01, -1.1215e+00],\n",
            "         [ 2.8082e-01, -8.4005e-01,  3.1139e-01,  ...,  7.9254e-01,\n",
            "           6.5298e-01,  2.5649e-01],\n",
            "         [-5.5102e-01, -1.4247e+00,  1.8282e+00,  ..., -5.9266e-01,\n",
            "          -6.7632e-01,  1.0449e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.4999e-01,  6.3736e-02, -4.6218e-01,  ..., -2.0185e+00,\n",
            "          -4.0376e-01, -2.5847e-01],\n",
            "         [ 2.6914e+00, -5.1584e-01,  3.2571e+00,  ...,  8.2172e-01,\n",
            "          -2.5357e-01, -9.1571e-01],\n",
            "         [ 1.5743e-01,  1.0479e+00, -2.3940e-02,  ..., -4.6233e-01,\n",
            "           4.6278e-01,  4.6434e-01],\n",
            "         ...,\n",
            "         [ 1.1863e+00, -8.8020e-01,  3.6640e-01,  ..., -3.9132e-01,\n",
            "           2.2933e+00,  4.3853e-01],\n",
            "         [-7.6561e-01,  5.0793e-01, -1.8021e+00,  ...,  2.5502e-02,\n",
            "           6.7963e-01, -8.1935e-02],\n",
            "         [ 3.5415e-01,  1.3797e-01, -2.7336e+00,  ..., -1.0091e+00,\n",
            "          -2.5997e-01,  1.1405e+00]],\n",
            "\n",
            "        [[ 3.1323e-01,  2.9150e-01,  1.4474e+00,  ..., -1.1238e+00,\n",
            "           4.7890e-01,  1.6011e+00],\n",
            "         [ 1.3903e+00, -1.2397e+00, -5.4188e-01,  ..., -1.7724e-01,\n",
            "          -1.2630e+00,  1.5711e+00],\n",
            "         [-5.9782e-01,  2.1999e-01,  4.4157e-01,  ...,  1.5799e+00,\n",
            "           4.3183e-02,  3.0040e-01],\n",
            "         ...,\n",
            "         [-5.8158e-01, -7.0022e-01, -8.8767e-02,  ...,  5.8125e-01,\n",
            "          -5.6927e-01, -1.9242e+00],\n",
            "         [-1.3900e-01, -1.5665e+00,  3.8480e-01,  ...,  6.0152e-01,\n",
            "           9.1410e-01, -1.5678e+00],\n",
            "         [ 2.3066e-01, -6.9746e-01, -3.8135e-01,  ...,  1.1897e-01,\n",
            "          -1.5813e+00,  4.0307e-01]],\n",
            "\n",
            "        [[ 6.5685e-01, -1.0581e+00, -1.7313e+00,  ..., -1.0389e+00,\n",
            "          -9.3247e-01,  1.5659e+00],\n",
            "         [ 1.4519e+00,  2.0627e+00,  8.8010e-01,  ..., -3.4330e-01,\n",
            "          -3.9141e-01,  8.0328e-01],\n",
            "         [-5.5401e-01, -1.3682e-01,  7.1026e-01,  ..., -4.6273e-03,\n",
            "          -3.2701e-02,  1.3637e-01],\n",
            "         ...,\n",
            "         [-4.5503e-01,  1.2088e+00, -6.0529e-01,  ...,  7.9586e-01,\n",
            "          -7.4975e-01,  1.8043e-01],\n",
            "         [-7.0266e-01,  5.9310e-01, -7.8661e-01,  ...,  1.5794e+00,\n",
            "          -6.9257e-01,  1.7462e-01],\n",
            "         [ 2.3273e+00,  6.8668e-01,  1.2037e+00,  ..., -7.3362e-01,\n",
            "           5.1367e-01,  1.2292e-01]]])\n",
            "Encoder Output tensor: tensor([[[ 3.5034e-01,  7.6476e-01, -8.0744e-01,  ...,  3.6243e-01,\n",
            "          -3.7654e-01, -7.1110e-01],\n",
            "         [ 1.1393e+00, -9.2396e-02, -1.7803e+00,  ..., -5.7490e-01,\n",
            "           6.9493e-01, -3.2248e-01],\n",
            "         [-1.5816e+00,  5.9919e-01, -8.3544e-02,  ...,  4.2150e-01,\n",
            "           5.0775e-01, -5.2637e-01],\n",
            "         ...,\n",
            "         [ 5.7827e-01, -6.3231e-01, -2.7181e-01,  ..., -1.1138e+00,\n",
            "           1.1928e+00, -2.4427e-01],\n",
            "         [-3.5063e-01,  8.2723e-01, -4.7171e-01,  ..., -1.2389e+00,\n",
            "          -1.1152e+00, -2.1957e+00],\n",
            "         [ 6.9624e-01, -1.3889e-01, -1.6167e+00,  ..., -1.3152e+00,\n",
            "          -5.7287e-01, -2.3803e+00]],\n",
            "\n",
            "        [[-1.7218e-01, -1.2937e+00,  7.4506e-01,  ...,  5.7052e-01,\n",
            "          -2.4589e+00, -1.1059e+00],\n",
            "         [ 1.0454e+00, -1.3752e+00,  1.0067e+00,  ..., -1.7092e+00,\n",
            "          -2.0916e-01,  1.1258e-01],\n",
            "         [-1.7746e+00,  4.1430e-01,  3.2209e-01,  ...,  1.1860e-01,\n",
            "          -7.2832e-01, -5.0352e-01],\n",
            "         ...,\n",
            "         [ 1.0368e+00,  8.5903e-01,  8.4139e-01,  ..., -1.2585e+00,\n",
            "          -1.6747e+00,  1.0713e+00],\n",
            "         [-5.5777e-01, -1.1391e+00, -6.6013e-01,  ..., -5.3391e-01,\n",
            "           1.1359e+00,  6.0598e-01],\n",
            "         [ 1.0344e-01,  1.4922e-01,  8.1031e-01,  ...,  1.1274e-01,\n",
            "          -2.0471e+00, -1.1689e+00]],\n",
            "\n",
            "        [[ 7.5082e-01,  1.0100e+00, -7.4658e-01,  ..., -1.5823e+00,\n",
            "          -3.4514e-01,  8.6282e-01],\n",
            "         [-3.0571e-01,  8.3335e-01,  5.3736e-01,  ..., -1.5931e+00,\n",
            "           3.3794e-01,  4.5733e-01],\n",
            "         [ 8.1708e-01,  7.0401e-01, -4.4351e-01,  ..., -6.5062e-01,\n",
            "          -1.6732e+00,  3.2373e-01],\n",
            "         ...,\n",
            "         [ 3.8344e-01, -5.1397e-01,  3.1061e-01,  ..., -6.4520e-01,\n",
            "          -9.3783e-01, -9.9390e-01],\n",
            "         [ 5.9575e-01, -1.6292e+00, -1.3017e-01,  ..., -1.0182e-01,\n",
            "          -2.9184e-01,  8.1252e-01],\n",
            "         [-1.6443e-01,  6.2542e-01,  1.2716e+00,  ..., -6.8517e-01,\n",
            "          -5.9800e-01,  8.6754e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.7150e-02,  2.9684e-01, -1.3916e+00,  ..., -2.9400e+00,\n",
            "          -4.6757e-04, -1.3499e-02],\n",
            "         [ 1.4122e+00, -1.8361e-01,  2.3820e+00,  ..., -1.0187e+00,\n",
            "          -1.0645e+00, -1.2125e+00],\n",
            "         [ 6.2572e-01,  7.8466e-01, -4.0155e-01,  ..., -8.1729e-01,\n",
            "           3.7374e-01,  3.9986e-01],\n",
            "         ...,\n",
            "         [ 1.2329e+00, -5.9986e-01,  3.3747e-01,  ..., -9.9735e-01,\n",
            "           1.1058e+00,  1.5144e+00],\n",
            "         [-3.3010e-02,  9.5718e-01, -1.1919e+00,  ..., -2.4999e-01,\n",
            "          -1.8964e-01, -2.9337e-01],\n",
            "         [ 7.2988e-01, -8.4741e-01, -1.6268e+00,  ..., -1.5947e+00,\n",
            "          -8.4272e-01,  1.0362e+00]],\n",
            "\n",
            "        [[ 4.2991e-01, -1.5428e-01,  1.2855e+00,  ..., -1.3169e+00,\n",
            "          -1.2604e-02,  3.6922e-01],\n",
            "         [ 1.4593e+00, -1.6922e+00, -7.9346e-01,  ..., -1.1965e-01,\n",
            "          -1.6197e+00,  1.9700e+00],\n",
            "         [-1.9983e-01, -1.7313e-01,  6.8938e-01,  ...,  8.1780e-01,\n",
            "          -4.6590e-01,  1.1997e+00],\n",
            "         ...,\n",
            "         [-2.7433e-01, -1.7385e-01,  8.4070e-02,  ...,  2.0333e-01,\n",
            "          -1.3110e+00, -8.7926e-01],\n",
            "         [-5.5987e-01, -1.9065e+00, -2.3822e-01,  ..., -1.4997e-01,\n",
            "          -3.3830e-01, -3.3704e-01],\n",
            "         [ 1.2090e+00, -1.5146e+00, -9.7100e-02,  ..., -1.1199e+00,\n",
            "          -1.6719e+00,  1.1651e+00]],\n",
            "\n",
            "        [[ 7.1900e-01, -1.2244e+00, -2.6492e+00,  ..., -5.8848e-01,\n",
            "          -5.1052e-01,  2.3946e+00],\n",
            "         [ 2.0070e+00,  1.5215e+00, -3.4551e-02,  ...,  1.5471e-01,\n",
            "          -1.0243e+00,  5.2784e-01],\n",
            "         [-3.0636e-01,  1.0402e+00,  5.0589e-01,  ..., -5.6717e-01,\n",
            "          -4.7834e-01,  5.8053e-01],\n",
            "         ...,\n",
            "         [-3.1783e-01,  7.9939e-01, -3.1083e-01,  ...,  5.7163e-01,\n",
            "          -1.6395e+00, -5.1977e-01],\n",
            "         [-5.8961e-01, -1.0808e+00, -4.4224e-01,  ...,  3.0318e-01,\n",
            "          -2.5280e-01,  5.6447e-02],\n",
            "         [ 1.5299e+00,  6.7286e-01,  8.3754e-01,  ..., -2.0459e+00,\n",
            "          -1.0932e-01,  6.7397e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder Input tensor: tensor([[[ 0.2883, -0.5715,  0.2551,  ..., -0.0227, -0.7003, -0.7795],\n",
            "         [ 1.5008, -0.7035,  0.2785,  ...,  1.2650, -0.7409,  0.2532],\n",
            "         [ 1.0814, -0.5511, -2.6488,  ..., -1.0591, -0.2279, -1.3641],\n",
            "         ...,\n",
            "         [ 1.1868,  1.7925,  0.0716,  ..., -0.6581,  0.5238, -0.7702],\n",
            "         [-0.8910, -0.6641, -1.0706,  ..., -0.8849,  2.0599,  2.4173],\n",
            "         [ 0.3621, -0.1379,  0.2841,  ...,  0.2775, -0.9145, -1.5788]],\n",
            "\n",
            "        [[-1.1276,  0.5426, -0.1407,  ...,  0.8760,  0.0075,  0.2589],\n",
            "         [ 0.3344,  1.2260,  1.0652,  ..., -0.6367,  0.3324,  1.4954],\n",
            "         [-0.1370,  1.4539, -0.9060,  ..., -0.5906, -0.5000,  0.6135],\n",
            "         ...,\n",
            "         [ 0.6977, -0.6336, -0.5138,  ...,  1.8798,  0.5968,  0.3295],\n",
            "         [ 1.1738, -1.6836,  0.7604,  ...,  0.0926,  1.6567,  0.6399],\n",
            "         [ 0.9031, -0.7515, -0.3920,  ...,  0.3753,  0.4519,  0.8579]],\n",
            "\n",
            "        [[ 0.1246, -0.9389,  0.2683,  ...,  2.0270,  0.7652,  1.5606],\n",
            "         [-0.2292,  1.4719,  1.1057,  ..., -0.6845, -0.8608, -0.9150],\n",
            "         [-0.6145,  0.3441,  0.6272,  ...,  0.8424, -0.6211,  0.6972],\n",
            "         ...,\n",
            "         [ 0.1016,  0.0555,  1.0080,  ...,  0.6859, -0.6324,  1.4015],\n",
            "         [-0.3579, -0.8263, -0.4767,  ...,  0.7998,  0.6856, -0.0154],\n",
            "         [-1.7052,  1.8258, -0.2332,  ...,  0.0754,  1.1400, -0.9560]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1120,  0.5090,  0.7785,  ...,  0.0980, -1.0174, -0.9586],\n",
            "         [-0.2809, -0.7785,  0.8184,  ..., -0.8365, -1.4253, -0.5264],\n",
            "         [-0.2229,  0.8997,  1.7682,  ...,  0.4071, -1.6295, -0.4455],\n",
            "         ...,\n",
            "         [-0.6203, -0.6127, -0.5024,  ...,  0.4155,  1.1776,  1.1107],\n",
            "         [-0.8464,  1.9015, -0.1228,  ..., -0.0331, -1.2955, -0.5916],\n",
            "         [-0.0058,  2.5601, -1.4566,  ..., -1.8364,  1.2981, -1.8958]],\n",
            "\n",
            "        [[ 0.6972, -0.8144,  1.2611,  ...,  0.3375,  1.3252,  2.1958],\n",
            "         [ 1.6804,  1.6187, -0.7517,  ..., -0.8849, -1.3571,  0.0465],\n",
            "         [-0.4462,  0.9057, -0.7803,  ...,  0.0843,  0.2444, -0.3059],\n",
            "         ...,\n",
            "         [ 1.6418,  0.6398,  0.8963,  ...,  0.1180, -1.0365,  0.5873],\n",
            "         [ 0.3026, -0.1664,  1.4526,  ..., -0.0632,  0.7880, -0.7575],\n",
            "         [-0.2050, -0.3883,  0.6350,  ..., -0.3184, -0.0305, -0.0927]],\n",
            "\n",
            "        [[ 0.4501, -1.3899,  1.7638,  ...,  0.5827, -0.7213, -1.0502],\n",
            "         [-0.9310, -1.9867,  1.5023,  ...,  1.4521, -0.4564,  0.5823],\n",
            "         [ 0.0377, -0.7389,  0.6605,  ..., -1.4443,  1.8522, -1.0693],\n",
            "         ...,\n",
            "         [ 0.1738, -0.2308, -0.5077,  ...,  0.3412, -0.1875,  0.4271],\n",
            "         [ 0.9419, -0.1037,  1.9640,  ..., -1.1488,  1.8743,  0.0693],\n",
            "         [-0.6830, -0.4334,  1.3817,  ..., -0.5197, -1.3827,  0.2386]]])\n",
            "Decoder Output tensor: tensor([[[ 0.4420,  0.1332,  0.6904,  ...,  0.8005, -1.2287, -1.5114],\n",
            "         [ 0.6178,  1.1133, -0.2453,  ...,  2.1389, -1.4200,  0.2715],\n",
            "         [ 1.1356, -0.3138, -1.0323,  ...,  0.2375, -0.0233, -0.6638],\n",
            "         ...,\n",
            "         [ 0.6683,  1.8424, -0.4104,  ...,  0.6272,  0.2795, -0.4660],\n",
            "         [-1.8529,  0.9959, -0.1430,  ..., -0.1981,  0.7015,  1.4411],\n",
            "         [-0.4704,  1.0025,  0.4165,  ...,  2.2838, -1.2109, -1.4056]],\n",
            "\n",
            "        [[-1.2771,  1.2158,  0.0367,  ...,  2.4949, -0.3829,  0.7197],\n",
            "         [ 0.3222,  2.1575,  1.0618,  ...,  1.4753, -0.5271,  1.6212],\n",
            "         [ 0.3086,  1.4410,  0.0500,  ..., -0.0925, -0.8596,  1.3086],\n",
            "         ...,\n",
            "         [ 0.4364, -0.4438, -0.0197,  ...,  2.2612, -0.3787,  1.1338],\n",
            "         [ 1.3892, -0.8750,  1.3728,  ...,  0.4104,  0.1713,  0.9796],\n",
            "         [ 0.2557, -0.2637,  1.0769,  ...,  1.1265, -0.4769,  0.9303]],\n",
            "\n",
            "        [[-0.6619, -1.1164,  1.2436,  ...,  1.7408,  0.0653,  1.1053],\n",
            "         [ 0.1847,  1.3704,  1.6570,  ..., -0.4668, -0.4917, -0.1978],\n",
            "         [-0.6552,  0.6534,  1.0130,  ...,  2.3786, -0.3819,  0.5067],\n",
            "         ...,\n",
            "         [-0.2043,  1.5193,  1.0368,  ...,  0.8785, -1.4546,  2.0599],\n",
            "         [ 0.1363,  0.0317,  0.2619,  ...,  1.5563, -0.2161,  0.5206],\n",
            "         [-0.2095,  0.8487,  0.1387,  ...,  0.2364,  0.5920,  0.1061]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3278,  0.6383,  1.2908,  ...,  1.6424, -1.6115, -1.0855],\n",
            "         [-0.0141,  0.8435,  1.2808,  ...,  0.0766, -2.4957, -0.2733],\n",
            "         [-0.0033,  1.0315,  1.8925,  ...,  1.3525, -2.0427, -0.7804],\n",
            "         ...,\n",
            "         [-1.4835,  0.4096, -0.2553,  ...,  1.1347,  0.3128,  0.9818],\n",
            "         [-1.7446,  2.1807, -0.5994,  ...,  0.9048, -2.5663,  0.2748],\n",
            "         [-0.2993,  2.6709,  0.1211,  ...,  0.4730,  0.2590, -0.3874]],\n",
            "\n",
            "        [[-0.4985,  0.1546,  2.0821,  ...,  1.7817, -0.1990,  1.1480],\n",
            "         [-0.5563,  1.5009, -0.6204,  ...,  0.8168, -1.5056,  0.3745],\n",
            "         [-0.3779,  1.1385, -0.4242,  ...,  0.8587, -1.3304,  0.3673],\n",
            "         ...,\n",
            "         [-0.1506,  1.2758,  0.8188,  ...,  1.8439, -1.9998,  1.1424],\n",
            "         [ 0.1184,  0.2219,  0.8497,  ...,  0.9126, -0.7387, -0.9318],\n",
            "         [-0.8049, -0.3849,  0.8918,  ...,  1.0230, -0.7832,  0.3618]],\n",
            "\n",
            "        [[-0.0805,  0.7205,  2.1187,  ...,  1.4349, -1.0669, -0.6796],\n",
            "         [-1.0535, -1.3944,  1.5820,  ...,  2.7362, -0.8038,  0.2759],\n",
            "         [-0.4027,  0.5082,  1.6344,  ..., -0.1269,  1.2325, -1.1059],\n",
            "         ...,\n",
            "         [ 0.5235,  0.1889,  0.2546,  ...,  1.7561, -0.1782,  0.0273],\n",
            "         [ 0.3270, -0.4523,  1.8193,  ...,  0.0995,  0.7380, -0.1863],\n",
            "         [ 0.0837,  0.5698,  1.9925,  ...,  1.3093, -2.5042,  0.5261]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Source Mask: tensor([[[[False,  True,  True,  ..., False,  True, False]]],\n",
            "\n",
            "\n",
            "        [[[False, False,  True,  ..., False, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False, False,  True,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ True, False,  True,  ..., False, False, False]]],\n",
            "\n",
            "\n",
            "        [[[False, False, False,  ...,  True, False, False]]],\n",
            "\n",
            "\n",
            "        [[[False, False,  True,  ...,  True, False, False]]]])\n",
            "Target Mask: tensor([[[[ True, False, False,  ...,  True,  True, False],\n",
            "          [False, False,  True,  ..., False,  True, False],\n",
            "          [ True,  True,  True,  ...,  True, False, False],\n",
            "          ...,\n",
            "          [False, False, False,  ...,  True,  True,  True],\n",
            "          [ True,  True,  True,  ...,  True, False,  True],\n",
            "          [False, False, False,  ..., False,  True, False]]],\n",
            "\n",
            "\n",
            "        [[[False,  True,  True,  ...,  True,  True,  True],\n",
            "          [False, False, False,  ...,  True, False,  True],\n",
            "          [ True, False, False,  ...,  True,  True, False],\n",
            "          ...,\n",
            "          [False,  True,  True,  ...,  True,  True, False],\n",
            "          [ True, False,  True,  ..., False, False, False],\n",
            "          [False,  True,  True,  ..., False, False,  True]]],\n",
            "\n",
            "\n",
            "        [[[False,  True, False,  ..., False,  True, False],\n",
            "          [False,  True,  True,  ..., False,  True,  True],\n",
            "          [ True, False, False,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [ True,  True, False,  ..., False,  True,  True],\n",
            "          [ True,  True, False,  ...,  True, False, False],\n",
            "          [False,  True, False,  ...,  True, False,  True]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[False, False, False,  ..., False,  True,  True],\n",
            "          [ True,  True, False,  ...,  True,  True, False],\n",
            "          [ True,  True,  True,  ...,  True, False,  True],\n",
            "          ...,\n",
            "          [False, False,  True,  ..., False,  True,  True],\n",
            "          [False,  True,  True,  ..., False,  True,  True],\n",
            "          [False,  True, False,  ...,  True,  True,  True]]],\n",
            "\n",
            "\n",
            "        [[[False, False,  True,  ...,  True, False,  True],\n",
            "          [False, False, False,  ...,  True,  True,  True],\n",
            "          [False, False, False,  ..., False,  True, False],\n",
            "          ...,\n",
            "          [False, False, False,  ..., False,  True, False],\n",
            "          [ True, False,  True,  ...,  True, False, False],\n",
            "          [False,  True, False,  ...,  True,  True,  True]]],\n",
            "\n",
            "\n",
            "        [[[ True, False, False,  ..., False,  True, False],\n",
            "          [False,  True, False,  ...,  True, False, False],\n",
            "          [ True,  True,  True,  ..., False,  True,  True],\n",
            "          ...,\n",
            "          [False,  True, False,  ..., False, False, False],\n",
            "          [False, False,  True,  ..., False, False,  True],\n",
            "          [False, False,  True,  ..., False, False,  True]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decoder：像“作家创作”一样逐步生成**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. 直观比喻：写作的三步法**\n",
        "想象一位作家在创作小说：\n",
        "1. **构思大纲（Masked Self-Attention）**：先想好已写部分的关系（不能偷看未写内容）\n",
        "   - *\"从前有个__\" → 只能基于\"从前有个\"思考，不能提前想空白处的内容*\n",
        "   \n",
        "2. **参考素材（Encoder-Decoder Attention）**：查阅历史资料（Encoder理解的原作）\n",
        "   - *把中文句子\"我爱机器学习\"的语义特征作为参考*\n",
        "   \n",
        "3. **润色段落（FeedForward）**：深化当前句子的表达\n",
        "   - *把\"模型\"优化为\"深度学习模型\"*\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 代码拆解：创作流水线**\n",
        "```python\n",
        "class DecoderLayer(nn.Module):\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # 第一步：构思已写内容（屏蔽未来信息）\n",
        "        x = self.masked_multi_head_attention(x, x, x, tgt_mask)\n",
        "        \n",
        "        # 第二步：参考素材库（编码器的理解）\n",
        "        x = self.encoder_decoder_attention(x, encoder_output, encoder_output, src_mask)\n",
        "        \n",
        "        # 第三步：精炼当前表达\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Decoder的三大核心设计**\n",
        "| 设计                | 作用                          | 类比                      |\n",
        "|---------------------|-----------------------------|-------------------------|\n",
        "| **Masked Self-Attention** | 防止偷看未来信息，保证自回归生成      | 写作文时用纸遮住后面的空白行       |\n",
        "| **Encoder-Decoder Attention** | 让生成内容与源语义对齐          | 翻译时随时对照原文             |\n",
        "| **渐进生成机制**      | 逐词生成，每个词依赖之前所有输出     | 接龙游戏：必须基于前文续写        |\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. 逐层工作原理示例**\n",
        "**任务**：把\"I love machine learning\"翻译成\"我 爱 机器 学习\"\n",
        "\n",
        "- **第1个解码步（生成\"我\"）**：\n",
        "  ```python\n",
        "  # Masked Self-Attention：只能看到起始符<start>\n",
        "  # Encoder-Decoder Attention：关注英文中的\"I\"\n",
        "  ```\n",
        "\n",
        "- **第3个解码步（生成\"机器\"）**：\n",
        "  ```python\n",
        "  # Masked Self-Attention：能看到\"我 爱\"\n",
        "  # Encoder-Decoder Attention：聚焦\"machine\"\n",
        "  ```\n",
        "\n",
        "- **最终步（生成\"学习\"）**：\n",
        "  ```python\n",
        "  # Masked Self-Attention：已生成\"我 爱 机器\"\n",
        "  # Encoder-Decoder Attention：关联\"learning\"\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. 关键机制详解**\n",
        "##### **5.1 Masked Self-Attention：防作弊面具**\n",
        "- **实现方式**：在注意力分数矩阵上加下三角掩码\n",
        "  ```python\n",
        "  # 假设序列长度为3，掩码矩阵为：\n",
        "  [[1, 0, 0],\n",
        "   [1, 1, 0],\n",
        "   [1, 1, 1]]\n",
        "  ```\n",
        "- **效果**：生成第2个词时，只能关注第1个词\n",
        "\n",
        "##### **5.2 Encoder-Decoder Attention：跨语言桥**\n",
        "- **Query来自Decoder**：当前生成位置的需求\n",
        "- **Key/Value来自Encoder**：源语言的知识库\n",
        "- **类比**：学生（Decoder）带着问题（Query），去课本（Encoder）里找答案\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. 与Encoder的对比**\n",
        "|                  | Encoder                      | Decoder                |\n",
        "|------------------|-----------------------------|------------------------|\n",
        "| **注意力类型**    | 全连接自注意力                 | Masked自注意力+交叉注意力  |\n",
        "| **输入依赖**      | 只处理源序列                   | 依赖源序列和目标序列         |\n",
        "| **运行方式**      | 一次完成全部计算                | 自回归逐词生成             |\n",
        "| **典型应用**      | 理解/分类任务                 | 生成/翻译任务             |\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. 可视化理解：瀑布式创作**\n",
        "想象Decoder的工作像瀑布流水：\n",
        "1. **顶层水流（第1层）**：粗糙的语义流向（确定要表达\"喜爱\"）\n",
        "2. **中层瀑布（第3层）**：细化表达方式（选择动词\"爱\"）\n",
        "3. **底层水池（第6层）**：精准用词（确定名词\"机器学习\"）\n",
        "\n",
        "每一层都在前一层的基础上加工，最终形成完整的语义瀑布。\n",
        "\n",
        "---\n",
        "\n",
        "### **终极总结**\n",
        "Decoder的设计蕴含三大智慧：\n",
        "1. **自律性**：通过Mask机制约束自己只关注已生成内容\n",
        "2. **开放性**：通过交叉注意力吸收外部知识（Encoder输出）\n",
        "3. **渐进性**：像画家作画般层层叠加细节\n",
        "\n",
        "正是这种既自律又开放的机制，使得Transformer能够完成：\n",
        "- *流畅的机器翻译*\n",
        "- *连贯的文本生成*\n",
        "- *精准的问答输出*\n",
        "\n",
        "就像一位严谨的作家，既要遵循创作纪律，又要广泛吸收素材，最终写出精彩篇章！ 📖"
      ],
      "metadata": {
        "id": "E1Y5Q3Ng3pV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Transformer模型\n",
        "\n",
        "    最后，我们将所有组件组合起来，构建完整的Transformer模型。\n"
      ],
      "metadata": {
        "id": "nGi8Ncm13pri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, max_len):\n",
        "        super().__init__()\n",
        "        self.encoder_input_embedding = InputEmbeddings(d_model, src_vocab_size)\n",
        "        self.decoder_input_embedding = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff)\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size)  # 输出线性层，将解码器的输出映射到目标词汇表\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Transformer模型\n",
        "\n",
        "        Args:\n",
        "            src: 源序列, shape (batch_size, src_seq_len)\n",
        "            tgt: 目标序列, shape (batch_size, tgt_seq_len)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, tgt_vocab_size)\n",
        "        \"\"\"\n",
        "        # 1. 计算源序列和目标序列的嵌入向量\n",
        "        src_embedded = self.encoder_input_embedding(src)  # (batch_size, src_seq_len, d_model)\n",
        "        tgt_embedded = self.decoder_input_embedding(tgt)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 2. 加上位置编码\n",
        "        src_embedded = self.positional_encoding(src_embedded)  # (batch_size, src_seq_len, d_model)\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 3. 通过编码器处理源序列\n",
        "        encoder_output = self.encoder(src_embedded, src_mask)  # (batch_size, src_seq_len, d_model)\n",
        "\n",
        "        # 4. 通过解码器处理目标序列\n",
        "        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 5. 通过线性层得到最终输出\n",
        "        output = self.projection(decoder_output)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "a9Zmibtu3qQL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Mask的实现\n",
        "\n",
        "    在Transformer模型中，mask用于屏蔽某些位置的信息，以防止模型在训练过程中作弊。\n",
        "\n",
        "    有两种类型的mask：源序列mask (src_mask): 用于屏蔽源序列中的填充（padding）token。目标序列mask (tgt_mask): 用于屏蔽目标序列中未来位置的token，以确保模型在生成每个token时只能依赖于之前生成的token。\n",
        "\n"
      ],
      "metadata": {
        "id": "ypcUzCJX3s2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_masks(src, tgt, pad_token):\n",
        "    \"\"\"\n",
        "    生成源序列和目标序列的mask\n",
        "\n",
        "    Args:\n",
        "        src: 源序列, shape (batch_size, src_seq_len)\n",
        "        tgt: 目标序列, shape (batch_size, tgt_seq_len)\n",
        "        pad_token: padding token的索引\n",
        "    Returns:\n",
        "        src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "        tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "    \"\"\"\n",
        "    batch_size = src.size(0)\n",
        "    src_seq_len = src.size(1)\n",
        "    tgt_seq_len = tgt.size(1)\n",
        "\n",
        "    # 1. 创建源序列mask\n",
        "    src_mask = (src != pad_token).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "    # 2. 创建目标序列的padding mask\n",
        "    tgt_pad_mask = (tgt != pad_token).unsqueeze(1).unsqueeze(3)  # (batch_size, 1, tgt_seq_len, 1)\n",
        "\n",
        "    # 3. 创建一个下三角矩阵，用于屏蔽未来位置的信息\n",
        "    subsequent_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=src.device)).bool()\n",
        "    subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(1)  # (1, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "    # 4. 组合padding mask和subsequent mask\n",
        "    tgt_mask = tgt_pad_mask & subsequent_mask  # (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "    return src_mask, tgt_mask\n"
      ],
      "metadata": {
        "id": "RtKV7uls3uFq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "12. 训练Transformer模型\n",
        "\n",
        "    现在，我们可以使用上述组件来训练Transformer模型。我们将使用一个简单的序列到序列的任务，例如将一种语言翻译成另一种语言。\n",
        "\n"
      ],
      "metadata": {
        "id": "tpm2taM031us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # 添加缺失的导入\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将词嵌入向量乘以根号d_model，以缩小数值范围，这有助于在训练过程中稳定梯度。\n",
        "        return self.embedding(x) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # 创建一个形状为 (max_len, d_model) 的矩阵，用于保存位置编码\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # 创建一个形状为 (max_len, 1) 的向量，包含位置信息\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        # 计算位置除以 (10000 ** (2i / d_model)) 的值，用于后续计算\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        # 计算偶数位置的正弦值\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # 计算奇数位置的余弦值\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        # 将 pe 的形状变为 (1, max_len, d_model)，以便与嵌入向量相加\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 将输入 x 的词嵌入向量与对应位置的位置编码相加\n",
        "        # x.shape: (batch_size, seq_len, d_model)\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # 每个head的维度\n",
        "\n",
        "        # 定义用于计算query, key, value的线性层\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_O = nn.Linear(d_model, d_model)  # 输出线性层\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)  # 用于屏蔽padding和未来信息\n",
        "        Returns:\n",
        "            attn_output: 注意力输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len_q = Q.size(1)\n",
        "        seq_len_k = K.size(1)\n",
        "        seq_len_v = V.size(1)\n",
        "\n",
        "        # 1. 线性变换得到Q, K, V\n",
        "        Q = self.W_Q(Q)  # (batch_size, seq_len_q, d_model)\n",
        "        K = self.W_K(K)  # (batch_size, seq_len_k, d_model)\n",
        "        V = self.W_V(V)  # (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        # 2. 将Q, K, V按head分割\n",
        "        Q = Q.view(batch_size, seq_len_q, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len_q, d_k)\n",
        "        K = K.view(batch_size, seq_len_k, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len_k, d_k)\n",
        "        V = V.view(batch_size, seq_len_v, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len_v, d_k)\n",
        "\n",
        "        # 3. 计算注意力分数\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # 4. 应用mask\n",
        "        if mask is not None:\n",
        "            # 确保mask与注意力分数的形状匹配\n",
        "            if mask.size(-1) != attn_scores.size(-1) or mask.size(-2) != attn_scores.size(-2):\n",
        "                # 调整mask大小以匹配注意力分数\n",
        "                if len(mask.size()) == 3:  # 如果mask是3D的(batch_size, 1, seq_len)\n",
        "                    mask = mask.unsqueeze(2)  # 变为(batch_size, 1, 1, seq_len)\n",
        "                mask = mask[:, :, :attn_scores.size(2), :attn_scores.size(3)]\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)  # 将mask为0的位置的score设为负无穷\n",
        "\n",
        "        # 5. 计算注意力权重\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "        # 6. 计算加权后的V\n",
        "        attn_output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len_q, d_k)\n",
        "\n",
        "        # 7. 拼接多头结果\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # 8. 通过线性层得到最终输出\n",
        "        attn_output = self.W_O(attn_output)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.self_attention = SelfAttention(d_model, n_heads)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        计算多头自注意力\n",
        "\n",
        "        Args:\n",
        "            Q: query, shape (batch_size, seq_len, d_model)\n",
        "            K: key, shape (batch_size, seq_len, d_model)\n",
        "            V: value, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 多头自注意力的输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 计算自注意力\n",
        "        attn_output = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        # dropout + 残差连接 + layer norm\n",
        "        output = self.dropout(attn_output) + Q\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # 线性层 + ReLU + 线性层\n",
        "        output = self.linear_2(self.relu(self.linear_1(x)))\n",
        "        output = self.dropout(output) + x  # dropout + 残差连接\n",
        "        output = self.layer_norm(output)  # layer norm\n",
        "        return output\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        output = self.multi_head_attention(x, x, x, mask)\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        编码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, seq_len, d_model)\n",
        "            mask: mask, shape (batch_size, 1, seq_len, seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.masked_multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器层\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Masked self attention\n",
        "        output = self.masked_multi_head_attention(x, x, x, tgt_mask)\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        output = self.encoder_decoder_attention(output, encoder_output, encoder_output, src_mask)\n",
        "\n",
        "        # Feed forward\n",
        "        output = self.feed_forward(output)\n",
        "        return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        解码器\n",
        "\n",
        "        Args:\n",
        "            x: 输入, shape (batch_size, tgt_seq_len, d_model)\n",
        "            encoder_output: 编码器的输出, shape (batch_size, src_seq_len, d_model)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, d_model)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, max_len):\n",
        "        super().__init__()\n",
        "        self.encoder_input_embedding = InputEmbeddings(d_model, src_vocab_size)\n",
        "        self.decoder_input_embedding = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff)\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size)  # 输出线性层，将解码器的输出映射到目标词汇表\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Transformer模型\n",
        "\n",
        "        Args:\n",
        "            src: 源序列, shape (batch_size, src_seq_len)\n",
        "            tgt: 目标序列, shape (batch_size, tgt_seq_len)\n",
        "            src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "            tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "        Returns:\n",
        "            output: 输出, shape (batch_size, tgt_seq_len, tgt_vocab_size)\n",
        "        \"\"\"\n",
        "        # 1. 计算源序列和目标序列的嵌入向量\n",
        "        src_embedded = self.encoder_input_embedding(src)  # (batch_size, src_seq_len, d_model)\n",
        "        tgt_embedded = self.decoder_input_embedding(tgt)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 2. 加上位置编码\n",
        "        src_embedded = self.positional_encoding(src_embedded)  # (batch_size, src_seq_len, d_model)\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 3. 通过编码器处理源序列\n",
        "        encoder_output = self.encoder(src_embedded, src_mask)  # (batch_size, src_seq_len, d_model)\n",
        "\n",
        "        # 4. 通过解码器处理目标序列\n",
        "        decoder_output = self.decoder(tgt_embedded, encoder_output, src_mask, tgt_mask)  # (batch_size, tgt_seq_len, d_model)\n",
        "\n",
        "        # 5. 通过线性层得到最终输出\n",
        "        output = self.projection(decoder_output)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "def create_masks(src, tgt, pad_token):\n",
        "    \"\"\"\n",
        "    生成源序列和目标序列的mask\n",
        "\n",
        "    Args:\n",
        "        src: 源序列, shape (batch_size, src_seq_len)\n",
        "        tgt: 目标序列, shape (batch_size, tgt_seq_len)\n",
        "        pad_token: padding token的索引\n",
        "    Returns:\n",
        "        src_mask: 源序列的mask, shape (batch_size, 1, 1, src_seq_len)\n",
        "        tgt_mask: 目标序列的mask, shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "    \"\"\"\n",
        "    batch_size = src.size(0)\n",
        "    src_seq_len = src.size(1)\n",
        "    tgt_seq_len = tgt.size(1)\n",
        "\n",
        "    # 1. 创建源序列mask\n",
        "    src_mask = (src != pad_token).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, src_seq_len)\n",
        "\n",
        "    # 2. 创建目标序列的padding mask\n",
        "    tgt_pad_mask = (tgt != pad_token).unsqueeze(1).unsqueeze(3)  # (batch_size, 1, tgt_seq_len, 1)\n",
        "\n",
        "    # 3. 创建一个下三角矩阵，用于屏蔽未来位置的信息\n",
        "    subsequent_mask = torch.tril(torch.ones((tgt_seq_len, tgt_seq_len), device=src.device)).bool()\n",
        "    subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(1)  # (1, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "    # 4. 组合padding mask和subsequent mask\n",
        "    tgt_mask = tgt_pad_mask & subsequent_mask  # (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
        "\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# 1. 定义超参数\n",
        "src_vocab_size = 10000\n",
        "tgt_vocab_size = 10000\n",
        "d_model = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "max_len = 100\n",
        "pad_token = 0  # 假设0为padding token\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.0001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. 创建Transformer模型\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, max_len).to(device)\n",
        "\n",
        "# 3. 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)  # 忽略padding token的损失\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 4. 创建虚拟数据\n",
        "src_data = torch.randint(1, src_vocab_size, (1000, max_len)).to(device)  # (1000, max_len)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (1000, max_len)).to(device)  # (1000, max_len)\n",
        "dataset = TensorDataset(src_data, tgt_data)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 5. 训练模型\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (src, tgt) in enumerate(dataloader):\n",
        "        # 准备数据\n",
        "        src = src.to(device)  # (batch_size, src_seq_len)\n",
        "        tgt = tgt.to(device)  # (batch_size, tgt_seq_len)\n",
        "\n",
        "        # 教师强制：使用tgt的前面部分(不包括最后一个token)作为解码器输入\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]  # 预测目标：tgt的后面部分(不包括第一个token)\n",
        "\n",
        "        # 创建mask\n",
        "        src_mask, tgt_mask = create_masks(src, tgt_input, pad_token)\n",
        "\n",
        "        # 前向传播\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask)  # (batch_size, tgt_seq_len-1, tgt_vocab_size)\n",
        "\n",
        "        # 计算损失\n",
        "        output = output.reshape(-1, tgt_vocab_size)  # (batch_size * (tgt_seq_len-1), tgt_vocab_size)\n",
        "        tgt_output = tgt_output.reshape(-1)  # (batch_size * (tgt_seq_len-1))\n",
        "        loss = criterion(output, tgt_output)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 反向传播和优化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪，防止梯度爆炸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # 打印损失\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # 打印每个epoch的平均损失\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "print('Training finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlNmLYwC3zAj",
        "outputId": "e7ac7564-89df-4cb7-ded7-07db5bd0ff78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/32], Loss: 9.3607\n",
            "Epoch [1/10], Step [20/32], Loss: 9.3210\n",
            "Epoch [1/10], Step [30/32], Loss: 9.3058\n",
            "Epoch [1/10], Average Loss: 9.3341\n",
            "Epoch [2/10], Step [10/32], Loss: 9.2285\n",
            "Epoch [2/10], Step [20/32], Loss: 9.2097\n",
            "Epoch [2/10], Step [30/32], Loss: 9.2260\n",
            "Epoch [2/10], Average Loss: 9.2215\n",
            "Epoch [3/10], Step [10/32], Loss: 9.1852\n",
            "Epoch [3/10], Step [20/32], Loss: 9.1984\n",
            "Epoch [3/10], Step [30/32], Loss: 9.2015\n",
            "Epoch [3/10], Average Loss: 9.1890\n",
            "Epoch [4/10], Step [10/32], Loss: 9.1524\n",
            "Epoch [4/10], Step [20/32], Loss: 9.1513\n",
            "Epoch [4/10], Step [30/32], Loss: 9.1694\n",
            "Epoch [4/10], Average Loss: 9.1499\n",
            "Epoch [5/10], Step [10/32], Loss: 8.9823\n",
            "Epoch [5/10], Step [20/32], Loss: 8.9682\n",
            "Epoch [5/10], Step [30/32], Loss: 8.9298\n",
            "Epoch [5/10], Average Loss: 9.0002\n",
            "Epoch [6/10], Step [10/32], Loss: 8.7823\n",
            "Epoch [6/10], Step [20/32], Loss: 8.7316\n",
            "Epoch [6/10], Step [30/32], Loss: 8.7828\n",
            "Epoch [6/10], Average Loss: 8.7828\n",
            "Epoch [7/10], Step [10/32], Loss: 8.5778\n",
            "Epoch [7/10], Step [20/32], Loss: 8.5883\n",
            "Epoch [7/10], Step [30/32], Loss: 8.6089\n",
            "Epoch [7/10], Average Loss: 8.5810\n",
            "Epoch [8/10], Step [10/32], Loss: 8.3668\n",
            "Epoch [8/10], Step [20/32], Loss: 8.3526\n",
            "Epoch [8/10], Step [30/32], Loss: 8.4290\n",
            "Epoch [8/10], Average Loss: 8.3979\n",
            "Epoch [9/10], Step [10/32], Loss: 8.2304\n",
            "Epoch [9/10], Step [20/32], Loss: 8.2450\n",
            "Epoch [9/10], Step [30/32], Loss: 8.2109\n",
            "Epoch [9/10], Average Loss: 8.2172\n",
            "Epoch [10/10], Step [10/32], Loss: 8.0644\n",
            "Epoch [10/10], Step [20/32], Loss: 8.0462\n",
            "Epoch [10/10], Step [30/32], Loss: 8.0741\n",
            "Epoch [10/10], Average Loss: 8.0402\n",
            "Training finished!\n"
          ]
        }
      ]
    }
  ]
}