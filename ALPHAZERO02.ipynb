{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMp+G/FqUeoGXjXlPqS4M5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/ALPHAZERO02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "game_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "    \"\"\"board for the game\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.width = int(kwargs.get('width', 6))\n",
        "        self.height = int(kwargs.get('height', 6))\n",
        "        #self.width = int(kwargs.get('width', 8))\n",
        "        #self.height = int(kwargs.get('height', 8))\n",
        "        # board states stored as a dict,\n",
        "        # key: move as location on the board,\n",
        "        # value: player as pieces type\n",
        "        self.states = {}\n",
        "        # need how many pieces in a row to win\n",
        "        self.n_in_row = int(kwargs.get('n_in_row', 4))\n",
        "        #self.n_in_row = int(kwargs.get('n_in_row', 5))\n",
        "        self.players = [1, 2]  # player1 and player2\n",
        "\n",
        "    def init_board(self, start_player=0):\n",
        "        if self.width < self.n_in_row or self.height < self.n_in_row:\n",
        "            raise Exception('board width and height can not be '\n",
        "                            'less than {}'.format(self.n_in_row))\n",
        "        self.current_player = self.players[start_player]  # start player\n",
        "        # keep available moves in a list\n",
        "        self.availables = list(range(self.width * self.height))\n",
        "        self.states = {}\n",
        "        self.last_move = -1\n",
        "\n",
        "    def move_to_location(self, move):\n",
        "        \"\"\"\n",
        "        3*3 board's moves like:\n",
        "        6 7 8\n",
        "        3 4 5\n",
        "        0 1 2\n",
        "        and move 5's location is (1,2)\n",
        "        \"\"\"\n",
        "        h = move // self.width\n",
        "        w = move % self.width\n",
        "        return [h, w]\n",
        "\n",
        "    def location_to_move(self, location):\n",
        "        if len(location) != 2:\n",
        "            return -1\n",
        "        h = location[0]\n",
        "        w = location[1]\n",
        "        move = h * self.width + w\n",
        "        if move not in range(self.width * self.height):\n",
        "            return -1\n",
        "        return move\n",
        "\n",
        "    def current_state(self):\n",
        "        \"\"\"return the board state from the perspective of the current player.\n",
        "        state shape: 4*width*height\n",
        "        \"\"\"\n",
        "\n",
        "        square_state = np.zeros((4, self.width, self.height))\n",
        "        if self.states:\n",
        "            moves, players = np.array(list(zip(*self.states.items())))\n",
        "            move_curr = moves[players == self.current_player]\n",
        "            move_oppo = moves[players != self.current_player]\n",
        "            square_state[0][move_curr // self.width,\n",
        "                            move_curr % self.height] = 1.0\n",
        "            square_state[1][move_oppo // self.width,\n",
        "                            move_oppo % self.height] = 1.0\n",
        "            # indicate the last move location\n",
        "            square_state[2][self.last_move // self.width,\n",
        "                            self.last_move % self.height] = 1.0\n",
        "        if len(self.states) % 2 == 0:\n",
        "            square_state[3][:, :] = 1.0  # indicate the colour to play\n",
        "        return square_state[:, ::-1, :]\n",
        "\n",
        "    def do_move(self, move):\n",
        "        self.states[move] = self.current_player\n",
        "        self.availables.remove(move)\n",
        "        self.current_player = (\n",
        "            self.players[0] if self.current_player == self.players[1]\n",
        "            else self.players[1]\n",
        "        )\n",
        "        self.last_move = move\n",
        "\n",
        "    def has_a_winner(self):\n",
        "        width = self.width\n",
        "        height = self.height\n",
        "        states = self.states\n",
        "        n = self.n_in_row\n",
        "\n",
        "        moved = list(set(range(width * height)) - set(self.availables))\n",
        "        if len(moved) < self.n_in_row *2-1:\n",
        "            return False, -1\n",
        "\n",
        "        for m in moved:\n",
        "            h = m // width\n",
        "            w = m % width\n",
        "            player = states[m]\n",
        "\n",
        "            if (w in range(width - n + 1) and\n",
        "                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n",
        "                return True, player\n",
        "\n",
        "            if (h in range(height - n + 1) and\n",
        "                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n",
        "                return True, player\n",
        "\n",
        "            if (w in range(width - n + 1) and h in range(height - n + 1) and\n",
        "                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n",
        "                return True, player\n",
        "\n",
        "            if (w in range(n - 1, width) and h in range(height - n + 1) and\n",
        "                    len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):\n",
        "                return True, player\n",
        "\n",
        "        return False, -1\n",
        "\n",
        "    def game_end(self):\n",
        "        \"\"\"Check whether the game is ended or not\"\"\"\n",
        "        win, winner = self.has_a_winner()\n",
        "        if win:\n",
        "            return True, winner\n",
        "        elif not len(self.availables):\n",
        "            return True, -1\n",
        "        return False, -1\n",
        "\n",
        "    def get_current_player(self):\n",
        "        return self.current_player\n",
        "\n",
        "\n",
        "class Game(object):\n",
        "    \"\"\"game server\"\"\"\n",
        "\n",
        "    def __init__(self, board, **kwargs):\n",
        "        self.board = board\n",
        "\n",
        "    def graphic(self, board, player1, player2):\n",
        "        \"\"\"Draw the board and show game info\"\"\"\n",
        "        width = board.width\n",
        "        height = board.height\n",
        "\n",
        "        print(\"Player\", player1, \"with X\".rjust(3))\n",
        "        print(\"Player\", player2, \"with O\".rjust(3))\n",
        "        print()\n",
        "        for x in range(width):\n",
        "            print(\"{0:8}\".format(x), end='')\n",
        "        print('\\r\\n')\n",
        "        for i in range(height - 1, -1, -1):\n",
        "            print(\"{0:4d}\".format(i), end='')\n",
        "            for j in range(width):\n",
        "                loc = i * width + j\n",
        "                p = board.states.get(loc, -1)\n",
        "                if p == player1:\n",
        "                    print('X'.center(8), end='')\n",
        "                elif p == player2:\n",
        "                    print('O'.center(8), end='')\n",
        "                else:\n",
        "                    print('_'.center(8), end='')\n",
        "            print('\\r\\n\\r\\n')\n",
        "\n",
        "    def start_play(self, player1, player2, start_player=0, is_shown=1):\n",
        "        \"\"\"start a game between two players\"\"\"\n",
        "        if start_player not in (0, 1):\n",
        "            raise Exception('start_player should be either 0 (player1 first) '\n",
        "                            'or 1 (player2 first)')\n",
        "        self.board.init_board(start_player)\n",
        "        p1, p2 = self.board.players\n",
        "        player1.set_player_ind(p1)\n",
        "        player2.set_player_ind(p2)\n",
        "        players = {p1: player1, p2: player2}\n",
        "        if is_shown:\n",
        "            self.graphic(self.board, player1.player, player2.player)\n",
        "        while True:\n",
        "            current_player = self.board.get_current_player()\n",
        "            player_in_turn = players[current_player]\n",
        "            move = player_in_turn.get_action(self.board)\n",
        "            self.board.do_move(move)\n",
        "            if is_shown:\n",
        "                self.graphic(self.board, player1.player, player2.player)\n",
        "            end, winner = self.board.game_end()\n",
        "            if end:\n",
        "                if is_shown:\n",
        "                    if winner != -1:\n",
        "                        print(\"Game end. Winner is\", players[winner])\n",
        "                    else:\n",
        "                        print(\"Game end. Tie\")\n",
        "                return winner\n",
        "\n",
        "    def start_self_play(self, player, is_shown=0, temp=1e-3):\n",
        "        \"\"\" start a self-play game using a MCTS player, reuse the search tree,\n",
        "        and store the self-play data: (state, mcts_probs, z) for training\n",
        "        \"\"\"\n",
        "        self.board.init_board()\n",
        "        p1, p2 = self.board.players\n",
        "        states, mcts_probs, current_players = [], [], []\n",
        "        while True:\n",
        "            move, move_probs = player.get_action(self.board,\n",
        "                                                 temp=temp,\n",
        "                                                 return_prob=1)\n",
        "            # store the data\n",
        "            states.append(self.board.current_state())\n",
        "            mcts_probs.append(move_probs)\n",
        "            current_players.append(self.board.current_player)\n",
        "            # perform a move\n",
        "            self.board.do_move(move)\n",
        "            if is_shown:\n",
        "                self.graphic(self.board, p1, p2)\n",
        "            end, winner = self.board.game_end()\n",
        "            if end:\n",
        "                # winner from the perspective of the current player of each state\n",
        "                winners_z = np.zeros(len(current_players))\n",
        "                if winner != -1:\n",
        "                    winners_z[np.array(current_players) == winner] = 1.0\n",
        "                    winners_z[np.array(current_players) != winner] = -1.0\n",
        "                # reset MCTS root node\n",
        "                player.reset_player()\n",
        "                if is_shown:\n",
        "                    if winner != -1:\n",
        "                        print(\"Game end. Winner is player:\", winner)\n",
        "                    else:\n",
        "                        print(\"Game end. Tie\")\n",
        "                return winner, zip(states, mcts_probs, winners_z)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "game_code_path = 'game.py'\n",
        "f = open(game_code_path, 'w')\n",
        "f.write(game_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "WarOvfdxrRW3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mcts_pure_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "A pure implementation of the Monte Carlo Tree Search (MCTS)\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "def rollout_policy_fn(board):\n",
        "    \"\"\"a coarse, fast version of policy_fn used in the rollout phase.\"\"\"\n",
        "    # rollout randomly\n",
        "    action_probs = np.random.rand(len(board.availables))\n",
        "    return zip(board.availables, action_probs)\n",
        "\n",
        "\n",
        "def policy_value_fn(board):\n",
        "    \"\"\"a function that takes in a state and outputs a list of (action, probability)\n",
        "    tuples and a score for the state\"\"\"\n",
        "    # return uniform probabilities and 0 score for pure MCTS\n",
        "    action_probs = np.ones(len(board.availables))/len(board.availables)\n",
        "    return zip(board.availables, action_probs), 0\n",
        "\n",
        "\n",
        "class TreeNode(object):\n",
        "    \"\"\"A node in the MCTS tree. Each node keeps track of its own value Q,\n",
        "    prior probability P, and its visit-count-adjusted prior score u.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parent, prior_p):\n",
        "        self._parent = parent\n",
        "        self._children = {}  # a map from action to TreeNode\n",
        "        self._n_visits = 0\n",
        "        self._Q = 0\n",
        "        self._u = 0\n",
        "        self._P = prior_p\n",
        "\n",
        "    def expand(self, action_priors):\n",
        "        \"\"\"Expand tree by creating new children.\n",
        "        action_priors: a list of tuples of actions and their prior probability\n",
        "            according to the policy function.\n",
        "        \"\"\"\n",
        "        for action, prob in action_priors:\n",
        "            if action not in self._children:\n",
        "                self._children[action] = TreeNode(self, prob)\n",
        "\n",
        "    def select(self, c_puct):\n",
        "        \"\"\"Select action among children that gives maximum action value Q\n",
        "        plus bonus u(P).\n",
        "        Return: A tuple of (action, next_node)\n",
        "        \"\"\"\n",
        "        return max(self._children.items(),\n",
        "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
        "\n",
        "    def update(self, leaf_value):\n",
        "        \"\"\"Update node values from leaf evaluation.\n",
        "        leaf_value: the value of subtree evaluation from the current player's\n",
        "            perspective.\n",
        "        \"\"\"\n",
        "        # Count visit.\n",
        "        self._n_visits += 1\n",
        "        # Update Q, a running average of values for all visits.\n",
        "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
        "\n",
        "    def update_recursive(self, leaf_value):\n",
        "        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
        "        \"\"\"\n",
        "        # If it is not root, this node's parent should be updated first.\n",
        "        if self._parent:\n",
        "            self._parent.update_recursive(-leaf_value)\n",
        "        self.update(leaf_value)\n",
        "\n",
        "    def get_value(self, c_puct):\n",
        "        \"\"\"Calculate and return the value for this node.\n",
        "        It is a combination of leaf evaluations Q, and this node's prior\n",
        "        adjusted for its visit count, u.\n",
        "        c_puct: a number in (0, inf) controlling the relative impact of\n",
        "            value Q, and prior probability P, on this node's score.\n",
        "        \"\"\"\n",
        "        self._u = (c_puct * self._P *\n",
        "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
        "        return self._Q + self._u\n",
        "\n",
        "    def is_leaf(self):\n",
        "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\n",
        "        \"\"\"\n",
        "        return self._children == {}\n",
        "\n",
        "    def is_root(self):\n",
        "        return self._parent is None\n",
        "\n",
        "\n",
        "class MCTS(object):\n",
        "    \"\"\"A simple implementation of Monte Carlo Tree Search.\"\"\"\n",
        "\n",
        "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
        "        \"\"\"\n",
        "        policy_value_fn: a function that takes in a board state and outputs\n",
        "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
        "            (i.e. the expected value of the end game score from the current\n",
        "            player's perspective) for the current player.\n",
        "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
        "            converges to the maximum-value policy. A higher value means\n",
        "            relying on the prior more.\n",
        "        \"\"\"\n",
        "        self._root = TreeNode(None, 1.0)\n",
        "        self._policy = policy_value_fn\n",
        "        self._c_puct = c_puct\n",
        "        self._n_playout = n_playout\n",
        "\n",
        "    def _playout(self, state):\n",
        "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
        "        the leaf and propagating it back through its parents.\n",
        "        State is modified in-place, so a copy must be provided.\n",
        "        \"\"\"\n",
        "        node = self._root\n",
        "        while(1):\n",
        "            if node.is_leaf():\n",
        "\n",
        "                break\n",
        "            # Greedily select next move.\n",
        "            action, node = node.select(self._c_puct)\n",
        "            state.do_move(action)\n",
        "\n",
        "        action_probs, _ = self._policy(state)\n",
        "        # Check for end of game\n",
        "        end, winner = state.game_end()\n",
        "        if not end:\n",
        "            node.expand(action_probs)\n",
        "        # Evaluate the leaf node by random rollout\n",
        "        leaf_value = self._evaluate_rollout(state)\n",
        "        # Update value and visit count of nodes in this traversal.\n",
        "        node.update_recursive(-leaf_value)\n",
        "\n",
        "    def _evaluate_rollout(self, state, limit=1000):\n",
        "        \"\"\"Use the rollout policy to play until the end of the game,\n",
        "        returning +1 if the current player wins, -1 if the opponent wins,\n",
        "        and 0 if it is a tie.\n",
        "        \"\"\"\n",
        "        player = state.get_current_player()\n",
        "        for i in range(limit):\n",
        "            end, winner = state.game_end()\n",
        "            if end:\n",
        "                break\n",
        "            action_probs = rollout_policy_fn(state)\n",
        "            max_action = max(action_probs, key=itemgetter(1))[0]\n",
        "            state.do_move(max_action)\n",
        "        else:\n",
        "            # If no break from the loop, issue a warning.\n",
        "            print(\"WARNING: rollout reached move limit\")\n",
        "        if winner == -1:  # tie\n",
        "            return 0\n",
        "        else:\n",
        "            return 1 if winner == player else -1\n",
        "\n",
        "    def get_move(self, state):\n",
        "        \"\"\"Runs all playouts sequentially and returns the most visited action.\n",
        "        state: the current game state\n",
        "        Return: the selected action\n",
        "        \"\"\"\n",
        "        for n in range(self._n_playout):\n",
        "            state_copy = copy.deepcopy(state)\n",
        "            self._playout(state_copy)\n",
        "        return max(self._root._children.items(),\n",
        "                   key=lambda act_node: act_node[1]._n_visits)[0]\n",
        "\n",
        "    def update_with_move(self, last_move):\n",
        "        \"\"\"Step forward in the tree, keeping everything we already know\n",
        "        about the subtree.\n",
        "        \"\"\"\n",
        "        if last_move in self._root._children:\n",
        "            self._root = self._root._children[last_move]\n",
        "            self._root._parent = None\n",
        "        else:\n",
        "            self._root = TreeNode(None, 1.0)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"MCTS\"\n",
        "\n",
        "\n",
        "class MCTSPlayer(object):\n",
        "    \"\"\"AI player based on MCTS\"\"\"\n",
        "    def __init__(self, c_puct=5, n_playout=2000):\n",
        "        self.mcts = MCTS(policy_value_fn, c_puct, n_playout)\n",
        "\n",
        "    def set_player_ind(self, p):\n",
        "        self.player = p\n",
        "\n",
        "    def reset_player(self):\n",
        "        self.mcts.update_with_move(-1)\n",
        "\n",
        "    def get_action(self, board):\n",
        "        sensible_moves = board.availables\n",
        "        if len(sensible_moves) > 0:\n",
        "            move = self.mcts.get_move(board)\n",
        "            self.mcts.update_with_move(-1)\n",
        "            return move\n",
        "        else:\n",
        "            print(\"WARNING: the board is full\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"MCTS {}\".format(self.player)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "mcts_pure_code_path = 'mcts_pure.py'\n",
        "f = open(mcts_pure_code_path, 'w')\n",
        "f.write(mcts_pure_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Xv2tDIoFrXX7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mcts_alphaZero_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Monte Carlo Tree Search in AlphaGo Zero style, which uses a policy-value\n",
        "network to guide the tree search and evaluate the leaf nodes\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    probs = np.exp(x - np.max(x))\n",
        "    probs /= np.sum(probs)\n",
        "    return probs\n",
        "\n",
        "\n",
        "class TreeNode(object):\n",
        "    \"\"\"A node in the MCTS tree.\n",
        "    Each node keeps track of its own value Q, prior probability P, and\n",
        "    its visit-count-adjusted prior score u.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, parent, prior_p):\n",
        "        self._parent = parent\n",
        "        self._children = {}  # a map from action to TreeNode\n",
        "        self._n_visits = 0\n",
        "        self._Q = 0\n",
        "        self._u = 0\n",
        "        self._P = prior_p\n",
        "\n",
        "    def expand(self, action_priors):\n",
        "        \"\"\"Expand tree by creating new children.\n",
        "        action_priors: a list of tuples of actions and their prior probability\n",
        "            according to the policy function.\n",
        "        \"\"\"\n",
        "        for action, prob in action_priors:\n",
        "            if action not in self._children:\n",
        "                self._children[action] = TreeNode(self, prob)\n",
        "\n",
        "    def select(self, c_puct):\n",
        "        \"\"\"Select action among children that gives maximum action value Q\n",
        "        plus bonus u(P).\n",
        "        Return: A tuple of (action, next_node)\n",
        "        \"\"\"\n",
        "        return max(self._children.items(),\n",
        "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
        "\n",
        "    def update(self, leaf_value):\n",
        "        \"\"\"Update node values from leaf evaluation.\n",
        "        leaf_value: the value of subtree evaluation from the current player's\n",
        "            perspective.\n",
        "        \"\"\"\n",
        "        # Count visit.\n",
        "        self._n_visits += 1\n",
        "        # Update Q, a running average of values for all visits.\n",
        "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
        "\n",
        "    def update_recursive(self, leaf_value):\n",
        "        \"\"\"Like a call to update(), but applied recursively for all ancestors.\n",
        "        \"\"\"\n",
        "        # If it is not root, this node's parent should be updated first.\n",
        "        if self._parent:\n",
        "            self._parent.update_recursive(-leaf_value)\n",
        "        self.update(leaf_value)\n",
        "\n",
        "    def get_value(self, c_puct):\n",
        "        \"\"\"Calculate and return the value for this node.\n",
        "        It is a combination of leaf evaluations Q, and this node's prior\n",
        "        adjusted for its visit count, u.\n",
        "        c_puct: a number in (0, inf) controlling the relative impact of\n",
        "            value Q, and prior probability P, on this node's score.\n",
        "        \"\"\"\n",
        "        self._u = (c_puct * self._P *\n",
        "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
        "        return self._Q + self._u\n",
        "\n",
        "    def is_leaf(self):\n",
        "        \"\"\"Check if leaf node (i.e. no nodes below this have been expanded).\"\"\"\n",
        "        return self._children == {}\n",
        "\n",
        "    def is_root(self):\n",
        "        return self._parent is None\n",
        "\n",
        "\n",
        "class MCTS(object):\n",
        "    \"\"\"An implementation of Monte Carlo Tree Search.\"\"\"\n",
        "\n",
        "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
        "        \"\"\"\n",
        "        policy_value_fn: a function that takes in a board state and outputs\n",
        "            a list of (action, probability) tuples and also a score in [-1, 1]\n",
        "            (i.e. the expected value of the end game score from the current\n",
        "            player's perspective) for the current player.\n",
        "        c_puct: a number in (0, inf) that controls how quickly exploration\n",
        "            converges to the maximum-value policy. A higher value means\n",
        "            relying on the prior more.\n",
        "        \"\"\"\n",
        "        self._root = TreeNode(None, 1.0)\n",
        "        self._policy = policy_value_fn\n",
        "        self._c_puct = c_puct\n",
        "        self._n_playout = n_playout\n",
        "\n",
        "    def _playout(self, state):\n",
        "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
        "        the leaf and propagating it back through its parents.\n",
        "        State is modified in-place, so a copy must be provided.\n",
        "        \"\"\"\n",
        "        node = self._root\n",
        "        while(1):\n",
        "            if node.is_leaf():\n",
        "                break\n",
        "            # Greedily select next move.\n",
        "            action, node = node.select(self._c_puct)\n",
        "            state.do_move(action)\n",
        "\n",
        "        # Evaluate the leaf using a network which outputs a list of\n",
        "        # (action, probability) tuples p and also a score v in [-1, 1]\n",
        "        # for the current player.\n",
        "        action_probs, leaf_value = self._policy(state)\n",
        "        # Check for end of game.\n",
        "        end, winner = state.game_end()\n",
        "        if not end:\n",
        "            node.expand(action_probs)\n",
        "        else:\n",
        "            # for end stateï¼Œreturn the \"true\" leaf_value\n",
        "            if winner == -1:  # tie\n",
        "                leaf_value = 0.0\n",
        "            else:\n",
        "                leaf_value = (\n",
        "                    1.0 if winner == state.get_current_player() else -1.0\n",
        "                )\n",
        "\n",
        "        # Update value and visit count of nodes in this traversal.\n",
        "        node.update_recursive(-leaf_value)\n",
        "\n",
        "    def get_move_probs(self, state, temp=1e-3):\n",
        "        \"\"\"Run all playouts sequentially and return the available actions and\n",
        "        their corresponding probabilities.\n",
        "        state: the current game state\n",
        "        temp: temperature parameter in (0, 1] controls the level of exploration\n",
        "        \"\"\"\n",
        "        for n in range(self._n_playout):\n",
        "            state_copy = copy.deepcopy(state)\n",
        "            self._playout(state_copy)\n",
        "\n",
        "        # calc the move probabilities based on visit counts at the root node\n",
        "        act_visits = [(act, node._n_visits)\n",
        "                      for act, node in self._root._children.items()]\n",
        "        acts, visits = zip(*act_visits)\n",
        "        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
        "\n",
        "        return acts, act_probs\n",
        "\n",
        "    def update_with_move(self, last_move):\n",
        "        \"\"\"Step forward in the tree, keeping everything we already know\n",
        "        about the subtree.\n",
        "        \"\"\"\n",
        "        if last_move in self._root._children:\n",
        "            self._root = self._root._children[last_move]\n",
        "            self._root._parent = None\n",
        "        else:\n",
        "            self._root = TreeNode(None, 1.0)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"MCTS\"\n",
        "\n",
        "\n",
        "class MCTSPlayer(object):\n",
        "    \"\"\"AI player based on MCTS\"\"\"\n",
        "\n",
        "    def __init__(self, policy_value_function,\n",
        "                 c_puct=5, n_playout=2000, is_selfplay=0):\n",
        "        self.mcts = MCTS(policy_value_function, c_puct, n_playout)\n",
        "        self._is_selfplay = is_selfplay\n",
        "\n",
        "    def set_player_ind(self, p):\n",
        "        self.player = p\n",
        "\n",
        "    def reset_player(self):\n",
        "        self.mcts.update_with_move(-1)\n",
        "\n",
        "    def get_action(self, board, temp=1e-3, return_prob=0):\n",
        "        sensible_moves = board.availables\n",
        "        # the pi vector returned by MCTS as in the alphaGo Zero paper\n",
        "        move_probs = np.zeros(board.width*board.height)\n",
        "        if len(sensible_moves) > 0:\n",
        "            acts, probs = self.mcts.get_move_probs(board, temp)\n",
        "            move_probs[list(acts)] = probs\n",
        "            if self._is_selfplay:\n",
        "                # add Dirichlet Noise for exploration (needed for\n",
        "                # self-play training)\n",
        "                move = np.random.choice(\n",
        "                    acts,\n",
        "                    p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs)))\n",
        "                )\n",
        "                # update the root node and reuse the search tree\n",
        "                self.mcts.update_with_move(move)\n",
        "            else:\n",
        "                # with the default temp=1e-3, it is almost equivalent\n",
        "                # to choosing the move with the highest prob\n",
        "                move = np.random.choice(acts, p=probs)\n",
        "                # reset the root node\n",
        "                self.mcts.update_with_move(-1)\n",
        "#                location = board.move_to_location(move)\n",
        "#                print(\"AI move: %d,%d\\n\" % (location[0], location[1]))\n",
        "\n",
        "            if return_prob:\n",
        "                return move, move_probs\n",
        "            else:\n",
        "                return move\n",
        "        else:\n",
        "            print(\"WARNING: the board is full\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"MCTS {}\".format(self.player)\n",
        "'''\n",
        "\n",
        "mcts_alphaZero_code_path = 'mcts_alphaZero.py'\n",
        "f = open(mcts_alphaZero_code_path, 'w')\n",
        "f.write(mcts_alphaZero_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "j6WmMW_MrYmb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l3kafedZouHM"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the training pipeline of AlphaZero for Gomoku\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from game import Board, Game\n",
        "from mcts_pure import MCTSPlayer as MCTS_Pure\n",
        "from mcts_alphaZero import MCTSPlayer\n",
        "# from policy_value_net_theano_lasagne import PolicyValueNet  # Theano and Lasagne\n",
        "# from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n",
        "\n",
        "# from policy_value_net_tensorflow import PolicyValueNet # Tensorflow\n",
        "from policy_value_net_keras import PolicyValueNet # Keras\n",
        "\n",
        "class TrainPipeline():\n",
        "    def __init__(self, init_model=None):\n",
        "        # params of the board and the game\n",
        "        self.board_width = 6\n",
        "        self.board_height = 6\n",
        "        self.n_in_row = 4\n",
        "        self.board = Board(width=self.board_width,\n",
        "                           height=self.board_height,\n",
        "                           n_in_row=self.n_in_row)\n",
        "        self.game = Game(self.board)\n",
        "        # training params\n",
        "        self.learn_rate = 2e-3\n",
        "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
        "        self.temp = 1.0  # the temperature param\n",
        "        self.n_playout = 400  # num of simulations for each move\n",
        "        self.c_puct = 5\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 512  # mini-batch size for training\n",
        "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
        "        self.play_batch_size = 1\n",
        "        self.epochs = 5  # num of train_steps for each update\n",
        "        self.kl_targ = 0.02\n",
        "        self.check_freq = 50\n",
        "        self.game_batch_num = 1500\n",
        "        self.best_win_ratio = 0.0\n",
        "        # num of simulations used for the pure mcts, which is used as\n",
        "        # the opponent to evaluate the trained policy\n",
        "        self.pure_mcts_playout_num = 1000\n",
        "        if init_model:\n",
        "            # start training from an initial policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height,\n",
        "                                                   model_file=init_model)\n",
        "        else:\n",
        "            # start training from a new policy-value net\n",
        "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
        "                                                   self.board_height)\n",
        "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                      c_puct=self.c_puct,\n",
        "                                      n_playout=self.n_playout,\n",
        "                                      is_selfplay=1)\n",
        "\n",
        "    def get_equi_data(self, play_data):\n",
        "        \"\"\"augment the data set by rotation and flipping\n",
        "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
        "        \"\"\"\n",
        "        extend_data = []\n",
        "        for state, mcts_porb, winner in play_data:\n",
        "            for i in [1, 2, 3, 4]:\n",
        "                # rotate counterclockwise\n",
        "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
        "                equi_mcts_prob = np.rot90(np.flipud(\n",
        "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "                # flip horizontally\n",
        "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
        "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
        "                extend_data.append((equi_state,\n",
        "                                    np.flipud(equi_mcts_prob).flatten(),\n",
        "                                    winner))\n",
        "        return extend_data\n",
        "\n",
        "    def collect_selfplay_data(self, n_games=1):\n",
        "        \"\"\"collect self-play data for training\"\"\"\n",
        "        for i in range(n_games):\n",
        "            winner, play_data = self.game.start_self_play(self.mcts_player,\n",
        "                                                          temp=self.temp)\n",
        "            play_data = list(play_data)[:]\n",
        "            self.episode_len = len(play_data)\n",
        "            # augment the data\n",
        "            play_data = self.get_equi_data(play_data)\n",
        "            self.data_buffer.extend(play_data)\n",
        "\n",
        "    def policy_update(self):\n",
        "        \"\"\"update the policy-value net\"\"\"\n",
        "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
        "        state_batch = [data[0] for data in mini_batch]\n",
        "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
        "        winner_batch = [data[2] for data in mini_batch]\n",
        "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
        "        for i in range(self.epochs):\n",
        "            loss, entropy = self.policy_value_net.train_step(\n",
        "                    state_batch,\n",
        "                    mcts_probs_batch,\n",
        "                    winner_batch,\n",
        "                    self.learn_rate*self.lr_multiplier)\n",
        "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
        "            kl = np.mean(np.sum(old_probs * (\n",
        "                    np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
        "                    axis=1)\n",
        "            )\n",
        "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
        "                break\n",
        "        # adaptively adjust the learning rate\n",
        "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
        "            self.lr_multiplier /= 1.5\n",
        "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
        "            self.lr_multiplier *= 1.5\n",
        "\n",
        "        explained_var_old = (1 -\n",
        "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        explained_var_new = (1 -\n",
        "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
        "                             np.var(np.array(winner_batch)))\n",
        "        print((\"kl:{:.5f},\"\n",
        "               \"lr_multiplier:{:.3f},\"\n",
        "               \"loss:{},\"\n",
        "               \"entropy:{},\"\n",
        "               \"explained_var_old:{:.3f},\"\n",
        "               \"explained_var_new:{:.3f}\"\n",
        "               ).format(kl,\n",
        "                        self.lr_multiplier,\n",
        "                        loss,\n",
        "                        entropy,\n",
        "                        explained_var_old,\n",
        "                        explained_var_new))\n",
        "        return loss, entropy\n",
        "\n",
        "    def policy_evaluate(self, n_games=10):\n",
        "        \"\"\"\n",
        "        Evaluate the trained policy by playing against the pure MCTS player\n",
        "        Note: this is only for monitoring the progress of training\n",
        "        \"\"\"\n",
        "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
        "                                         c_puct=self.c_puct,\n",
        "                                         n_playout=self.n_playout)\n",
        "        pure_mcts_player = MCTS_Pure(c_puct=5,\n",
        "                                     n_playout=self.pure_mcts_playout_num)\n",
        "        win_cnt = defaultdict(int)\n",
        "        for i in range(n_games):\n",
        "            winner = self.game.start_play(current_mcts_player,\n",
        "                                          pure_mcts_player,\n",
        "                                          start_player=i % 2,\n",
        "                                          is_shown=0)\n",
        "            win_cnt[winner] += 1\n",
        "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1]) / n_games\n",
        "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
        "                self.pure_mcts_playout_num,\n",
        "                win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
        "        return win_ratio\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"run the training pipeline\"\"\"\n",
        "        try:\n",
        "            for i in range(self.game_batch_num):\n",
        "                self.collect_selfplay_data(self.play_batch_size)\n",
        "                print(\"batch i:{}, episode_len:{}\".format(\n",
        "                        i+1, self.episode_len))\n",
        "                if len(self.data_buffer) > self.batch_size:\n",
        "                    loss, entropy = self.policy_update()\n",
        "                # check the performance of the current model,\n",
        "                # and save the model params\n",
        "                if (i+1) % self.check_freq == 0:\n",
        "                    print(\"current self-play batch: {}\".format(i+1))\n",
        "                    win_ratio = self.policy_evaluate()\n",
        "                    self.policy_value_net.save_model('./current_policy.model')\n",
        "                    if win_ratio > self.best_win_ratio:\n",
        "                        print(\"New best policy!!!!!!!!\")\n",
        "                        self.best_win_ratio = win_ratio\n",
        "                        # update the best_policy\n",
        "                        self.policy_value_net.save_model('./best_policy.model')\n",
        "                        if (self.best_win_ratio == 1.0 and\n",
        "                                self.pure_mcts_playout_num < 5000):\n",
        "                            self.pure_mcts_playout_num += 1000\n",
        "                            self.best_win_ratio = 0.0\n",
        "        except KeyboardInterrupt:\n",
        "            print('\\n\\rquit')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    training_pipeline = TrainPipeline()\n",
        "    training_pipeline.run()\n",
        "'''\n",
        "\n",
        "train_code_path = 'train.py'\n",
        "f = open(train_code_path, 'w')\n",
        "f.write(train_code)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_value_net_keras_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the policyValueNet with Keras\n",
        "Tested under Keras 2.0.5 with tensorflow-gpu 1.2.1 as backend\n",
        "@author: LLY\n",
        "\"\"\" \n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.engine.topology import Input\n",
        "from keras.engine.training import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.core import Activation, Dense, Flatten\n",
        "from keras.layers.merge import Add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "class PolicyValueNet():\n",
        "    \"\"\"policy-value network \"\"\"\n",
        "    def __init__(self, board_width, board_height, model_file=None):\n",
        "        self.board_width = board_width\n",
        "        self.board_height = board_height \n",
        "        self.l2_const = 1e-4  # coef of l2 penalty \n",
        "        self.create_policy_value_net()   \n",
        "        self._loss_train_op()\n",
        "\n",
        "        if model_file:\n",
        "            net_params = pickle.load(open(model_file, 'rb'))\n",
        "            self.model.set_weights(net_params)\n",
        "        \n",
        "    def create_policy_value_net(self):\n",
        "        \"\"\"create the policy value network \"\"\"   \n",
        "        in_x = network = Input((4, self.board_width, self.board_height))\n",
        "\n",
        "        # conv layers\n",
        "        network = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n",
        "        network = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n",
        "        network = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n",
        "        # action policy layers\n",
        "        policy_net = Conv2D(filters=4, kernel_size=(1, 1), data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n",
        "        policy_net = Flatten()(policy_net)\n",
        "        self.policy_net = Dense(self.board_width*self.board_height, activation=\"softmax\", kernel_regularizer=l2(self.l2_const))(policy_net)\n",
        "        # state value layers\n",
        "        value_net = Conv2D(filters=2, kernel_size=(1, 1), data_format=\"channels_first\", activation=\"relu\", kernel_regularizer=l2(self.l2_const))(network)\n",
        "        value_net = Flatten()(value_net)\n",
        "        value_net = Dense(64, kernel_regularizer=l2(self.l2_const))(value_net)\n",
        "        self.value_net = Dense(1, activation=\"tanh\", kernel_regularizer=l2(self.l2_const))(value_net)\n",
        "\n",
        "        self.model = Model(in_x, [self.policy_net, self.value_net])\n",
        "        \n",
        "        def policy_value(state_input):\n",
        "            state_input_union = np.array(state_input)\n",
        "            results = self.model.predict_on_batch(state_input_union)\n",
        "            return results\n",
        "        self.policy_value = policy_value\n",
        "        \n",
        "    def policy_value_fn(self, board):\n",
        "        \"\"\"\n",
        "        input: board\n",
        "        output: a list of (action, probability) tuples for each available action and the score of the board state\n",
        "        \"\"\"\n",
        "        legal_positions = board.availables\n",
        "        current_state = board.current_state()\n",
        "        act_probs, value = self.policy_value(current_state.reshape(-1, 4, self.board_width, self.board_height))\n",
        "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
        "        return act_probs, value[0][0]\n",
        "\n",
        "    def _loss_train_op(self):\n",
        "        \"\"\"\n",
        "        Three loss termsï¼š\n",
        "        loss = (z - v)^2 + pi^T * log(p) + c||theta||^2\n",
        "        \"\"\"\n",
        "\n",
        "        # get the train op   \n",
        "        opt = Adam()\n",
        "        losses = ['categorical_crossentropy', 'mean_squared_error']\n",
        "        self.model.compile(optimizer=opt, loss=losses)\n",
        "\n",
        "        def self_entropy(probs):\n",
        "            return -np.mean(np.sum(probs * np.log(probs + 1e-10), axis=1))\n",
        "\n",
        "        def train_step(state_input, mcts_probs, winner, learning_rate):\n",
        "            state_input_union = np.array(state_input)\n",
        "            mcts_probs_union = np.array(mcts_probs)\n",
        "            winner_union = np.array(winner)\n",
        "            loss = self.model.evaluate(state_input_union, [mcts_probs_union, winner_union], batch_size=len(state_input), verbose=0)\n",
        "            action_probs, _ = self.model.predict_on_batch(state_input_union)\n",
        "            entropy = self_entropy(action_probs)\n",
        "            K.set_value(self.model.optimizer.lr, learning_rate)\n",
        "            self.model.fit(state_input_union, [mcts_probs_union, winner_union], batch_size=len(state_input), verbose=0)\n",
        "            return loss[0], entropy\n",
        "        \n",
        "        self.train_step = train_step\n",
        "\n",
        "    def get_policy_param(self):\n",
        "        net_params = self.model.get_weights()        \n",
        "        return net_params\n",
        "\n",
        "    def save_model(self, model_file):\n",
        "        \"\"\" save model params to file \"\"\"\n",
        "        net_params = self.get_policy_param()\n",
        "        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)\n",
        "'''\n",
        "\n",
        "policy_value_net_keras_code_path = 'policy_value_net_keras.py'\n",
        "f = open(policy_value_net_keras_code_path, 'w')\n",
        "f.write(policy_value_net_keras_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "VhtIi6ihp0cF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_value_net_tensorflow_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the policyValueNet in Tensorflow\n",
        "Tested in Tensorflow 1.4 and 1.5\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class PolicyValueNet():\n",
        "    def __init__(self, board_width, board_height, model_file=None):\n",
        "        self.board_width = board_width\n",
        "        self.board_height = board_height\n",
        "\n",
        "        # Define the tensorflow neural network\n",
        "        # 1. Input:\n",
        "        self.input_states = tf.placeholder(\n",
        "                tf.float32, shape=[None, 4, board_height, board_width])\n",
        "        self.input_state = tf.transpose(self.input_states, [0, 2, 3, 1])\n",
        "        # 2. Common Networks Layers\n",
        "        self.conv1 = tf.layers.conv2d(inputs=self.input_state,\n",
        "                                      filters=32, kernel_size=[3, 3],\n",
        "                                      padding=\"same\", data_format=\"channels_last\",\n",
        "                                      activation=tf.nn.relu)\n",
        "        self.conv2 = tf.layers.conv2d(inputs=self.conv1, filters=64,\n",
        "                                      kernel_size=[3, 3], padding=\"same\",\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      activation=tf.nn.relu)\n",
        "        self.conv3 = tf.layers.conv2d(inputs=self.conv2, filters=128,\n",
        "                                      kernel_size=[3, 3], padding=\"same\",\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      activation=tf.nn.relu)\n",
        "        # 3-1 Action Networks\n",
        "        self.action_conv = tf.layers.conv2d(inputs=self.conv3, filters=4,\n",
        "                                            kernel_size=[1, 1], padding=\"same\",\n",
        "                                            data_format=\"channels_last\",\n",
        "                                            activation=tf.nn.relu)\n",
        "        # Flatten the tensor\n",
        "        self.action_conv_flat = tf.reshape(\n",
        "                self.action_conv, [-1, 4 * board_height * board_width])\n",
        "        # 3-2 Full connected layer, the output is the log probability of moves\n",
        "        # on each slot on the board\n",
        "        self.action_fc = tf.layers.dense(inputs=self.action_conv_flat,\n",
        "                                         units=board_height * board_width,\n",
        "                                         activation=tf.nn.log_softmax)\n",
        "        # 4 Evaluation Networks\n",
        "        self.evaluation_conv = tf.layers.conv2d(inputs=self.conv3, filters=2,\n",
        "                                                kernel_size=[1, 1],\n",
        "                                                padding=\"same\",\n",
        "                                                data_format=\"channels_last\",\n",
        "                                                activation=tf.nn.relu)\n",
        "        self.evaluation_conv_flat = tf.reshape(\n",
        "                self.evaluation_conv, [-1, 2 * board_height * board_width])\n",
        "        self.evaluation_fc1 = tf.layers.dense(inputs=self.evaluation_conv_flat,\n",
        "                                              units=64, activation=tf.nn.relu)\n",
        "        # output the score of evaluation on current state\n",
        "        self.evaluation_fc2 = tf.layers.dense(inputs=self.evaluation_fc1,\n",
        "                                              units=1, activation=tf.nn.tanh)\n",
        "\n",
        "        # Define the Loss function\n",
        "        # 1. Label: the array containing if the game wins or not for each state\n",
        "        self.labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        # 2. Predictions: the array containing the evaluation score of each state\n",
        "        # which is self.evaluation_fc2\n",
        "        # 3-1. Value Loss function\n",
        "        self.value_loss = tf.losses.mean_squared_error(self.labels,\n",
        "                                                       self.evaluation_fc2)\n",
        "        # 3-2. Policy Loss function\n",
        "        self.mcts_probs = tf.placeholder(\n",
        "                tf.float32, shape=[None, board_height * board_width])\n",
        "        self.policy_loss = tf.negative(tf.reduce_mean(\n",
        "                tf.reduce_sum(tf.multiply(self.mcts_probs, self.action_fc), 1)))\n",
        "        # 3-3. L2 penalty (regularization)\n",
        "        l2_penalty_beta = 1e-4\n",
        "        vars = tf.trainable_variables()\n",
        "        l2_penalty = l2_penalty_beta * tf.add_n(\n",
        "            [tf.nn.l2_loss(v) for v in vars if 'bias' not in v.name.lower()])\n",
        "        # 3-4 Add up to be the Loss function\n",
        "        self.loss = self.value_loss + self.policy_loss + l2_penalty\n",
        "\n",
        "        # Define the optimizer we use for training\n",
        "        self.learning_rate = tf.placeholder(tf.float32)\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "                learning_rate=self.learning_rate).minimize(self.loss)\n",
        "\n",
        "        # Make a session\n",
        "        self.session = tf.Session()\n",
        "\n",
        "        # calc policy entropy, for monitoring only\n",
        "        self.entropy = tf.negative(tf.reduce_mean(\n",
        "                tf.reduce_sum(tf.exp(self.action_fc) * self.action_fc, 1)))\n",
        "\n",
        "        # Initialize variables\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.session.run(init)\n",
        "\n",
        "        # For saving and restoring\n",
        "        self.saver = tf.train.Saver()\n",
        "        if model_file is not None:\n",
        "            self.restore_model(model_file)\n",
        "\n",
        "    def policy_value(self, state_batch):\n",
        "        \"\"\"\n",
        "        input: a batch of states\n",
        "        output: a batch of action probabilities and state values\n",
        "        \"\"\"\n",
        "        log_act_probs, value = self.session.run(\n",
        "                [self.action_fc, self.evaluation_fc2],\n",
        "                feed_dict={self.input_states: state_batch}\n",
        "                )\n",
        "        act_probs = np.exp(log_act_probs)\n",
        "        return act_probs, value\n",
        "\n",
        "    def policy_value_fn(self, board):\n",
        "        \"\"\"\n",
        "        input: board\n",
        "        output: a list of (action, probability) tuples for each available\n",
        "        action and the score of the board state\n",
        "        \"\"\"\n",
        "        legal_positions = board.availables\n",
        "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
        "                -1, 4, self.board_width, self.board_height))\n",
        "        act_probs, value = self.policy_value(current_state)\n",
        "        act_probs = zip(legal_positions, act_probs[0][legal_positions])\n",
        "        return act_probs, value\n",
        "\n",
        "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
        "        \"\"\"perform a training step\"\"\"\n",
        "        winner_batch = np.reshape(winner_batch, (-1, 1))\n",
        "        loss, entropy, _ = self.session.run(\n",
        "                [self.loss, self.entropy, self.optimizer],\n",
        "                feed_dict={self.input_states: state_batch,\n",
        "                           self.mcts_probs: mcts_probs,\n",
        "                           self.labels: winner_batch,\n",
        "                           self.learning_rate: lr})\n",
        "        return loss, entropy\n",
        "\n",
        "    def save_model(self, model_path):\n",
        "        self.saver.save(self.session, model_path)\n",
        "\n",
        "    def restore_model(self, model_path):\n",
        "        self.saver.restore(self.session, model_path)\n",
        "'''\n",
        "\n",
        "policy_value_net_tensorflow_code_path = 'policy_value_net_tensorflow.py'\n",
        "f = open(policy_value_net_tensorflow_code_path, 'w')\n",
        "f.write(policy_value_net_tensorflow_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1fYz5Pinp2Va"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_value_net_pytorch_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the policyValueNet in PyTorch\n",
        "Tested in PyTorch 0.2.0 and 0.3.0\n",
        "@author: Junxiao Song\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_learning_rate(optimizer, lr):\n",
        "    \"\"\"Sets the learning rate to the given value\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"policy-value network module\"\"\"\n",
        "    def __init__(self, board_width, board_height):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.board_width = board_width\n",
        "        self.board_height = board_height\n",
        "        # common layers\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        # action policy layers\n",
        "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
        "        self.act_fc1 = nn.Linear(4*board_width*board_height,\n",
        "                                 board_width*board_height)\n",
        "        # state value layers\n",
        "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
        "        self.val_fc1 = nn.Linear(2*board_width*board_height, 64)\n",
        "        self.val_fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state_input):\n",
        "        # common layers\n",
        "        x = F.relu(self.conv1(state_input))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        # action policy layers\n",
        "        x_act = F.relu(self.act_conv1(x))\n",
        "        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n",
        "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
        "        # state value layers\n",
        "        x_val = F.relu(self.val_conv1(x))\n",
        "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
        "        x_val = F.relu(self.val_fc1(x_val))\n",
        "        x_val = F.tanh(self.val_fc2(x_val))\n",
        "        return x_act, x_val\n",
        "\n",
        "\n",
        "class PolicyValueNet():\n",
        "    \"\"\"policy-value network \"\"\"\n",
        "    def __init__(self, board_width, board_height,\n",
        "                 model_file=None, use_gpu=False):\n",
        "        self.use_gpu = use_gpu\n",
        "        self.board_width = board_width\n",
        "        self.board_height = board_height\n",
        "        self.l2_const = 1e-4  # coef of l2 penalty\n",
        "        # the policy value net module\n",
        "        if self.use_gpu:\n",
        "            self.policy_value_net = Net(board_width, board_height).cuda()\n",
        "        else:\n",
        "            self.policy_value_net = Net(board_width, board_height)\n",
        "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n",
        "                                    weight_decay=self.l2_const)\n",
        "\n",
        "        if model_file:\n",
        "            net_params = torch.load(model_file)\n",
        "            self.policy_value_net.load_state_dict(net_params)\n",
        "\n",
        "    def policy_value(self, state_batch):\n",
        "        \"\"\"\n",
        "        input: a batch of states\n",
        "        output: a batch of action probabilities and state values\n",
        "        \"\"\"\n",
        "        if self.use_gpu:\n",
        "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
        "            log_act_probs, value = self.policy_value_net(state_batch)\n",
        "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
        "            return act_probs, value.data.cpu().numpy()\n",
        "        else:\n",
        "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
        "            log_act_probs, value = self.policy_value_net(state_batch)\n",
        "            act_probs = np.exp(log_act_probs.data.numpy())\n",
        "            return act_probs, value.data.numpy()\n",
        "\n",
        "    def policy_value_fn(self, board):\n",
        "        \"\"\"\n",
        "        input: board\n",
        "        output: a list of (action, probability) tuples for each available\n",
        "        action and the score of the board state\n",
        "        \"\"\"\n",
        "        legal_positions = board.availables\n",
        "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
        "                -1, 4, self.board_width, self.board_height))\n",
        "        if self.use_gpu:\n",
        "            log_act_probs, value = self.policy_value_net(\n",
        "                    Variable(torch.from_numpy(current_state)).cuda().float())\n",
        "            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
        "        else:\n",
        "            log_act_probs, value = self.policy_value_net(\n",
        "                    Variable(torch.from_numpy(current_state)).float())\n",
        "            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n",
        "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
        "        value = value.data[0][0]\n",
        "        return act_probs, value\n",
        "\n",
        "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
        "        \"\"\"perform a training step\"\"\"\n",
        "        # wrap in Variable\n",
        "        if self.use_gpu:\n",
        "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
        "            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n",
        "            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n",
        "        else:\n",
        "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
        "            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n",
        "            winner_batch = Variable(torch.FloatTensor(winner_batch))\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        # set learning rate\n",
        "        set_learning_rate(self.optimizer, lr)\n",
        "\n",
        "        # forward\n",
        "        log_act_probs, value = self.policy_value_net(state_batch)\n",
        "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
        "        # Note: the L2 penalty is incorporated in optimizer\n",
        "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
        "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
        "        loss = value_loss + policy_loss\n",
        "        # backward and optimize\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # calc policy entropy, for monitoring only\n",
        "        entropy = -torch.mean(\n",
        "                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
        "                )\n",
        "        return loss.data[0], entropy.data[0]\n",
        "        #for pytorch version >= 0.5 please use the following line instead.\n",
        "        #return loss.item(), entropy.item()\n",
        "\n",
        "    def get_policy_param(self):\n",
        "        net_params = self.policy_value_net.state_dict()\n",
        "        return net_params\n",
        "\n",
        "    def save_model(self, model_file):\n",
        "        \"\"\" save model params to file \"\"\"\n",
        "        net_params = self.get_policy_param()  # get model params\n",
        "        torch.save(net_params, model_file)\n",
        "'''\n",
        "\n",
        "policy_value_net_pytorch_code_path = 'policy_value_net_pytorch.py'\n",
        "f = open(policy_value_net_pytorch_code_path, 'w')\n",
        "f.write(policy_value_net_pytorch_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "TlNq8vsCq2qq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_value_net_theano_lasagne_code=r'''\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "An implementation of the policyValueNet in Theano and Lasagne\n",
        "@author: LLY\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "import lasagne\n",
        "import pickle\n",
        "\n",
        "\n",
        "class PolicyValueNet():\n",
        "    \"\"\"policy-value network \"\"\"\n",
        "    def __init__(self, board_width, board_height, model_file=None):\n",
        "        self.board_width = board_width\n",
        "        self.board_height = board_height\n",
        "        self.learning_rate = T.scalar('learning_rate')\n",
        "        self.l2_const = 1e-4  # coef of l2 penalty\n",
        "        self.create_policy_value_net()\n",
        "        self._loss_train_op()\n",
        "        if model_file:\n",
        "            try:\n",
        "                net_params = pickle.load(open(model_file, 'rb'))\n",
        "            except:\n",
        "                # To support loading pretrained model in python3\n",
        "                net_params = pickle.load(open(model_file, 'rb'),\n",
        "                                         encoding='bytes')\n",
        "            lasagne.layers.set_all_param_values(\n",
        "                    [self.policy_net, self.value_net], net_params\n",
        "                    )\n",
        "\n",
        "    def create_policy_value_net(self):\n",
        "        \"\"\"create the policy value network \"\"\"\n",
        "        self.state_input = T.tensor4('state')\n",
        "        self.winner = T.vector('winner')\n",
        "        self.mcts_probs = T.matrix('mcts_probs')\n",
        "        network = lasagne.layers.InputLayer(\n",
        "                shape=(None, 4, self.board_width, self.board_height),\n",
        "                input_var=self.state_input\n",
        "                )\n",
        "        # conv layers\n",
        "        network = lasagne.layers.Conv2DLayer(\n",
        "                network, num_filters=32, filter_size=(3, 3), pad='same')\n",
        "        network = lasagne.layers.Conv2DLayer(\n",
        "                network, num_filters=64, filter_size=(3, 3), pad='same')\n",
        "        network = lasagne.layers.Conv2DLayer(\n",
        "                network, num_filters=128, filter_size=(3, 3), pad='same')\n",
        "        # action policy layers\n",
        "        policy_net = lasagne.layers.Conv2DLayer(\n",
        "                network, num_filters=4, filter_size=(1, 1))\n",
        "        self.policy_net = lasagne.layers.DenseLayer(\n",
        "                policy_net, num_units=self.board_width*self.board_height,\n",
        "                nonlinearity=lasagne.nonlinearities.softmax)\n",
        "        # state value layers\n",
        "        value_net = lasagne.layers.Conv2DLayer(\n",
        "                network, num_filters=2, filter_size=(1, 1))\n",
        "        value_net = lasagne.layers.DenseLayer(value_net, num_units=64)\n",
        "        self.value_net = lasagne.layers.DenseLayer(\n",
        "                value_net, num_units=1,\n",
        "                nonlinearity=lasagne.nonlinearities.tanh)\n",
        "        # get action probs and state score value\n",
        "        self.action_probs, self.value = lasagne.layers.get_output(\n",
        "                [self.policy_net, self.value_net])\n",
        "        self.policy_value = theano.function([self.state_input],\n",
        "                                            [self.action_probs, self.value],\n",
        "                                            allow_input_downcast=True)\n",
        "\n",
        "    def policy_value_fn(self, board):\n",
        "        \"\"\"\n",
        "        input: board\n",
        "        output: a list of (action, probability) tuples for each available\n",
        "            action and the score of the board state\n",
        "        \"\"\"\n",
        "        legal_positions = board.availables\n",
        "        current_state = board.current_state()\n",
        "        act_probs, value = self.policy_value(\n",
        "            current_state.reshape(-1, 4, self.board_width, self.board_height)\n",
        "            )\n",
        "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
        "        return act_probs, value[0][0]\n",
        "\n",
        "    def _loss_train_op(self):\n",
        "        \"\"\"\n",
        "        Three loss termsï¼š\n",
        "        loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
        "        \"\"\"\n",
        "        params = lasagne.layers.get_all_params(\n",
        "                [self.policy_net, self.value_net], trainable=True)\n",
        "        value_loss = lasagne.objectives.squared_error(\n",
        "                self.winner, self.value.flatten())\n",
        "        policy_loss = lasagne.objectives.categorical_crossentropy(\n",
        "                self.action_probs, self.mcts_probs)\n",
        "        l2_penalty = lasagne.regularization.apply_penalty(\n",
        "                params, lasagne.regularization.l2)\n",
        "        self.loss = self.l2_const*l2_penalty + lasagne.objectives.aggregate(\n",
        "                value_loss + policy_loss, mode='mean')\n",
        "        # policy entropyï¼Œfor monitoring only\n",
        "        self.entropy = -T.mean(T.sum(\n",
        "                self.action_probs * T.log(self.action_probs + 1e-10), axis=1))\n",
        "        # get the train op\n",
        "        updates = lasagne.updates.adam(self.loss, params,\n",
        "                                       learning_rate=self.learning_rate)\n",
        "        self.train_step = theano.function(\n",
        "            [self.state_input, self.mcts_probs, self.winner, self.learning_rate],\n",
        "            [self.loss, self.entropy],\n",
        "            updates=updates,\n",
        "            allow_input_downcast=True\n",
        "            )\n",
        "\n",
        "    def get_policy_param(self):\n",
        "        net_params = lasagne.layers.get_all_param_values(\n",
        "                [self.policy_net, self.value_net])\n",
        "        return net_params\n",
        "\n",
        "    def save_model(self, model_file):\n",
        "        \"\"\" save model params to file \"\"\"\n",
        "        net_params = self.get_policy_param()  # get model params\n",
        "        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)\n",
        "'''\n",
        "\n",
        "policy_value_net_theano_lasagne_code_path = 'policy_value_net_theano_lasagne.py'\n",
        "f = open(policy_value_net_theano_lasagne_code_path, 'w')\n",
        "f.write(policy_value_net_theano_lasagne_code)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "5YOtYN7DsiIH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tips for training:\n",
        "\n",
        "1.It is good to start with a 6 * 6 board and 4 in a row. For this case, we may obtain a reasonably good model within 500~1000 self-play games in about 2 hours.\n",
        "\n",
        "\n",
        "2.For the case of 8 * 8 board and 5 in a row, it may need 2000~3000 self-play games to get a good model, and it may take about 2 days on a single PC."
      ],
      "metadata": {
        "id": "xoQkbemfr34a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install tensorflow==1.14\n",
        "#!pip install keras #ModuleNotFoundError: No module named 'tensorflow.contrib'\n",
        "!pip install keras==2.0.5\n",
        "!pip install keras\n",
        "#!pip install tensorflow==1.5\n",
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxmmVonGrneg",
        "outputId": "a1c28fad-d404-48ed-9f6b-dd408b93a98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.48.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.0.5\n",
            "    Uninstalling Keras-2.0.5:\n",
            "      Successfully uninstalled Keras-2.0.5\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed keras-2.10.0 tensorboard-2.10.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.14\n",
            "  Using cached tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.48.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Using cached tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.4.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Using cached tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.10.0\n",
            "    Uninstalling tensorflow-estimator-2.10.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.10.1\n",
            "    Uninstalling tensorboard-2.10.1:\n",
            "      Successfully uninstalled tensorboard-2.10.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.10.0\n",
            "    Uninstalling tensorflow-2.10.0:\n",
            "      Successfully uninstalled tensorflow-2.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.0.5\n",
            "  Using cached Keras-2.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.5) (6.0)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from keras==2.0.5) (1.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras==2.0.5) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano->keras==2.0.5) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano->keras==2.0.5) (1.21.6)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.10.0\n",
            "    Uninstalling keras-2.10.0:\n",
            "      Successfully uninstalled keras-2.10.0\n",
            "Successfully installed keras-2.0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.0.5)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from keras) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from theano->keras) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from theano->keras) (1.7.3)\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1190: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:699: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2751: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2290: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-09-28 02:29:33.506150: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-09-28 02:29:33.510095: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-09-28 02:29:33.510366: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x229f800 executing computations on platform Host. Devices:\n",
            "2022-09-28 02:29:33.510409: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2022-09-28 02:29:33.540417: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "batch i:1, episode_len:18\n",
            "batch i:2, episode_len:22\n",
            "batch i:3, episode_len:27\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:601: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "kl:0.01095,lr_multiplier:1.000,loss:4.507872581481934,entropy:3.571803092956543,explained_var_old:-0.001,explained_var_new:0.142\n"
          ]
        }
      ]
    }
  ]
}