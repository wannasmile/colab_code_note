{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIb3hOHSIPcubJOJELBi5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/IRC013.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " GBNet 是一个将梯度提升框架（如 XGBoost 和 LightGBM）与 PyTorch 深度学习框架结合的工具库，其核心思想是通过 PyTorch 的自动微分和计算图能力，为传统梯度提升模型（GBM）提供更灵活的训练和扩展能力。以下是通俗易懂的讲解：\n",
        "\n",
        "---\n",
        "\n",
        "### **1. GBNet 的核心思想**\n",
        "GBNet 的目标是将 **梯度提升算法**（Gradient Boosting）与 **PyTorch 的神经网络训练流程** 结合，实现以下功能：\n",
        "- **复用 PyTorch 生态**：利用 PyTorch 的损失函数、优化器和自动微分功能，简化梯度提升模型的训练过程。\n",
        "- **灵活扩展模型**：支持同时训练多个梯度提升模型（如 XGBoost 和 LightGBM），并通过 PyTorch 的模块化设计组合它们（例如拼接输出或相乘），实现更复杂的预测逻辑。\n",
        "- **统一训练流程**：将传统 GBM 的串行训练过程融入 PyTorch 的 `fit`/`step` 循环中，降低用户学习成本。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. 梯度提升算法基础**\n",
        "在深入 GBNet 之前，需要先理解 **梯度提升** 的基本原理：\n",
        "1. **初始化模型**：从简单模型（如均值预测）开始。\n",
        "2. **迭代改进**：每轮训练一个新模型（通常是决策树），专门拟合上一轮模型的 **残差**（即真实值与预测值的差）。\n",
        "3. **累加结果**：新模型的预测结果加到总预测中，逐步逼近真实值。\n",
        "\n",
        "例如，预测年龄时，第一棵树预测 20 岁，发现残差为 -6 岁（实际 14 岁），第二棵树专门预测 -6 岁，最终累加结果准确。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. GBNet 的实现原理**\n",
        "GBNet 通过以下方式将梯度提升与 PyTorch 结合：\n",
        "#### **(1) PyTorch 模块封装**\n",
        "GBNet 提供了 `XGBModule` 和 `LGBModule` 两个类，将 XGBoost/LightGBM 封装为 PyTorch 模块：\n",
        "- **输入输出**：接受数据（如 `X`）并输出预测结果（如 `F(X)`）。\n",
        "- **自动微分**：通过 `loss.backward()` 计算梯度，调用 `gb_step()` 更新模型参数。\n",
        "\n",
        "#### **(2) 训练流程**\n",
        "与传统 GBM 类似，但融入 PyTorch 的训练循环：\n",
        "```python\n",
        "# 初始化模型\n",
        "model = XGBModule(input_dim, output_dim)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 迭代训练\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()  # 清空梯度\n",
        "    preds = model(X)  # 前向传播\n",
        "    loss = loss_fn(preds, y)  # 计算损失\n",
        "    loss.backward()  # 反向传播\n",
        "    model.gb_step()  # 更新 GBM 参数\n",
        "    optimizer.step()  # 更新优化器参数\n",
        "```\n",
        "- **关键区别**：传统 GBM 使用小批量或随机梯度，而 GBNet 需要全量数据以支持缓存优化。\n",
        "\n",
        "#### **(3) 联合训练多个模型**\n",
        "GBNet 支持同时训练多个梯度提升模型，并通过 PyTorch 的 `nn.Module` 组合它们：\n",
        "```python\n",
        "class CombinedModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.xgb = XGBModule(...)\n",
        "        self.lgb = LGBModule(...)\n",
        "        self.linear = torch.nn.Linear(...)\n",
        "\n",
        "    def forward(self, x):\n",
        "        xgb_out = self.xgb(x)\n",
        "        lgb_out = self.lgb(x)\n",
        "        combined = self.linear(xgb_out + lgb_out)\n",
        "        return combined\n",
        "```\n",
        "这种设计允许用户像搭积木一样组合不同模型，例如用户和物品的嵌入向量相乘。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. GBNet 的优势**\n",
        "- **灵活性**：支持自定义损失函数和优化器，适应不同任务（如回归、分类、时间序列预测）。\n",
        "- **扩展性**：可轻松集成到现有 PyTorch 项目中，支持分布式训练和混合精度计算。\n",
        "- **性能**：在部分场景（如时间序列预测）中，GBNet 的自定义模型（如 `Forecast`）表现优于传统工具（如 Prophet）。\n",
        "\n",
        "---\n",
        "\n",
        "### **5. 总结**\n",
        "GBNet 的本质是 **用 PyTorch 重新包装梯度提升算法**，使其兼具深度学习的灵活性和梯度提升的高效性。用户无需深入理解 GBM 的底层细节，即可通过类似神经网络的 API 快速构建和优化复杂模型。其核心价值在于 **降低梯度提升与深度学习的迁移成本**，同时保留 GBM 在结构化数据任务中的优势。"
      ],
      "metadata": {
        "id": "5sds55L66urC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M9K0HVCZ6nBZ"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 定义一个抽象基类，继承自PyTorch的nn.Module和Python的abc.ABC\n",
        "# 用于规范梯度提升模块的通用接口和功能实现\n",
        "class BaseGBModule(nn.Module, abc.ABC):\n",
        "   # 类属性，定义最小Hessian值，默认为0.0\n",
        "   min_hess = 0.0\n",
        "\n",
        "   # 构造方法：初始化基类和模块属性\n",
        "   # min_hess: 允许用户设置最小Hessian值\n",
        "   # grad/hess: 用于存储梯度/二阶导数数据，默认初始化为None\n",
        "   def __init__(self, min_hess=0.0):\n",
        "       super(BaseGBModule, self).__init__()  # 调用nn.Module的构造方法\n",
        "       self.min_hess = min_hess  # 设置最小Hessian值\n",
        "       self.grad = None  # 初始化梯度存储变量\n",
        "       self.hess = None  # 初始化二阶导数存储变量\n",
        "\n",
        "   # 抽象方法：输入数据验证与预处理\n",
        "   # input_data: 模型特定格式的输入数据\n",
        "   # 返回值: 处理后的输入数据（直接返回原数据或进行格式转换）\n",
        "   @abc.abstractmethod\n",
        "   def _input_checking_setting(self, input_data):\n",
        "       pass  # 具体实现由子类完成\n",
        "\n",
        "   # 抽象方法：模型前向传播\n",
        "   # input_data: 模型特定格式的输入数据\n",
        "   # return_tensor: 是否返回PyTorch张量（默认True）\n",
        "   # 返回值: 模型预测结果（张量或NumPy数组）\n",
        "   @abc.abstractmethod\n",
        "   def forward(self, input_data, return_tensor=True):\n",
        "       pass  # 具体实现由子类完成\n",
        "\n",
        "   # 内部方法：计算梯度与二阶导数（基于FX的梯度）\n",
        "   # grad: 梯度值（按样本数缩放后的结果）\n",
        "   # hess: 二阶导数矩阵（每列对应一个输出特征的Hessian）\n",
        "   def _get_grad_hess_FX(self):\n",
        "       # 计算梯度：原始梯度乘以样本数量（用于批量计算的平均）\n",
        "       grad = self.FX.grad * self.FX.shape[0]\n",
        "\n",
        "       # 初始化Hessian列表\n",
        "       hesses = []\n",
        "       # 按输出特征维度逐列计算Hessian\n",
        "       for i in range(self.output_dim):\n",
        "           # 对梯度列求和，计算该列的Hessian\n",
        "           # retain_graph=True保留计算图以便多次反向传播\n",
        "           hessian_col = torch.autograd.grad(\n",
        "               grad[:, i].sum(), self.FX, retain_graph=True\n",
        "           )[0][:, i : (i + 1)]\n",
        "           hesses.append(hessian_col)\n",
        "       # 拼接所有Hessian列，并确保值不低于min_hess\n",
        "       hess = torch.maximum(\n",
        "           torch.cat(hesses, axis=1), torch.Tensor([self.min_hess])\n",
        "       )\n",
        "       return grad, hess  # 返回梯度和Hessian\n",
        "\n",
        "   # 抽象方法：执行一次梯度提升迭代\n",
        "   # 包含三个核心步骤：\n",
        "   # 1. 获取当前样本的梯度/二阶导数\n",
        "   # 2. 训练一个弱学习器（基模型）\n",
        "   # 3. 更新模型预测结果\n",
        "   @abc.abstractmethod\n",
        "   def gb_step(self):\n",
        "       pass  # 具体实现由子类完成"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面将用通俗的语言解释这段代码中二阶导数（Hessian 矩阵）的计算原理，并结合上下文说明它的作用。\n",
        "\n",
        "---\n",
        "\n",
        "### **1. 什么是二阶导数（Hessian 矩阵）？**\n",
        "在数学中，二阶导数描述了函数的「曲率」。对于机器学习模型来说：\n",
        "- **一阶导数（梯度）**：告诉我们模型在哪个方向上调整参数能最快降低损失（类似爬山时往陡坡下走）。\n",
        "- **二阶导数（Hessian 矩阵）**：告诉我们这个方向上的「陡峭程度」。如果二阶导数为正，说明这是一个山谷；如果为负，则是山峰。在优化中，Hessian 用于调整学习步长，防止步子太大或太小。\n",
        "\n",
        "在梯度提升中，Hessian 的作用类似于正则化项，能让模型更新更稳定。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. GBNet 中的二阶导数计算逻辑**\n",
        "代码中的 `_get_grad_hess_FX` 方法负责计算梯度和 Hessian 矩阵。以下是分步解释：\n",
        "\n",
        "#### **(1) 计算梯度（一阶导数）**\n",
        "```python\n",
        "grad = self.FX.grad * self.FX.shape[0]\n",
        "```\n",
        "- `self.FX` 是模型的预测值（类似 `y_pred`）。\n",
        "- `self.FX.grad` 是预测值对输入数据的梯度（即损失函数对 `FX` 的导数）。\n",
        "- `self.FX.shape[0]` 是对梯度进行缩放（假设输入是批量数据，这里取平均梯度）。\n",
        "\n",
        "#### **(2) 计算 Hessian 矩阵（二阶导数）**\n",
        "```python\n",
        "hesses = []\n",
        "for i in range(self.output_dim):\n",
        "    hessian_col = torch.autograd.grad(\n",
        "        grad[:, i].sum(),  # 对第i列梯度求和\n",
        "        self.FX,  # 反向传播的目标变量\n",
        "        retain_graph=True  # 保留计算图以复用\n",
        "    )[0][:, i : (i + 1)]  # 提取第i列的Hessian\n",
        "    hesses.append(hessian_col)\n",
        "hess = torch.maximum(torch.cat(hesses, axis=1), torch.Tensor([self.min_hess]))\n",
        "```\n",
        "- **逐列计算**：Hessian 矩阵的每一列对应一个输出的二阶导数。\n",
        "- **为什么对梯度列求和？**  \n",
        "  目的是将单个样本的梯度信息聚合为整体方向（类似求平均值），从而稳定 Hessian 的计算。\n",
        "- **反向传播**：通过 `torch.autograd.grad` 计算梯度列的和对 `FX` 的导数，得到该列对应的 Hessian。\n",
        "- **数值稳定性**：`torch.maximum(..., self.min_hess)` 确保 Hessian 最小值为 `min_hess`，防止矩阵中出现零或负数导致优化失败。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Hessian 在梯度提升中的作用**\n",
        "GBNet 中的 Hessian 主要有以下用途：\n",
        "1. **学习率自适应**：  \n",
        "   在梯度提升中，Hessian 用于调整每棵新树的学习率（类似 XGBoost 中的 `eta` 参数）。较大的 Hessian 值（曲率更大）意味着模型在这一区域的预测较敏感，需要更小的步长。\n",
        "2. **正则化**：  \n",
        "   通过限制 Hessian 的最小值（`min_hess`），可以防止优化过程中出现数值不稳定或过拟合。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. 举例类比**\n",
        "假设我们要预测房价（输出是连续值），某轮迭代中模型的预测值 `FX` 和真实值 `y` 之间的残差较大。此时：\n",
        "- **梯度**：指向减小残差最快的方向（类似地图上的指南针）。\n",
        "- **Hessian**：告诉你这个方向是陡坡还是平地。如果是陡坡（Hessian 大），你需要迈小步；如果是平地（Hessian 小），可以迈大步。\n",
        "\n",
        "---\n",
        "\n",
        "### **总结**\n",
        "这段代码通过 PyTorch 的自动微分功能，实现了梯度提升中二阶导数的计算：\n",
        "1. **梯度计算**：指导模型往损失下降最快的方向走。\n",
        "2. **Hessian 计算**：控制步长大小，确保更新稳定且不陷入局部最优。\n",
        "\n",
        "这种设计让 GBNet 在保留梯度提升高效性的同时，能够灵活集成到 PyTorch 的深度学习框架中。"
      ],
      "metadata": {
        "id": "XDhAdkS7BsEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.linalg import cho_solve, cho_factor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from gbnet.base import BaseGBModule\n",
        "\n",
        "\n",
        "class GBLinear(BaseGBModule):\n",
        "    \"\"\"实现基于梯度提升的线性模型模块\n",
        "\n",
        "    该模块通过梯度提升框架训练线性模型，维护迭代状态并通过计算的梯度/二阶导数更新参数\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,  # 输入特征维度\n",
        "        output_dim,  # 输出预测维度\n",
        "        bias=True,  # 是否包含偏置项\n",
        "        lr=0.1,  # 参数更新学习率\n",
        "        min_hess=0.0,  # Hessian最小阈值\n",
        "        lambd=0.01  # L2正则化系数\n",
        "    ):\n",
        "        super(BaseGBModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.min_hess = min_hess\n",
        "        self.bias = bias\n",
        "        self.lr = lr\n",
        "        self.lambd = lambd\n",
        "\n",
        "        # 创建PyTorch线性层\n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim, bias=self.bias)\n",
        "        # 缓存变量\n",
        "        self.FX = None  # 当前预测值\n",
        "        self.input = None  # 输入数据缓存\n",
        "        self.g = None  # 梯度缓存\n",
        "        self.h = None  # Hessian缓存\n",
        "\n",
        "\n",
        "    def _input_checking_setting(self, x: Union[torch.Tensor, np.ndarray, pd.DataFrame]):\n",
        "        \"\"\"统一输入格式并进行预处理\n",
        "\n",
        "        确保输入数据转换为PyTorch张量，并在训练模式下缓存原始数据\n",
        "        \"\"\"\n",
        "        assert isinstance(x, (torch.Tensor, np.ndarray, pd.DataFrame)), \"输入类型不支持\"\n",
        "\n",
        "        # 类型转换\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.Tensor(x)\n",
        "        elif isinstance(x, pd.DataFrame):\n",
        "            x = torch.Tensor(np.array(x))\n",
        "\n",
        "        # 训练模式需要缓存输入数据\n",
        "        if self.training:\n",
        "            self.input = x.detach().numpy()  # 转换为NumPy数组并断开计算图\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x: Union[torch.Tensor, np.ndarray, pd.DataFrame]):\n",
        "        \"\"\"前向传播计算预测值\n",
        "\n",
        "        将输入数据通过线性层得到预测结果，并在训练模式下保留梯度信息\n",
        "        \"\"\"\n",
        "        x = self._input_checking_setting(x)  # 统一输入格式\n",
        "\n",
        "        self.FX = self.linear(x)  # 线性变换得到预测值\n",
        "        if self.training:\n",
        "            self.FX.retain_grad()  # 保留梯度以便后续反向传播\n",
        "        return self.FX\n",
        "\n",
        "\n",
        "    def gb_calc(self):\n",
        "        \"\"\"计算梯度并存储\n",
        "\n",
        "        调用基类方法获取梯度/二阶导数，用于参数更新\n",
        "        \"\"\"\n",
        "        if self.FX is None or self.FX.grad is None:\n",
        "            raise RuntimeError(\"必须先进行反向传播\")\n",
        "\n",
        "        self.g, self.h = self._get_grad_hess_FX()  # 获取梯度和Hessian矩阵\n",
        "\n",
        "\n",
        "    def gb_step(self):\n",
        "        \"\"\"执行梯度提升迭代步骤\n",
        "\n",
        "        使用缓存的梯度信息更新线性层参数，包含正则化和学习率控制\n",
        "        \"\"\"\n",
        "        if self.g is None and self.h is None:\n",
        "            self.gb_calc()  # 确保已计算梯度\n",
        "\n",
        "        with torch.no_grad():  # 禁用梯度计算以加速运算\n",
        "            # 构建设计矩阵X（包含偏置项）\n",
        "            if self.bias:\n",
        "                X = np.concatenate(\n",
        "                    [np.ones([self.input.shape[0], 1]), self.input],  # 添加全1列作为偏置项\n",
        "                    axis=1\n",
        "                )\n",
        "            else:\n",
        "                X = self.input\n",
        "\n",
        "            # 解决带L2正则化的线性回归问题\n",
        "            beta = ridge_regression(X, (self.g / self.h).detach().numpy(), self.lambd)\n",
        "\n",
        "            # 更新权重参数\n",
        "            self.linear.weight -= self.lr * torch.Tensor(beta[1:])  # 偏移参数\n",
        "            if self.bias:\n",
        "                self.linear.bias -= self.lr * torch.Tensor(beta[0])  # 偏置项\n",
        "\n",
        "\n",
        "def ridge_regression(X, y, lambd):\n",
        "    \"\"\"使用Cholesky分解求解岭回归\n",
        "\n",
        "    通过正规方程和数值稳定的方法快速计算带正则化的系数\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    # 构造正规方程矩阵A = X^TX + λI\n",
        "    A = X.T @ X + lambd * np.eye(d)\n",
        "    # 构造右侧向量c = X^Ty\n",
        "    c = X.T @ y\n",
        "\n",
        "    # Cholesky分解\n",
        "    L = cho_factor(A)\n",
        "    # 求解线性方程组Ly = c\n",
        "    beta = cho_solve(L, c)\n",
        "    return beta"
      ],
      "metadata": {
        "id": "pR_D7h6WBstR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#from gbnet.base import BaseGBModule\n",
        "\n",
        "\n",
        "class LGBModule(BaseGBModule):\n",
        "    \"\"\"封装LightGBM的PyTorch模块\n",
        "\n",
        "    该模块将LightGBM梯度提升算法集成到PyTorch框架中，支持训练和推理两种模式\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size,  # 训练批次大小\n",
        "        input_dim,  # 输入特征维度\n",
        "        output_dim,  # 输出预测维度\n",
        "        params={},  # LightGBM参数字典\n",
        "        min_hess=0  # Hessian最小阈值\n",
        "    ):\n",
        "        super(BaseGBModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.params = params\n",
        "        self.bst = None  # LightGBM模型实例\n",
        "\n",
        "        # 初始化预测张量（可训练参数）\n",
        "        self.FX = nn.Parameter(\n",
        "            torch.tensor(\n",
        "                np.zeros([batch_size, output_dim]),  # [batch_size,output_dim]形状的零张量\n",
        "                dtype=torch.float\n",
        "            )\n",
        "        )\n",
        "        self.train_dat = None  # 缓存的LightGBM数据集\n",
        "        self.min_hess = min_hess\n",
        "        self.grad = None  # 梯度缓存\n",
        "        self.hess = None  # Hessian缓存\n",
        "\n",
        "\n",
        "    def _set_train_dat(self, input_dataset: lgb.Dataset):\n",
        "        \"\"\"设置训练数据集并配置参数\n",
        "\n",
        "        确保训练时不输出日志信息\n",
        "        \"\"\"\n",
        "        if input_dataset.params is None:\n",
        "            input_dataset.params = {\"verbose\": -1}\n",
        "        else:\n",
        "            input_dataset.params.update({\"verbose\": -1})\n",
        "        input_dataset.free_raw_data = False  # 保留原始数据内存\n",
        "        self.train_dat = input_dataset\n",
        "\n",
        "\n",
        "    def _input_checking_setting(\n",
        "        self, input_dataset: Union[lgb.Dataset, np.ndarray, pd.DataFrame]\n",
        "    ):\n",
        "        \"\"\"统一输入格式并进行预处理\n",
        "\n",
        "        确保输入数据类型正确，并在训练模式时锁定数据集\n",
        "        \"\"\"\n",
        "        assert isinstance(input_dataset, (lgb.Dataset, np.ndarray, pd.DataFrame)), \"输入类型不支持\"\n",
        "\n",
        "        if self.training:\n",
        "            if self.train_dat is None:\n",
        "                # 创建LightGBM数据集\n",
        "                if isinstance(input_dataset, lgb.Dataset):\n",
        "                    self._set_train_dat(input_dataset)\n",
        "                else:\n",
        "                    self._set_train_dat(lgb.Dataset(input_dataset))\n",
        "            if self.bst is None:\n",
        "                return self.train_dat\n",
        "            # 训练期间禁止更换数据集\n",
        "            assert isinstance(input_dataset, lgb.Dataset), \"训练中不能更换数据集\"\n",
        "            input_dataset.free_raw_data = False\n",
        "            return self.train_dat\n",
        "        else:\n",
        "            # 推理模式返回原始数据\n",
        "            if isinstance(input_dataset, lgb.Dataset):\n",
        "                input_dataset.free_raw_data = False\n",
        "                input_dataset.construct()\n",
        "                return input_dataset.get_data()\n",
        "            return input_dataset\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_dataset: Union[lgb.Dataset, np.ndarray, pd.DataFrame],\n",
        "        return_tensor=True,\n",
        "    ):\n",
        "        \"\"\"前向传播计算预测值\n",
        "\n",
        "        支持训练和推理两种模式，自动维护预测状态\n",
        "        \"\"\"\n",
        "        input_dataset = self._input_checking_setting(input_dataset)\n",
        "\n",
        "        # 根据模式获取预测结果\n",
        "        if self.training:\n",
        "            if self.bst is not None:\n",
        "                # 使用现有模型预测\n",
        "                preds = self.bst._Booster__inner_predict(0).copy()  # 获取内部预测结果\n",
        "            else:\n",
        "                # 初始化为零矩阵\n",
        "                preds = np.zeros([self.batch_size, self.output_dim])\n",
        "        else:\n",
        "            if self.bst is not None:\n",
        "                # 推理模式使用完整模型预测\n",
        "                preds = self.bst.predict(input_dataset).copy()\n",
        "            else:\n",
        "                # 未训练时返回零张量\n",
        "                preds = np.zeros(\n",
        "                    [input_dataset.shape[0], self.output_dim], dtype=torch.float\n",
        "                )\n",
        "\n",
        "        # 更新训练模式的预测缓存\n",
        "        if self.training:\n",
        "            FX_detach = self.FX.detach()\n",
        "            FX_detach.copy_(\n",
        "                torch.tensor(\n",
        "                    preds.reshape([self.batch_size, self.output_dim]),  # 调整形状为[batch,output]\n",
        "                    dtype=torch.float\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # 返回结果\n",
        "        if return_tensor:\n",
        "            if self.training:\n",
        "                return self.FX\n",
        "            else:\n",
        "                return torch.tensor(\n",
        "                    preds.reshape([-1, self.output_dim]),  # 自动适应批量大小\n",
        "                    dtype=torch.float\n",
        "                )\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def gb_calc(self):\n",
        "        \"\"\"计算梯度并存储\n",
        "\n",
        "        调用基类方法获取梯度/二阶导数\n",
        "        \"\"\"\n",
        "        self.grad, self.hess = self._get_grad_hess_FX()\n",
        "\n",
        "\n",
        "    def gb_step(self):\n",
        "        \"\"\"执行梯度提升迭代步骤\n",
        "\n",
        "        使用计算的梯度更新LightGBM模型\n",
        "        \"\"\"\n",
        "        if self.grad is None and self.hess is None:\n",
        "            self.gb_calc()\n",
        "\n",
        "        # 创建自定义目标函数\n",
        "        obj = LightGBObj(self.grad, self.hess)\n",
        "        input_params = self.params.copy()\n",
        "        input_params.update(\n",
        "            {\n",
        "                \"objective\": obj,  # 自定义损失函数\n",
        "                \"num_class\": self.output_dim,  # 输出类别数\n",
        "                \"verbose\": -1,  # 关闭日志输出\n",
        "                \"verbosity\": -1  # 更高级别静默\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # 更新或创建模型\n",
        "        if self.bst is not None:\n",
        "            self.bst.update(train_set=self.train_dat, fobj=obj)\n",
        "        else:\n",
        "            # 训练新模型（仅需一轮提升）\n",
        "            self.bst = lgb.train(\n",
        "                params=input_params,\n",
        "                train_set=self.train_dat,\n",
        "                num_boost_round=1,  # 只训练一个基学习器\n",
        "                keep_training_booster=True  # 保留训练过程中的全部模型\n",
        "            )\n",
        "        self.grad = None\n",
        "        self.hess = None\n",
        "\n",
        "\n",
        "class LightGBObj:\n",
        "    \"\"\"LightGBM专用目标函数封装类\n",
        "\n",
        "    将PyTorch计算的梯度转换为LightGBM可接受的格式\n",
        "    \"\"\"\n",
        "    def __init__(self, grad, hess):\n",
        "        self.grad = grad.detach().numpy()  # 转换为NumPy数组\n",
        "        self.hess = hess.detach().numpy()\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        \"\"\"实现LightGBM的目标函数接口\n",
        "\n",
        "        返回梯度和Hessian矩阵\n",
        "        \"\"\"\n",
        "        if self.grad.shape[1] > 1:\n",
        "            return self.grad, self.hess\n",
        "        else:\n",
        "            return self.grad.flatten(), self.hess.flatten()"
      ],
      "metadata": {
        "id": "FZXX-fJfFzxj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "总结来说，forward方法在训练时返回当前模型的预测值，用于计算残差，而gb_step方法利用这些残差和Hessian信息，通过自定义目标函数指导LightGBM模型的更新，每轮迭代训练一个基学习器，逐步提升模型性能。这些步骤背后的数学原理涉及梯度提升框架中的目标函数优化、残差计算以及决策树的分裂策略。\n",
        "\n",
        " ### 1. **通过 `forward` 方法实现预测前向流的数学原理**\n",
        "\n",
        "`forward` 方法的核心功能是完成模型的前向传播，生成预测值。其数学原理与梯度提升框架（GBDT）的迭代预测过程密切相关：\n",
        "\n",
        "- **训练模式**：  \n",
        "  在训练阶段，`forward` 方法返回当前模型的预测值 `FX`（作为可训练参数），用于计算残差（即真实值与预测值的差值）。根据梯度提升框架，残差表示当前模型未能拟合的部分，后续迭代将基于此残差构建新模型。  \n",
        "  - **数学表达**：设第 $m$ 轮模型的预测值为 $F_m(x)$，则残差 $r_{mi} = y_i - F_m(x_i)$，其中 $y_i$ 为真实值，$x_i$ 为输入特征。`forward` 方法返回的 `FX` 即为 $F_m(x)$，用于计算残差 $r_{mi}$。\n",
        "\n",
        "- **推理模式**：  \n",
        "  在推理阶段，`forward` 方法直接调用 LightGBM 完整模型的预测结果，确保输出与实际应用一致。\n",
        "\n",
        "### 2. **利用 `gb_step` 方法完成梯度提升迭代的数学原理**\n",
        "\n",
        "`gb_step` 方法通过梯度提升框架实现模型迭代优化，其核心数学原理如下：\n",
        "\n",
        "- **梯度与 Hessian 的作用**：  \n",
        "  梯度提升的目标是最小化损失函数 $L(y, F(x))$，通过迭代添加新模型 $h_m(x)$ 逐步逼近最优函数 $F(x)$：  \n",
        "  $$\n",
        "  F_{m+1}(x) = F_m(x) + \\eta h_m(x)\n",
        "  $$  \n",
        "  其中，$\\eta$ 为学习率，$h_m(x)$ 需最小化目标函数的一阶导数（梯度）和二阶导数（Hessian）：\n",
        "\n",
        "  $$\n",
        "  h_m(x) = \\arg\\min_h \\left[ \\sum_{i=1}^n \\left( \\frac{\\partial L(y_i, F_m(x_i))}{\\partial F_m(x_i)} \\right) h(x_i) + \\frac{1}{2} \\sum_{i=1}^n \\left( \\frac{\\partial^2 L(y_i, F_m(x_i))}{\\partial (F_m(x_i))^2} \\right) h^2(x_i) \\right]\n",
        "  $$\n",
        "  \n",
        "  在代码中，`gb_calc` 方法计算当前预测值 `FX` 的梯度 `grad` 和 Hessian `hess`，传递给自定义目标函数 `LightGBObj`，用于指导 LightGBM 的分裂点选择。\n",
        "\n",
        "- **LightGBM 的优化策略**：  \n",
        "  LightGBM 通过直方图算法和 Leaf-wise 生长策略高效实现上述目标：  \n",
        "  - **直方图算法**：将连续特征离散化为固定区间（桶），减少分裂点计算量，将时间复杂度从 $O(n \\cdot d)$ 降至 $O(k \\cdot d)$（$k$ 为桶数）。  \n",
        "  - **Leaf-wise 策略**：每次选择增益最大的叶子节点进行分裂，相比 Level-wise 策略更高效，但需结合最大深度限制防止过拟合。\n",
        "\n",
        "- **迭代过程**：  \n",
        "  `gb_step` 方法每次仅训练一个基学习器（`num_boost_round=1`），通过 `bst.update` 更新模型。每轮迭代后，残差 $r_{mi}$ 被新模型 $h_m(x)$ 修正，逐步逼近真实值。\n",
        "\n",
        "### 总结\n",
        "\n",
        "- **`forward` 方法**：通过返回当前模型预测值（或残差），为梯度提升提供迭代所需的基础预测流。  \n",
        "- **`gb_step` 方法**：基于梯度与 Hessian 优化目标函数，利用 LightGBM 的高效算法（直方图、Leaf-wise）完成模型迭代，逐步提升预测性能。"
      ],
      "metadata": {
        "id": "A8xtHJ0VLNcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **关键结论**\n",
        "- **模型初始化**时通过 `lgb.train()` 创建全新模型，仅一轮迭代生成基础预测器。  \n",
        "- **模型更新**时通过 `update()` 方法追加弱学习器，逐步优化预测性能。  \n",
        "- **梯度计算**与 **Hessian 矩阵**通过 PyTorch 自动微分实现，指导 LightGBM 的特征分裂策略。  \n",
        "- **推理过程**直接调用 LightGBM 的预测接口，返回与训练模式兼容的结果。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 在LightGBM中，训练全新模型时仅一轮迭代而追加模型时需调用`update()`方法的原因与梯度提升框架的迭代机制有关：\n",
        "\n",
        "1. **全新模型初始化**  \n",
        "   当训练一个全新模型时，LightGBM首先会初始化一个基础预测值（如常数或简单模型），然后通过一轮迭代生成第一个弱学习器（决策树），用于拟合初始预测值与真实标签的残差。这一轮迭代是模型构建的起点，目的是生成首个纠正项。\n",
        "\n",
        "2. **追加弱学习器**  \n",
        "   当已有模型需要继续优化时，`update()`方法通过新一轮迭代生成新的弱学习器，用于拟合当前模型预测值与真实标签的残差。这种迭代方式逐步累积多个弱学习器，形成集成模型。每次调用`update()`相当于GBDT框架中的一轮迭代，通过梯度优化方向调整模型。\n",
        "\n",
        "这种设计符合梯度提升的核心思想：每轮迭代通过新弱学习器最小化损失函数，逐步逼近真实值。初始化时仅需一轮生成首个树，后续追加时需多次迭代以持续优化模型性能。"
      ],
      "metadata": {
        "id": "1FCwVxCqW2ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unittest import mock, TestCase\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "#from gbnet import lgbmodule as lgm\n",
        "\n",
        "# 测试基本损失函数计算与梯度更新流程\n",
        "def test_basic_loss():\n",
        "   # 创建LGBModule实例，参数包括5个叶子节点，3个树，1个输出，设置min_data_in_leaf为0\n",
        "   gbm = LGBModule(5, 3, 1, params={\"min_data_in_leaf\": 0})\n",
        "   # 定义均方误差损失函数\n",
        "   floss = torch.nn.MSELoss()\n",
        "\n",
        "   # 清空梯度\n",
        "   gbm.zero_grad()\n",
        "   # 设置随机种子保证结果可复现\n",
        "   np.random.seed(11010)\n",
        "   # 生成5行3列的随机数据作为输入数据集\n",
        "   input_dataset = lgb.Dataset(np.random.random([5, 3]))\n",
        "   # 前向传播计算预测值\n",
        "   preds = gbm(input_dataset)\n",
        "   # 计算预测值与目标值[1,2,3,4,5]的均方误差损失\n",
        "   loss = floss(preds.flatten(), torch.Tensor(np.array([1, 2, 3, 4, 5])).flatten())\n",
        "\n",
        "   # 执行反向传播，创建计算图以便后续计算高阶导数\n",
        "   loss.backward(create_graph=True)\n",
        "\n",
        "   # 使用mock模拟LightGBM的LightGBObj和lgb.train函数\n",
        "   m_obj = mock.MagicMock(side_effect=LightGBObj)\n",
        "   m_train = mock.MagicMock(side_effect=lgb.train)\n",
        "   # 在上下文管理器中执行梯度提升步骤，此时会调用模拟的LightGBObj和lgb.train\n",
        "   with (\n",
        "       #mock.patch(\"gbnet.lgbmodule.LightGBObj\", m_obj),\n",
        "       mock.patch(\"__main__.LightGBObj\", m_obj),\n",
        "       mock.patch(\"lightgbm.train\", m_train),\n",
        "   ):\n",
        "       gbm.gb_step()\n",
        "\n",
        "   # 断言模拟的LightGBObj最后一次调用的梯度参数与理论值误差小于1e-8\n",
        "   assert (\n",
        "       np.max(\n",
        "           np.abs(\n",
        "               m_obj.call_args_list[-1].args[0].detach().numpy()\n",
        "               - np.array([-2, -4, -6, -8, -10]).reshape([-1, 1])\n",
        "           )\n",
        "       )\n",
        "       < 1e-8\n",
        "   )\n",
        "\n",
        "   # 断言模拟的LightGBObj最后一次调用的hessian参数与理论值误差小于1e-8\n",
        "   assert (\n",
        "       np.max(\n",
        "           np.abs(\n",
        "               m_obj.call_args_list[-1].args[1].detach().numpy()\n",
        "               - np.array([2, 2, 2, 2, 2]).reshape([-1, 1])\n",
        "           )\n",
        "       )\n",
        "       < 1e-8\n",
        "   )\n",
        "\n",
        "   # 断言lgb.train被调用了一次\n",
        "   m_train.assert_called_once()\n",
        "\n",
        "# 测试LightGBObj的梯度计算功能\n",
        "def test_LightGBObj():\n",
        "   # 生成随机梯度矩阵（20行10列）和hessian矩阵（20行10列）\n",
        "   grad = torch.tensor(np.random.random([20, 10]))\n",
        "   hess = torch.tensor(np.random.random([20, 10]))\n",
        "\n",
        "   # 创建LightGBObj实例\n",
        "   obj = LightGBObj(grad, hess)\n",
        "\n",
        "   # 调用obj(1,2)计算梯度与hessian\n",
        "   ograd, ohess = obj(1, 2)\n",
        "   # 断言计算出的梯度与原始梯度完全一致\n",
        "   assert (\n",
        "       np.max(np.abs(ograd - grad.detach().numpy())) == 0\n",
        "   ), \"LightGBObj grad does not match instantiation\"\n",
        "   # 断言计算出的hessian与原始hessian完全一致\n",
        "   assert (\n",
        "       np.max(np.abs(ohess - hess.detach().numpy())) == 0\n",
        "   ), \"LightGBObj hess does not match instantiation\"\n",
        "\n",
        "# 测试LGBModule的输入检查功能\n",
        "class TestLGBModule(TestCase):\n",
        "   # 测试输入为lgb.Dataset且训练模式为True时的行为\n",
        "   def test_input_is_dataset_training_true_train_dat_none(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 生成随机数据\n",
        "       data = np.random.rand(100, 10)\n",
        "       # 转换为lgb.Dataset\n",
        "       dataset = lgb.Dataset(data)\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(dataset)\n",
        "       # 断言返回结果与train_dat相同且类型正确\n",
        "       self.assertIs(result, module.train_dat)\n",
        "       self.assertIsInstance(result, lgb.Dataset)\n",
        "\n",
        "   # 测试输入为np.ndarray且训练模式为True时的行为\n",
        "   def test_input_is_ndarray_training_true_train_dat_none(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 生成随机数据\n",
        "       data = np.random.rand(100, 10)\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(data)\n",
        "       # 断言返回结果与train_dat相同且类型正确\n",
        "       self.assertIs(result, module.train_dat)\n",
        "       self.assertIsInstance(result, lgb.Dataset)\n",
        "\n",
        "   # 测试输入为pd.DataFrame且训练模式为True时的行为\n",
        "   def test_input_is_dataframe_training_true_train_dat_none(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 生成随机数据\n",
        "       data = pd.DataFrame(np.random.rand(100, 10))\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(data)\n",
        "       # 断言返回结果与train_dat相同且类型正确\n",
        "       self.assertIs(result, module.train_dat)\n",
        "       self.assertIsInstance(result, lgb.Dataset)\n",
        "\n",
        "   # 测试输入为lgb.Dataset且训练模式为True时，当train_dat已设置且行数相同的行为\n",
        "   def test_input_is_dataset_training_true_train_dat_set_same_nrows(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 生成随机数据\n",
        "       data = np.random.rand(100, 10)\n",
        "       # 转换为lgb.Dataset\n",
        "       dataset = lgb.Dataset(data)\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(dataset)\n",
        "       # 断言返回结果与train_dat相同\n",
        "       self.assertIs(result, module.train_dat)\n",
        "       self.assertIs(result, dataset)\n",
        "\n",
        "   # 测试输入为lgb.Dataset且训练模式为False时的行为\n",
        "   def test_input_is_dataset_training_false(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 切换到评估模式\n",
        "       module.eval()\n",
        "       # 生成随机数据\n",
        "       data = np.random.rand(100, 10)\n",
        "       # 转换为lgb.Dataset\n",
        "       dataset = lgb.Dataset(data)\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(dataset)\n",
        "       # 断言返回原始数据\n",
        "       self.assertTrue(np.array_equal(result, data))\n",
        "       # 断言数据集不释放原始数据\n",
        "       self.assertFalse(dataset.free_raw_data)\n",
        "\n",
        "   # 测试输入为np.ndarray且训练模式为False时的行为\n",
        "   def test_input_is_ndarray_training_false(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(100, 10, 1)\n",
        "       # 切换到评估模式\n",
        "       module.eval()\n",
        "       # 生成随机数据\n",
        "       data = np.random.rand(100, 10)\n",
        "       # 调用输入检查方法\n",
        "       result = module._input_checking_setting(data)\n",
        "       # 断言返回原始数据\n",
        "       self.assertIs(result, data)\n",
        "\n",
        "   # 测试输入为无效类型时的行为\n",
        "   def test_input_invalid_type(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(10, 5, 1)\n",
        "       # 生成无效输入数据\n",
        "       data = [1, 2, 3]\n",
        "       # 调用输入检查方法，预期抛出AssertionError\n",
        "       with self.assertRaises(AssertionError):\n",
        "           module._input_checking_setting(data)\n",
        "\n",
        "   # 测试当self.bst为None时的行为\n",
        "   def test_if_bst_is_none(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(10, 5, 1)\n",
        "       # 设置train_dat\n",
        "       arr = np.random.rand(10, 5)\n",
        "       module._set_train_dat(lgb.Dataset(arr))\n",
        "       # 调用输入检查方法，预期返回train_dat\n",
        "       result = module._input_checking_setting(np.random.rand(10, 5))\n",
        "       self.assertIs(result, module.train_dat)\n",
        "\n",
        "   # 测试输入为lgb.Dataset且self.bst不为None时的行为\n",
        "   def test_raises_input_changed_lgb_dataset(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(10, 5, 1)\n",
        "       # 设置train_dat\n",
        "       arr = np.random.rand(10, 5)\n",
        "       initial_dataset = lgb.Dataset(arr)\n",
        "       initial_dataset.construct()\n",
        "       module._set_train_dat(initial_dataset)\n",
        "       # 设置bst为非None对象\n",
        "       module.bst = object()\n",
        "       # 调用输入检查方法，预期抛出AssertionError\n",
        "       with self.assertRaises(AssertionError):\n",
        "           module._input_checking_setting(lgb.Dataset(np.random.random([10, 5])))\n",
        "\n",
        "   # 测试输入为ndarray且self.bst不为None时的行为\n",
        "   def test_raises_input_changed_ndarray(self):\n",
        "       # 创建LGBModule实例\n",
        "       module = LGBModule(10, 5, 1)\n",
        "       # 设置train_dat\n",
        "       arr = np.random.rand(10, 5)\n",
        "       initial_dataset = lgb.Dataset(arr)\n",
        "       initial_dataset.construct()\n",
        "       module._set_train_dat(initial_dataset)\n",
        "       # 设置bst为非None对象\n",
        "       module.bst = object()\n",
        "       # 调用输入检查方法，预期抛出AssertionError\n",
        "       with self.assertRaises(AssertionError):\n",
        "           module._input_checking_setting(np.random.random([11, 5]))"
      ],
      "metadata": {
        "id": "4QTKpGexYCsb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_basic_loss()\n",
        "test_LightGBObj()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFRTFYMzaawk",
        "outputId": "f6a2d064-3f5c-47b2-cc0f-4459f69bdcc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 `test_basic_loss` 测试用例中，`mock` 模块的核心作用是通过**模拟 LightGBM 的关键函数**（`LightGBObj` 和 `lgb.train`），实现对 `LGBModule` 类训练过程的**纯单元测试**。以下是具体实现的详细解析：\n",
        "\n",
        "---\n",
        "\n",
        "### **1. 为什么要使用 mock？**\n",
        "- **隔离依赖**：  \n",
        "  `LGBModule` 的训练依赖于外部库 `lightgbm`，直接测试会导致以下问题：\n",
        "  1. 测试结果可能受 LightGBM 版本或配置的影响；\n",
        "  2. 实际训练过程耗时且不可控（如树的生长、正则化项计算等）；\n",
        "  3. 难以直接验证内部梯度传递的逻辑是否正确。\n",
        "\n",
        "- **精确控制测试流程**：  \n",
        "  通过模拟 `LightGBObj` 和 `lgb.train`，可以：\n",
        "  1. **截断**真实的梯度提升过程，仅关注 `LGBModule` 的输入输出是否符合预期；\n",
        "  2. **伪造**梯度/二阶导数值，验证模型参数更新逻辑；\n",
        "  3. **统计**函数调用次数和参数，确保代码符合设计意图。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. mock 的具体用法**\n",
        "#### **(1) 模拟 `LightGBObj` 和 `lgb.train`**\n",
        "```python\n",
        "m_obj = mock.MagicMock(side_effect=lgm.LightGBObj)\n",
        "m_train = mock.MagicMock(side_effect=lgb.train)\n",
        "```\n",
        "- **`MagicMock`**：  \n",
        "  创建一个通用的模拟对象，自动模拟任意未定义的方法和属性。\n",
        "- **`side_effect`**：  \n",
        "  指定当模拟函数被调用时，返回预先定义的对象（如 `lgm.LightGBObj`）或函数（如 `lgb.train`）。\n",
        "\n",
        "#### **(2) 上下文管理器中的 `patch`**\n",
        "```python\n",
        "with (\n",
        "    mock.patch(\"gbnet.lgbmodule.LightGBObj\", m_obj),\n",
        "    mock.patch(\"lightgbm.train\", m_train),\n",
        "):\n",
        "    gbm.gb_step()\n",
        "```\n",
        "- **`patch` 装饰器**：  \n",
        "  临时替换目标函数/类（如 `LightGBObj` 和 `lgb.train`）为模拟对象。\n",
        "- **作用域控制**：  \n",
        "  仅在 `with` 块内生效，测试结束后自动恢复原函数。\n",
        "\n",
        "#### **(3) 验证模拟调用**\n",
        "```python\n",
        "# 断言 LightGBObj 最后一次调用的梯度参数是否正确\n",
        "assert np.max(abs(m_obj.call_args_list[-1].args[0] - expected_grad)) < 1e-8\n",
        "\n",
        "# 断言 lgb.train 是否被调用了一次\n",
        "m_train.assert_called_once()\n",
        "```\n",
        "- **`call_args_list`**：  \n",
        "  记录模拟对象的所有调用参数列表。\n",
        "- **`assert_called_once()`**：  \n",
        "  验证 `lgb.train` 是否被调用且仅调用一次。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. 在上下文管理器中执行梯度提升步骤**\n",
        "#### **(1) 执行 `gb_step()` 的逻辑**\n",
        "```python\n",
        "gbm.gb_step()\n",
        "```\n",
        "- **`gb_step()`** 是 `LGBModule` 的核心训练方法，包含以下步骤：\n",
        "  1. 调用 `gb_calc()` 计算梯度 (`self.grad`) 和 Hessian (`self.hess`)；\n",
        "  2. 将梯度转换为 LightGBM 兼容格式 (`LightGBObj`)；\n",
        "  3. 根据当前模型状态（是否存在 `self.bst`）选择更新现有模型或创建新模型。\n",
        "\n",
        "#### **(2) 上下文管理器的作用**\n",
        "- **隔离模拟环境**：  \n",
        "  在 `gb_step()` 执行期间，所有对 `LightGBObj` 和 `lgb.train` 的调用都被替换为模拟对象。\n",
        "- **捕获调用细节**：  \n",
        "  模拟对象记录了以下关键信息：\n",
        "  - `LightGBObj` 接收到的梯度 (`args[0]`) 和 Hessian (`args[1]`)；\n",
        "  - `lgb.train` 是否被调用及其参数。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. 测试的核心验证点**\n",
        "#### **(1) 梯度计算的正确性**\n",
        "```python\n",
        "expected_grad = np.array([-2, -4, -6, -8, -10]).reshape([-1, 1])\n",
        "```\n",
        "- **理论值**：  \n",
        "  通过反向传播计算的梯度应为 `[y_true - preds] * learning_rate`（此处简化为固定值）。\n",
        "- **断言逻辑**：  \n",
        "  验证 `LightGBObj` 接收到的梯度与理论值的误差小于 `1e-8`，确保 `LGBModule` 的梯度计算无误。\n",
        "\n",
        "#### **(2) 模型更新的逻辑**\n",
        "```python\n",
        "m_train.assert_called_once()\n",
        "```\n",
        "- **预期行为**：  \n",
        "  在首次训练时（`self.bst` 为空），应调用 `lgb.train()` 创建新模型；后续迭代调用 `update()` 追加弱学习器。\n",
        "- **断言逻辑**：  \n",
        "  确保 `lgb.train()` 仅被调用一次，符合单轮迭代的设定。\n",
        "\n",
        "---\n",
        "\n",
        "### **5. 为什么需要 `create_graph=True`？**\n",
        "```python\n",
        "loss.backward(create_graph=True)\n",
        "```\n",
        "- **目的**：  \n",
        "  启用 PyTorch 的高阶导数计算能力，以获取 Hessian 矩阵（二阶导数）。\n",
        "- **必要性**：  \n",
        "  LightGBM 的目标函数优化依赖梯度和 Hessian（如正则化项 `λΩ(h)`），需通过二阶泰勒展开近似损失函数。\n",
        "\n",
        "---\n",
        "\n",
        "### **总结**\n",
        "通过 `mock` 模拟 `LightGBObj` 和 `lgb.train`，测试实现了以下目标：\n",
        "1. **隔离外部依赖**：避免直接调用 LightGBM 库，使测试独立于库版本和配置。\n",
        "2. **精确验证逻辑**：通过伪造梯度/二阶导数值，检查 `LGBModule` 的参数传递是否正确。\n",
        "3. **控制训练流程**：确保 `gb_step()` 方法的行为符合设计（如首次创建模型或追加弱学习器）。\n",
        "4. **加速测试执行**：跳过真实的树生长和迭代过程，大幅缩短测试时间。\n",
        "\n",
        "这种基于 `mock` 的单元测试模式是深度学习框架集成测试中的常见实践，尤其在混合精度训练和复杂梯度计算场景中至关重要。"
      ],
      "metadata": {
        "id": "M9QZ3WnO2m4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "from typing import Union\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import xgboost as xgb\n",
        "from torch import nn\n",
        "\n",
        "#from gbnet.base import BaseGBModule\n",
        "\n",
        "\n",
        "class XGBModule(BaseGBModule):\n",
        "    \"\"\"XGBoost模块封装类，将XGBoost梯度提升集成到PyTorch框架中\n",
        "\n",
        "    实现XGBoost模型的训练和推理功能，维护模型状态并与PyTorch计算图深度集成\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, input_dim, output_dim, params={}, min_hess=0):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size  # 批次大小\n",
        "        self.input_dim = input_dim  # 输入特征维度\n",
        "        self.output_dim = output_dim  # 输出预测维度\n",
        "\n",
        "        self.params = params  # XGBoost参数字典\n",
        "        self.params[\"objective\"] = \"reg:squarederror\"  # 设置损失函数为均方误差\n",
        "        self.params[\"base_score\"] = 0  # 初始化基学习器预测值为0\n",
        "        self.n_completed_boost_rounds = 0  # 完成的提升轮数\n",
        "        self.min_hess = min_hess  # Hessian最小阈值\n",
        "\n",
        "        # 初始化XGBoost模型，使用零矩阵作为初始数据\n",
        "        init_matrix = np.zeros([batch_size, input_dim])\n",
        "        self.bst = xgb.train(\n",
        "            self.params,\n",
        "            xgb.DMatrix(init_matrix, label=np.zeros(batch_size * output_dim)),\n",
        "            num_boost_round=0,\n",
        "        )\n",
        "        self.n_completed_boost_rounds = 0\n",
        "        self.dtrain = None  # 训练数据集缓存\n",
        "        self.training_n = None  # 训练样本数量\n",
        "\n",
        "        # 初始化预测张量（可训练参数）\n",
        "        self.FX = nn.Parameter(\n",
        "            torch.tensor(\n",
        "                np.zeros([batch_size, output_dim]),\n",
        "                dtype=torch.float,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    def _check_training_data(self):\n",
        "        \"\"\"检查训练数据权重设置\"\"\"\n",
        "        if self.dtrain.get_weight().shape[0] > 0:\n",
        "            warnings.warn(\n",
        "                \"当输入数据包含权重时，建议通过损失函数而非DMatrix直接设置权重\"\n",
        "            )\n",
        "\n",
        "\n",
        "    def _input_checking_setting(\n",
        "        self, input_data: Union[xgb.DMatrix, pd.DataFrame, np.ndarray]\n",
        "    ):\n",
        "        \"\"\"统一输入格式并进行预处理\n",
        "\n",
        "        确保训练模式下输入数据一致性，推理模式直接返回原始数据\n",
        "        \"\"\"\n",
        "        assert isinstance(input_data, (xgb.DMatrix, pd.DataFrame, np.ndarray))\n",
        "\n",
        "        if self.training:\n",
        "            if self.dtrain is None:\n",
        "                # 创建训练数据集\n",
        "                if isinstance(input_data, xgb.DMatrix):\n",
        "                    input_data.set_label(np.zeros(self.batch_size * self.output_dim))\n",
        "                    self.dtrain = input_data\n",
        "                    self.training_n = input_data.num_row()\n",
        "                    self._check_training_data()\n",
        "                else:\n",
        "                    self.dtrain = xgb.DMatrix(\n",
        "                        input_data, label=np.zeros(self.batch_size * self.output_dim)\n",
        "                    )\n",
        "                    self.training_n = input_data.shape[0]\n",
        "            # 检查输入数据一致性\n",
        "            compare_n = (\n",
        "                input_data.num_row() if isinstance(input_data, xgb.DMatrix) else input_data.shape[0]\n",
        "            )\n",
        "            assert (\n",
        "                compare_n == self.training_n\n",
        "            ), \"训练期间禁止更换数据集\"\n",
        "            return self.dtrain\n",
        "        else:\n",
        "            # 推理模式返回原始数据格式\n",
        "            return (\n",
        "                input_data\n",
        "                if isinstance(input_data, xgb.DMatrix)\n",
        "                else xgb.DMatrix(input_data)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_data: Union[xgb.DMatrix, np.ndarray, pd.DataFrame],\n",
        "        return_tensor: bool = True,\n",
        "    ):\n",
        "        \"\"\"前向传播计算预测值\n",
        "\n",
        "        支持训练和推理两种模式，自动维护预测状态\n",
        "        \"\"\"\n",
        "        input_data = self._input_checking_setting(input_data)\n",
        "        preds = self.bst.predict(input_data)\n",
        "\n",
        "        if self.training:\n",
        "            FX_detach = self.FX.detach()\n",
        "            FX_detach.copy_(\n",
        "                torch.tensor(\n",
        "                    preds.reshape([self.batch_size, self.output_dim]),  # 调整形状为[batch,output]\n",
        "                    dtype=torch.float\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if return_tensor:\n",
        "            if self.training:\n",
        "                return self.FX\n",
        "            else:\n",
        "                return torch.tensor(\n",
        "                    preds.reshape([-1, self.output_dim]),  # 自动适应批量大小\n",
        "                    dtype=torch.float\n",
        "                )\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def gb_calc(self):\n",
        "        \"\"\"计算梯度并存储\"\"\"\n",
        "        self.grad, self.hess = self._get_grad_hess_FX()\n",
        "\n",
        "\n",
        "    def gb_step(self):\n",
        "        \"\"\"执行梯度提升迭代步骤\n",
        "\n",
        "        1. 计算当前梯度和Hessian\n",
        "        2. 更新XGBoost模型\n",
        "        \"\"\"\n",
        "        if self.grad is None and self.hess is None:\n",
        "            self.gb_calc()\n",
        "\n",
        "        self._gb_step_grad_hess(self.grad, self.hess)\n",
        "        self.grad = None\n",
        "        self.hess = None\n",
        "\n",
        "\n",
        "    def _gb_step_grad_hess(self, grad, hess):\n",
        "        obj = XGBObj(grad, hess)\n",
        "        g, h = obj(np.zeros([self.batch_size, self.output_dim]), None)\n",
        "\n",
        "        # 根据XGBoost版本调用对应训练方法\n",
        "        if xgb.__version__ <= \"2.0.3\":\n",
        "            self.bst_boost(\n",
        "                self.dtrain,\n",
        "                self.n_completed_boost_rounds + 1,\n",
        "                g,\n",
        "                h,\n",
        "            )\n",
        "        else:\n",
        "            #self.bst_boost(\n",
        "            self.bst.boost(\n",
        "                self.dtrain,\n",
        "                self.n_completed_boost_rounds + 1,\n",
        "                g,\n",
        "                h,\n",
        "            )\n",
        "        self.n_completed_boost_rounds += 1\n",
        "\n",
        "\n",
        "class XGBObj:\n",
        "    \"\"\"XGBoost专用目标函数封装类\n",
        "\n",
        "    将PyTorch计算的梯度转换为XGBoost可接受的格式\n",
        "    \"\"\"\n",
        "    def __init__(self, grad, hess):\n",
        "        self.grad = grad\n",
        "        self.hess = hess\n",
        "\n",
        "    def __call__(self, preds, dtrain):\n",
        "        if len(preds.shape) == 2:\n",
        "            M = preds.shape[0]\n",
        "            N = preds.shape[1]\n",
        "        else:\n",
        "            M = preds.shape[0]\n",
        "            N = 1\n",
        "\n",
        "        # 根据XGBoost版本调整输出格式\n",
        "        if xgb.__version__ >= \"2.1.0\":\n",
        "            g = self.grad.detach().numpy().reshape([M, N])\n",
        "            h = self.hess.detach().numpy().reshape([M, N])\n",
        "        else:\n",
        "            g = self.grad.detach().numpy().reshape([M * N, 1])\n",
        "            h = self.hess.detach().numpy().reshape([M * N, 1])\n",
        "\n",
        "        return g, h"
      ],
      "metadata": {
        "id": "vS7OWTUY2p_9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "print(xgb.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO8jMBcVeA4h",
        "outputId": "1158ce2d-b542-4e61-cb8b-62352ad77122"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入必要的库\n",
        "from unittest import mock, TestCase  # 单元测试框架[1](@ref)\n",
        "import numpy as np  # 数组操作库\n",
        "import torch  # 深度学习框架\n",
        "import xgboost as xgb  # XGBoost库\n",
        "\n",
        "# 测试基本损失函数计算\n",
        "def test_basic_loss():\n",
        "   # 初始化XGBoost模块（5个特征，3个树，1个输出）\n",
        "   gbm = XGBModule(5, 3, 1)\n",
        "   # 定义均方误差损失函数\n",
        "   floss = torch.nn.MSELoss()\n",
        "\n",
        "   # 清空梯度缓存\n",
        "   gbm.zero_grad()\n",
        "   # 设置随机种子保证结果可复现\n",
        "   np.random.seed(11010)\n",
        "   # 生成随机输入数据并转换为DMatrix格式\n",
        "   input_dmatrix = xgb.DMatrix(np.random.random([5, 3]))\n",
        "   # 前向传播计算预测值\n",
        "   preds = gbm(input_dmatrix)\n",
        "   # 计算预测值与真实值（1-5）的MSE损失\n",
        "   loss = floss(preds.flatten(), torch.Tensor(np.array([1, 2, 3, 4, 5])).flatten())\n",
        "\n",
        "   # 反向传播计算梯度（create_graph=True保留二阶导数信息）\n",
        "   loss.backward(create_graph=True)\n",
        "\n",
        "   # 使用mock模拟XGBObj类和boost方法\n",
        "   with (\n",
        "       mock.patch(\"__main__.XGBObj\", side_effect=XGBObj) as m_obj,\n",
        "       mock.patch.object(gbm.bst, \"boost\", side_effect=gbm.bst.boost) as m_boost,\n",
        "   ):\n",
        "       # 执行一次梯度提升步骤\n",
        "       gbm.gb_step()\n",
        "\n",
        "   # 验证XGBObj的梯度计算结果是否符合预期（-2, -4, -6, -8, -10）\n",
        "   assert np.all(\n",
        "       np.isclose(\n",
        "           m_obj.call_args_list[-1].args[0].detach().numpy(),\n",
        "           np.array([-2, -4, -6, -8, -10]).reshape([-1, 1]),\n",
        "       )\n",
        "   )\n",
        "   # 验证hessian矩阵是否为常数2\n",
        "   assert np.all(\n",
        "       np.isclose(\n",
        "           m_obj.call_args_list[-1].args[1].detach().numpy(),\n",
        "           np.array([2, 2, 2, 2, 2]).reshape([-1, 1]),\n",
        "       )\n",
        "   )\n",
        "\n",
        "   # 验证boost方法被正确调用一次\n",
        "   m_boost.assert_called_once()\n",
        "\n",
        "# 测试XGBObj在不同XGBoost版本下的行为\n",
        "def test_XGBObj():\n",
        "   # 生成随机梯度矩阵和hessian矩阵\n",
        "   grad = torch.tensor(np.random.random([20, 10]))\n",
        "   hess = torch.tensor(np.random.random([20, 10]))\n",
        "\n",
        "   # 初始化XGBObj对象\n",
        "   obj = XGBObj(grad, hess)\n",
        "\n",
        "   # 测试XGBoost 2.0.0版本行为\n",
        "   with mock.patch(\"xgboost.__version__\", new=\"2.0.0\"):\n",
        "       ograd, ohess = obj(grad, hess)\n",
        "       # 验证梯度矩阵形状调整为列向量\n",
        "       assert np.all(np.isclose(ograd, grad.detach().numpy().reshape([-1, 1])))\n",
        "       # 验证hessian矩阵形状调整为列向量\n",
        "       assert np.all(np.isclose(ohess, hess.detach().numpy().reshape([-1, 1])))\n",
        "\n",
        "   # 测试XGBoost 2.1.0版本行为\n",
        "   with mock.patch(\"xgboost.__version__\", new=\"2.1.0\"):\n",
        "       ograd, ohess = obj(grad, hess)\n",
        "       # 验证梯度矩阵保持原始形状\n",
        "       assert np.all(np.isclose(ograd, grad.detach().numpy()))\n",
        "       # 验证hessian矩阵保持原始形状\n",
        "       assert np.all(np.isclose(ohess, hess.detach().numpy()))\n",
        "\n",
        "# 测试输入数据类型检查逻辑\n",
        "class TestInputChecking(TestCase):\n",
        "   # 测试DMatrix输入且训练模式开启的情况\n",
        "   def test_input_is_dmatrix_training_true_dtrain_none(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       data = np.random.rand(10, 5)\n",
        "       dmatrix = xgb.DMatrix(data)\n",
        "       result = module._input_checking_setting(dmatrix)\n",
        "       # 验证dtrain属性被正确设置为输入DMatrix\n",
        "       self.assertIs(result, module.dtrain)\n",
        "       # 验证返回结果与输入DMatrix是同一对象\n",
        "       self.assertIs(result, dmatrix)\n",
        "       # 验证标签矩阵全零且形状正确\n",
        "       np.testing.assert_array_equal(\n",
        "           result.get_label(), np.zeros(module.batch_size * module.output_dim)\n",
        "       )\n",
        "       # 验证训练样本数量正确\n",
        "       self.assertEqual(module.training_n, dmatrix.num_row())\n",
        "\n",
        "   # 测试ndarray输入且训练模式开启的情况\n",
        "   def test_input_is_ndarray_training_true_dtrain_none(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       data = np.random.rand(10, 5)\n",
        "       result = module._input_checking_setting(data)\n",
        "       # 验证dtrain属性被正确转换为DMatrix\n",
        "       self.assertIs(result, module.dtrain)\n",
        "       # 验证返回结果为DMatrix类型\n",
        "       self.assertIsInstance(result, xgb.DMatrix)\n",
        "       # 验证标签矩阵全零且形状正确\n",
        "       np.testing.assert_array_equal(\n",
        "           result.get_label(), np.zeros(module.batch_size * module.output_dim)\n",
        "       )\n",
        "       # 验证训练样本数量正确\n",
        "       self.assertEqual(module.training_n, data.shape[0])\n",
        "\n",
        "   # 测试DMatrix输入且训练模式开启，dtrain已设置且行数相同的情况\n",
        "   def test_input_is_dmatrix_training_true_dtrain_set_same_nrows(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       data = np.random.rand(10, 5)\n",
        "       dmatrix = xgb.DMatrix(data)\n",
        "       result = module._input_checking_setting(dmatrix)\n",
        "       # 验证返回结果与已设置的dtrain是同一对象\n",
        "       self.assertIs(result, module.dtrain)\n",
        "       # 验证返回结果与输入DMatrix是同一对象\n",
        "       self.assertIs(result, dmatrix)\n",
        "\n",
        "   # 测试DMatrix输入且训练模式开启，dtrain已设置但行数不同的情况\n",
        "   def test_input_is_dmatrix_training_true_dtrain_set_different_nrows(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       data1 = np.random.rand(10, 5)\n",
        "       module(data1)\n",
        "       data2 = np.random.rand(5, 5)\n",
        "       # 验证输入数据行数不同时抛出异常\n",
        "       with self.assertRaises(AssertionError) as context:\n",
        "           module(data2)\n",
        "       # 验证异常信息包含特定错误提示\n",
        "       self.assertIn(\n",
        "           \"训练过程中不支持更改数据集\",\n",
        "           str(context.exception),\n",
        "       )\n",
        "\n",
        "   # 测试DMatrix输入且训练模式关闭的情况\n",
        "   def test_input_is_dmatrix_training_false(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       module(np.random.rand(10, 5))\n",
        "       module.eval()\n",
        "       data = np.random.rand(10, 5)\n",
        "       dmatrix = xgb.DMatrix(data)\n",
        "       result = module._input_checking_setting(dmatrix)\n",
        "       # 验证返回结果为输入的DMatrix\n",
        "       self.assertIs(result, dmatrix)\n",
        "\n",
        "   # 测试ndarray输入且训练模式关闭的情况\n",
        "   def test_input_is_ndarray_training_false(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       module(np.random.rand(10, 5))\n",
        "       module.eval()\n",
        "       data = np.random.rand(10, 5)\n",
        "       result = module._input_checking_setting(data)\n",
        "       # 验证返回结果为DMatrix类型\n",
        "       self.assertIsInstance(result, xgb.DMatrix)\n",
        "       # 验证DMatrix行数与输入ndarray一致\n",
        "       self.assertEqual(result.num_row(), data.shape[0])\n",
        "\n",
        "   # 测试无效输入类型的情况\n",
        "   def test_input_invalid_type(self):\n",
        "       module = XGBModule(10, 5, 3)\n",
        "       data = [1, 2, 3]  # 非DMatrix或ndarray类型\n",
        "       # 验证输入无效类型时抛出异常\n",
        "       with self.assertRaises(AssertionError):\n",
        "           module._input_checking_setting(data)"
      ],
      "metadata": {
        "id": "6Ivyg2NHaHYT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_XGBObj()"
      ],
      "metadata": {
        "id": "1XfCx7rhaiEz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_basic_loss()"
      ],
      "metadata": {
        "id": "ooLpbGYodkcU"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}