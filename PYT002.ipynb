{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/PYT002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwhl8SU0blzm"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HrHGcWdblzo"
      },
      "source": [
        "\n",
        "\n",
        "张量\n",
        "=======\n",
        "\n",
        "张量是一种专门的数据结构，它们与数组和矩阵非常相似。在PyTorch中，我们使用张量来编码模型的输入和输出，以及模型的参数。\n",
        "\n",
        "张量类似于NumPy的ndarrays，只不过张量可以在GPU或其他硬件加速器上运行。事实上，张量和NumPy数组往往可以共享同一段底层内存，这样就无需复制数据。张量还针对自动求微分进行了优化（我们将在后面的Autograd部分进一步了解这一点）。如果你已经熟悉ndarrays，那么使用Tensor API就会感觉很自在。如果不熟悉，也可以跟着继续学习！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67vLSaQMblzp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP2IUTcjblzq"
      },
      "source": [
        "初始化张量\n",
        "=====================\n",
        "\n",
        "\n",
        "张量可以通过多种方式初始化。以下是一些示例：\n",
        "\n",
        "\n",
        "**直接从数据初始化**\n",
        "\n",
        "张量可以直接从数据创建。数据类型会自动推断。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BR4D-2wblzq"
      },
      "outputs": [],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh3d4aVBblzq"
      },
      "source": [
        "**从NumPy数组**\n",
        "\n",
        "张量可以从NumPy数组创建（反之亦然）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb95i6Prblzq"
      },
      "outputs": [],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bd9DTE9blzr"
      },
      "source": [
        "**从另一个张量**\n",
        "\n",
        "新张量保留了参数张量的属性（形状、数据类型），除非显式覆盖。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJVK5R5dblzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae94c35-d3cf-42e2-f7a4-7b114a12c8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.6681, 0.6524],\n",
            "        [0.1420, 0.6029]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDyYRhmoblzr"
      },
      "source": [
        "**使用随机值或常量值**\n",
        "\n",
        "shape是一个张量维度的元组。在下面的函数中，它决定了输出张量的维度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhDY6fhDblzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1070658-ea59-4e7a-c0c2-f9d8b85d346e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.8797, 0.4959, 0.1810],\n",
            "        [0.9698, 0.3987, 0.6632]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubRC-EIjblzs"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rCX4f0qblzs"
      },
      "source": [
        "张量的属性\n",
        "======================\n",
        "\n",
        "张量的属性描述了它们的形状、数据类型以及它们所存储的设备。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqi77tlNblzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95090c2-08cb-4d34-e663-018b63c26763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3SBEYwWblzs"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvH9hE4sblzs"
      },
      "source": [
        "张量上的操作\n",
        "=====================\n",
        "\n",
        "包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等等在内的100多个张量操作在此处有详细描述。\n",
        "\n",
        "这些操作都可以在GPU上运行（通常比在CPU上运行速度更快）。如果您使用的是Colab，可以通过前往“运行时”>“更改运行时类型”>“GPU”来分配GPU。\n",
        "\n",
        "默认情况下，张量是在CPU上创建的。我们需要使用.to方法显式地将张量移动到GPU上（在检查GPU可用性之后）。请注意，在不同设备之间复制大张量在时间和内存方面可能代价很大！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NA1oPfwblzs"
      },
      "outputs": [],
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP5YBtefblzt"
      },
      "source": [
        "尝试从列表中执行一些操作。如果您已经熟悉NumPy API，您会发现Tensor API非常易于使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D1wOEmhblzt"
      },
      "source": [
        "**标准的类似NumPy的索引和切片**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVXybRe6blzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b48765d-4120-47fd-ae16-272b667cc765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSN2HbFkblzt"
      },
      "source": [
        "**连接张量**\n",
        "\n",
        "您可以使用torch.cat来沿着给定的维度连接一系列张量。另请参阅torch.stack，这是另一种略有不同的张量连接操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5sVtzvrblzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e6101a-e8df-4dbf-f888-9c25fbd3719a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
        "print(t2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwcxgrOMl0Ku",
        "outputId": "d9565fd2-defa-49f8-9b79-fff5410b2536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9amXwwBblzt"
      },
      "source": [
        "**算术操作**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2W0eSWblzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3856cf-2485-4edc-bde3-cb890d547aa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码展示了在PyTorch中对张量（tensor）进行矩阵乘法（也称为矩阵乘积）和逐元素乘法（也称为逐元素积）的几种不同方法。我们将逐一解释每个部分。\n",
        "\n",
        "### 矩阵乘法\n",
        "\n",
        "1. **使用`@`运算符**\n",
        "   ```python\n",
        "   y1 = tensor @ tensor.T\n",
        "   ```\n",
        "   这里使用了`@`运算符来进行矩阵乘法。`tensor.T`表示`tensor`的转置。结果是`y1`成为`tensor`和它的转置的矩阵乘积。\n",
        "\n",
        "2. **使用`matmul`方法**\n",
        "   ```python\n",
        "   y2 = tensor.matmul(tensor.T)\n",
        "   ```\n",
        "   这也是进行矩阵乘法，`matmul`方法直接作用于`tensor`对象上，同样需要对其中一个张量进行转置以确保乘法操作的正确执行。\n",
        "\n",
        "3. **使用`matmul`函数并指定输出张量**\n",
        "   ```python\n",
        "   y3 = torch.rand_like(y1)\n",
        "   torch.matmul(tensor, tensor.T, out=y3)\n",
        "   ```\n",
        "   这行代码首先创建了一个与`y1`形状和数据类型相同的随机张量`y3`，然后通过`torch.matmul`函数进行矩阵乘法，并将结果存储在`y3`中。这里`out=y3`参数指定了输出张量。\n",
        "\n",
        "### 逐元素乘法\n",
        "\n",
        "1. **使用`*`运算符**\n",
        "   ```python\n",
        "   z1 = tensor * tensor\n",
        "   ```\n",
        "   `*`运算符用于逐元素乘法，即将`tensor`中的每个元素自乘，结果赋值给`z1`。\n",
        "\n",
        "2. **使用`mul`方法**\n",
        "   ```python\n",
        "   z2 = tensor.mul(tensor)\n",
        "   ```\n",
        "   同样，`mul`方法也是用来执行逐元素乘法，这里的`tensor`对象调用了`mul`方法对自身进行乘法操作。\n",
        "\n",
        "3. **使用`mul`函数并指定输出张量**\n",
        "   ```python\n",
        "   z3 = torch.rand_like(tensor)\n",
        "   torch.mul(tensor, tensor, out=z3)\n",
        "   ```\n",
        "   这里首先创建了一个与`tensor`具有相同形状和数据类型的随机张量`z3`，然后使用`torch.mul`函数执行逐元素乘法，并将结果存储在`z3`中。\n",
        "\n",
        "通过这些示例，可以看出PyTorch提供了多种方法来进行相同的操作，包括使用运算符、方法或函数，并且还支持直接在一个预先分配好的张量中存储结果，这样做可以在进行大规模操作时节省内存和时间。"
      ],
      "metadata": {
        "id": "-Xu9OXJMmzmp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLEO9nCKblzt"
      },
      "source": [
        "**单元素张量**\n",
        "\n",
        "如果您有一个单元素的张量，例如，通过将所有张量值汇总为一个值，您可以使用item()方法将其转换为Python数值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RigY-iWblzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121b7c20-7170-4ab8-f25f-314cf66e049c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orXd338gblzt"
      },
      "source": [
        "**就地操作**\n",
        "\n",
        "\n",
        "将结果存储在操作数中的操作称为就地操作。它们通过下划线后缀来表示。例如：x.copy_(y)，x.t_()，将会改变x。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkVFnbiMblzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c0d050-f4e6-4191-ed27-ca942e871f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ],
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVAgNWqOblzu"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>注意：</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>就地操作可以节省一些内存，但在计算导数时会因为立即失去历史记录而变得问题重重。因此，不鼓励使用就地操作。</p>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opLfOlBjblzu"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR14Ng3sblzu"
      },
      "source": [
        "与NumPy的桥接\n",
        "=================\n",
        "\n",
        "CPU上的张量和NumPy数组可以共享它们的基础内存位置，改变其中一个也将改变另一个。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a6hdyJblzu"
      },
      "source": [
        "张量转NumPy数组\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2eVVnqqblzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc13537-2a87-4842-a232-f3f94e292508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a20HIRPbblzu"
      },
      "source": [
        "张量的更改会反映在NumPy数组中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBfdeuWeblzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489bf8cc-5ac9-48ea-ae72-ba8c063693ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJd_nUciblzu"
      },
      "source": [
        "NumPy数组转张量\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9FbRVnFblzv"
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60B_YH6Dblzv"
      },
      "source": [
        "NumPy数组的更改会反映在张量中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObNxavITblzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec30b734-c0e9-4868-f0b8-0bddb385863c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}