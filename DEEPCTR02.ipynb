{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE9fNt0GMMJNeadVwsEWdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/DEEPCTR02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "深度兴趣网络（Deep Interest Network, DIN）是阿里巴巴在2018年提出的一种用于点击率预测（CTR）的深度学习模型。其核心思想可以用一个生活化的比喻来理解：想象你是一位网购爱好者，平台需要根据你过去浏览的商品（比如运动鞋、咖啡机、小说）来猜测你现在可能对哪个广告感兴趣。传统方法就像把所有历史行为“一视同仁”地分析，但DIN却能像人类一样，**动态关注与当前广告最相关的行为**。例如，当你看到一款新跑鞋广告时，DIN会重点参考你过去浏览运动鞋的记录，而忽略咖啡机和小说这类无关行为。\n",
        "\n",
        "### 一、DIN解决了什么问题？\n",
        "点击率预测（CTR）是推荐系统和广告投放的核心任务，目标是预测用户点击某个内容（如广告、商品）的概率。传统模型（如逻辑回归、因子分解机）存在两大局限：\n",
        "1. **兴趣建模僵化**：用户的历史行为被压缩成固定长度的向量，无法灵活表达多样的兴趣。例如，用户可能同时喜欢“运动”和“文学”，但传统模型难以区分这两种兴趣在不同场景下的权重。\n",
        "2. **噪声干扰**：用户行为中混杂大量无关历史（例如误点或随意浏览），传统模型无法有效过滤这些噪声。\n",
        "\n",
        "### 二、DIN的核心创新：像人一样“动态关注”\n",
        "DIN通过以下三个关键设计解决上述问题：\n",
        "1. **注意力机制（Attention）**  \n",
        "   这是DIN的灵魂。模型会为每个用户行为计算一个“相关性权重”：与当前广告越相关的行为，权重越高。例如，用户的历史行为包括“运动鞋、咖啡机、小说”，当预测跑鞋广告的点击率时，模型会给“运动鞋”行为赋予高权重，而“咖啡机”和“小说”的权重则很低。这就像人类看到广告时，只会回想相关的购买经历。\n",
        "\n",
        "2. **自适应激活函数（Dice）**  \n",
        "   传统激活函数（如ReLU）的阈值是固定的，但DIN的Dice函数能根据数据分布动态调整阈值。例如，当用户行为数据差异较大时，Dice会自动适应不同场景，提升模型的灵活性。\n",
        "\n",
        "3. **高效正则化（Mini-batch Aware Regularization）**  \n",
        "   面对海量数据，传统正则化方法计算成本极高。DIN只对当前训练批次（mini-batch）中出现过的特征进行正则化，既防止过拟合，又大幅减少计算量。\n",
        "\n",
        "### 三、DIN的实际效果如何？\n",
        "实验表明，DIN在多个场景下显著优于传统模型：\n",
        "- **离线测试**：在亚马逊和MovieLens数据集上，DIN的AUC（衡量预测准确性的指标）比传统模型（如Wide&Deep、PNN）提升约1.89%。\n",
        "- **线上应用**：在阿里巴巴广告系统中，DIN使点击率（CTR）提升10%，广告收入增长3.8%。这意味着每展示100次广告，DIN能多带来1次点击，这在亿级流量场景下效益巨大。\n",
        "\n",
        "### 四、DIN的启示：从“静态画像”到“动态兴趣”\n",
        "DIN的成功揭示了推荐系统的未来方向：**用户的兴趣是多样且动态变化的**。与其用固定标签定义用户（如“运动爱好者”），不如根据具体场景实时捕捉兴趣焦点。这种思路也被后续模型（如DIEN）进一步扩展，加入了兴趣演化的时序建模。\n",
        "\n",
        "总结来说，DIN的核心思想是 **“动态相关性”** ——让模型像人一样，在不同场景下灵活关注最相关的历史行为，从而更精准地预测用户的点击意愿。这一创新不仅提升了技术指标，也推动了推荐系统从“粗放推荐”向“智能理解”的跨越。"
      ],
      "metadata": {
        "id": "6o_E1abjBxWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 生成一个包含 10 个随机整数的一维张量，范围是 [0, 10)\n",
        "random_tensor = torch.randint(0, 10, (10,))\n",
        "print(random_tensor)\n",
        "\n",
        "# 生成一个 3x2 的二维张量，范围是 [1, 5)\n",
        "random_tensor_2d = torch.randint(1, 5, (3, 2))\n",
        "print(random_tensor_2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULNPxiIV9aRC",
        "outputId": "bf070204-0d26-4363-b098-55d2788b7752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 0, 8, 4, 1, 1, 5, 9, 9, 1])\n",
            "tensor([[4, 3],\n",
            "        [3, 4],\n",
            "        [1, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unsqueeze(1) 的作用？**\n",
        "\n",
        "unsqueeze(1) 是 PyTorch 张量的一个方法，用于在指定维度上增加一个维度。1 表示在第二个维度（索引为 1）上增加维度。\n",
        "\n",
        "例如，假设有一个形状为 (64,) 的张量 user_ages，表示 64 个用户的年龄。执行 user_ages.unsqueeze(1) 后，张量的形状会变成 (64, 1)。相当于把原来的一维张量变成了一个二维张量，其中每个元素都被放在一个单独的行中。\n",
        "\n",
        "**为什么需要增加一个维度？**\n",
        "\n",
        "在深度学习模型中，输入数据的维度通常需要满足特定的要求。例如，在 DIN 模型中，用户的年龄和商品价格都是数值型特征，通常会使用一个线性层来处理这些特征。线性层的输入需要是一个二维张量，其中第一维表示样本数量，第二维表示特征维度。\n",
        "\n",
        "因此，如果用户的年龄或商品价格只有一维，就需要使用 unsqueeze(1) 在第二个维度上增加一个维度，使其变成一个二维张量，以便与线性层的输入维度匹配。"
      ],
      "metadata": {
        "id": "Sis9aReW_rSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 定义模型的超参数\n",
        "embedding_dim = 32  # Embedding 向量的维度，每个用户/商品/类别都会被映射到一个 32 维的向量\n",
        "hidden_units = [64, 32]  # 全连接层隐藏单元数量，表示网络中两个隐藏层的节点数\n",
        "num_users = 1000  # 用户总数\n",
        "num_items = 2000  # 商品总数\n",
        "num_categories = 10  # 商品类别总数\n",
        "\n",
        "\n",
        "# 定义一个函数来生成模拟数据\n",
        "def generate_sample_data(batch_size=64):\n",
        "    # 生成用户 ID，范围在 [0, num_users) 之间，形状为 (batch_size,)\n",
        "    user_ids = torch.randint(0, num_users, (batch_size,))\n",
        "    # 生成用户性别，0 或 1，形状为 (batch_size,)\n",
        "    user_genders = torch.randint(0, 2, (batch_size,))\n",
        "    # 生成用户年龄，范围在 [18, 60) 之间，形状为 (batch_size,)，并转换为浮点数\n",
        "    user_ages = torch.randint(18, 60, (batch_size,)).float()\n",
        "\n",
        "    # 生成商品 ID，范围在 [0, num_items) 之间，形状为 (batch_size,)\n",
        "    item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成商品类别，范围在 [0, num_categories) 之间，形状为 (batch_size,)\n",
        "    item_categories = torch.randint(0, num_categories, (batch_size,))\n",
        "    # 生成商品价格，范围在 [0, 1000) 之间，形状为 (batch_size,)\n",
        "    item_prices = torch.rand(batch_size) * 1000\n",
        "\n",
        "    # 生成用户的历史行为序列，每个用户随机浏览 1-5 个商品\n",
        "    history_item_ids_list = []\n",
        "    for _ in range(batch_size):\n",
        "        # 随机生成序列长度，范围在 [1, 6) 之间\n",
        "        seq_len = torch.randint(1, 6, (1,)).item()\n",
        "        # 生成历史行为序列，范围在 [0, num_items) 之间，形状为 (seq_len,)\n",
        "        history_seq = torch.randint(0, num_items, (seq_len,))\n",
        "        # 将生成的序列添加到列表中\n",
        "        history_item_ids_list.append(history_seq)\n",
        "\n",
        "    # 将历史行为序列填充到相同长度 (最长序列长度)，并转换为张量\n",
        "    # 获取最长序列长度\n",
        "    max_len = max([seq.size(0) for seq in history_item_ids_list])\n",
        "    # 创建一个全零张量，形状为 (batch_size, max_len)，用于存储填充后的序列\n",
        "    padded_history_item_ids = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    # 遍历每个用户的历史行为序列\n",
        "    for i, seq in enumerate(history_item_ids_list):\n",
        "        # 将序列填充到 padded_history_item_ids 中\n",
        "        padded_history_item_ids[i, :seq.size(0)] = seq\n",
        "\n",
        "    # 生成目标商品 ID，范围在 [0, num_items) 之间，形状为 (batch_size,)\n",
        "    target_item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成点击标签，0 或 1，形状为 (batch_size,)，并转换为浮点数\n",
        "    labels = torch.randint(0, 2, (batch_size,)).float()\n",
        "\n",
        "    # 将所有数据打包成一个字典返回\n",
        "    return {\n",
        "        'user_id': user_ids,\n",
        "        'user_gender': user_genders,\n",
        "        'user_age': user_ages.unsqueeze(1),  # 增加一个维度，以便与模型输入匹配\n",
        "        'item_id': item_ids,\n",
        "        'item_category': item_categories,\n",
        "        'item_price': item_prices.unsqueeze(1),  # 增加一个维度，以便与模型输入匹配\n",
        "        'history_item_ids': padded_history_item_ids,\n",
        "        'target_item_id': target_item_ids,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# 调用 generate_sample_data 函数生成一个 batch 的数据\n",
        "sample_data = generate_sample_data()\n",
        "# 打印示例数据的键值，以便查看数据结构\n",
        "print(\"示例数据：\", sample_data.keys())\n",
        "# 打印历史行为序列的形状，以便查看数据维度\n",
        "print(\"历史行为序列形状:\", sample_data['history_item_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJ_mWPW7N6S",
        "outputId": "be9b1458-d58d-4001-d1e8-57f1ba025465"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "示例数据： dict_keys(['user_id', 'user_gender', 'user_age', 'item_id', 'item_category', 'item_price', 'history_item_ids', 'target_item_id', 'label'])\n",
            "历史行为序列形状: torch.Size([64, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Embedding 的工作原理**\n",
        "\n",
        "nn.Embedding 层的作用是将离散的特征（例如商品 ID）映射成连续的向量表示（embedding 向量）。它内部维护一个查找表，表中的每一行代表一个特征的 embedding 向量。\n",
        "\n",
        "当我们输入一个特征 ID 时，nn.Embedding 层会根据这个 ID 在查找表中找到对应的 embedding 向量并返回。\n",
        "\n",
        "**处理不同维度输入的关键**\n",
        "\n",
        "nn.Embedding 层可以处理任意形状的整数张量作为输入。它会将输入张量中的每个元素都视为一个特征 ID，并根据 ID 查找对应的 embedding 向量。\n",
        "\n",
        "处理 item_id: item_id 的形状是 (batch_size,)，它包含了每个样本的商品 ID。nn.Embedding 层会将 item_id 中的每个 ID 都映射成一个 embedding 向量，最终返回一个形状为 (batch_size, embedding_dim) 的张量。\n",
        "\n",
        "处理 history_item_ids: history_item_ids 的形状是 (batch_size, seq_len)，它包含了每个样本的历史行为序列，序列中的每个元素都是一个商品 ID。nn.Embedding 层会将 history_item_ids 中的每个 ID 都映射成一个 embedding 向量，最终返回一个形状为 (batch_size, seq_len, embedding_dim) 的张量。\n",
        "\n",
        "**总结**\n",
        "\n",
        "nn.Embedding 层能够处理不同维度的输入，因为它会将输入张量中的每个元素都视为一个特征 ID，并根据 ID 查找对应的 embedding 向量。\n",
        "\n",
        "因此，即使 item_id 和 history_item_ids 的维度不同，item_embedding 函数仍然可以处理它们，并返回相应形状的 embedding 向量张量。"
      ],
      "metadata": {
        "id": "KGZ_Iv5kMavS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DIN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_categories, embedding_dim, hidden_units):\n",
        "        super(DIN, self).__init__()\n",
        "\n",
        "        # 创建用户嵌入层，将用户ID映射到embedding_dim维度的向量空间\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # 创建商品嵌入层，将商品ID映射到embedding_dim维度的向量空间\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # 创建类别嵌入层，将类别ID映射到embedding_dim维度的向量空间\n",
        "        self.category_embedding = nn.Embedding(num_categories, embedding_dim)\n",
        "\n",
        "        # 创建数值型特征的线性层，将用户年龄和商品价格映射到embedding_dim维度的向量空间\n",
        "        self.numerical_fc = nn.Linear(2, embedding_dim)\n",
        "\n",
        "        # 创建注意力池化层，用于根据目标商品对历史行为进行加权\n",
        "        self.attention_pooling = AttentionPoolingLayer(embedding_dim)\n",
        "\n",
        "        # 创建全连接层，用于最终的点击率预测\n",
        "        fc_layers = []\n",
        "        # 初始化全连接层的输入维度，包括用户、商品、类别、数值型特征和注意力输出的embedding维度之和\n",
        "        input_dim = embedding_dim * 4 + embedding_dim\n",
        "        # 逐层构建全连接层，并使用ReLU作为激活函数\n",
        "        for units in hidden_units:\n",
        "            fc_layers.append(nn.Linear(input_dim, units))\n",
        "            fc_layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        # 添加最后一层全连接层，输出维度为1，表示点击率预测值\n",
        "        fc_layers.append(nn.Linear(input_dim, 1))\n",
        "        # 将所有全连接层组合成一个序列\n",
        "        self.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, user_id, user_gender, user_age, item_id, item_category, item_price, history_item_ids, target_item_id):\n",
        "        \"\"\"\n",
        "        DIN模型的前向传播函数\n",
        "\n",
        "        输入:\n",
        "            user_id: 用户ID, 形状: (batch_size,)\n",
        "            user_gender: 用户性别, 形状: (batch_size,)\n",
        "            user_age: 用户年龄, 形状: (batch_size, 1)\n",
        "            item_id: 商品ID, 形状: (batch_size,)\n",
        "            item_category: 商品类别, 形状: (batch_size,)\n",
        "            item_price: 商品价格, 形状: (batch_size, 1)\n",
        "            history_item_ids: 用户历史行为序列, 形状: (batch_size, seq_len)\n",
        "            target_item_id: 目标商品ID, 形状: (batch_size,)\n",
        "\n",
        "        输出:\n",
        "            output: 点击率预测值, 形状: (batch_size,)\n",
        "        \"\"\"\n",
        "        # 1. 获取各个特征的嵌入向量\n",
        "        # 获取用户嵌入向量\n",
        "        user_embed = self.user_embedding(user_id)\n",
        "        # 获取商品嵌入向量\n",
        "        item_embed = self.item_embedding(item_id)\n",
        "        # 获取类别嵌入向量\n",
        "        category_embed = self.category_embedding(item_category)\n",
        "        # 获取目标商品嵌入向量\n",
        "        target_item_embed = self.item_embedding(target_item_id)\n",
        "        # 获取历史行为序列嵌入向量\n",
        "        history_item_embed = self.item_embedding(history_item_ids)\n",
        "\n",
        "\n",
        "        # 2. 处理数值型特征\n",
        "        # 将用户年龄和商品价格拼接成一个张量\n",
        "        numerical_features = torch.cat([user_age, item_price], dim=-1)\n",
        "        # 使用线性层和ReLU激活函数将数值型特征映射到embedding_dim维度的向量空间\n",
        "        numerical_embed = F.relu(self.numerical_fc(numerical_features))\n",
        "\n",
        "        # 3. 使用注意力池化层对历史行为进行加权\n",
        "        # 使用注意力机制，根据目标商品，对用户的历史行为序列进行加权聚合，得到用户的兴趣表示\n",
        "        attention_output = self.attention_pooling(queries=target_item_embed, keys=history_item_embed)\n",
        "\n",
        "        # 4. 将所有特征拼接在一起\n",
        "        # 将用户、商品、类别、数值型特征和注意力输出的embedding向量拼接在一起\n",
        "        concat_features = torch.cat([user_embed, item_embed, numerical_embed, attention_output, target_item_embed], dim=-1)\n",
        "\n",
        "        # 5. 通过全连接层进行预测\n",
        "        # 将拼接后的特征向量输入到全连接层，得到预测结果\n",
        "        output = self.fc(concat_features)\n",
        "        # 使用 sigmoid 激活函数将输出转换为点击率 (0-1)\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        # 返回预测结果，并移除维度为 1 的维度\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "# 注意力池化层的实现（！存在问题！）\n",
        "class AttentionPoolingLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(AttentionPoolingLayer, self).__init__()\n",
        "        # 创建一个线性层，将 embedding 向量映射到注意力权重\n",
        "        self.attention_fc = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, queries, keys):\n",
        "        \"\"\"\n",
        "        前向传播函数，计算注意力权重并进行加权池化\n",
        "        :param queries: 目标商品的嵌入向量 (batch_size, embedding_dim)\n",
        "        :param keys: 历史行为序列的嵌入向量 (batch_size, seq_len, embedding_dim)\n",
        "        :return: 注意力池化后的用户兴趣表示 (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. 计算注意力分数\n",
        "        # 扩展目标商品嵌入向量的维度，以便与历史行为序列嵌入向量进行元素乘法\n",
        "        queries = queries.unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
        "        # 计算目标商品与每个历史行为商品的相似度，作为注意力分数\n",
        "        attention_scores = torch.sum(queries * keys, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 2. 使用全连接层进一步学习注意力分数\n",
        "        # 将历史行为序列嵌入向量进行reshape，然后通过线性层进行变换，得到新的注意力分数\n",
        "        attention_scores = self.attention_fc(keys.view(-1, keys.size(-1))).view(keys.size(0), keys.size(1))  # (batch_size, seq_len)\n",
        "\n",
        "        # 3. 使用 Softmax 归一化注意力分数，得到注意力权重\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 4. 加权求和得到注意力池化后的结果\n",
        "        # 扩展注意力权重的维度，与历史行为序列嵌入向量相乘，然后在序列维度上求和\n",
        "        attention_output = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "# 初始化 DIN 模型\n",
        "model = DIN(num_users, num_items, num_categories, embedding_dim, hidden_units)\n",
        "print(model) # 打印模型结构"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GTkd9SI7cby",
        "outputId": "30cb1a6f-979d-4cd0-c9e8-34452632f263"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIN(\n",
            "  (user_embedding): Embedding(1000, 32)\n",
            "  (item_embedding): Embedding(2000, 32)\n",
            "  (category_embedding): Embedding(10, 32)\n",
            "  (numerical_fc): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (attention_pooling): AttentionPoolingLayer(\n",
            "    (attention_fc): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**queries * keys**: 这是两个张量的元素乘法操作。\n",
        "\n",
        "queries 代表目标商品的 embedding 向量，形状为 (batch_size, 1, embedding_dim)。\n",
        "keys 代表用户历史行为序列中每个商品的 embedding 向量，形状为 (batch_size, seq_len, embedding_dim)。\n",
        "\n",
        "元素乘法操作会将 queries 和 keys 中对应位置的元素相乘，得到一个新的张量，形状为 (batch_size, seq_len, embedding_dim)。\n",
        "\n",
        "**torch.sum(...)**: 这是对张量进行求和的操作。\n",
        "\n",
        "dim=-1 指定了求和的维度，-1 表示最后一个维度，也就是 embedding_dim 这个维度。\n",
        "因此，torch.sum(queries * keys, dim=-1) 会将 queries * keys 这个张量在 embedding_dim 维度上进行求和，得到一个新的张量，形状为 (batch_size, seq_len)。\n",
        "\n",
        "\n",
        "**整体含义**:\n",
        "\n",
        "这行代码的整体含义是计算目标商品与每个历史行为商品之间的相似度，作为注意力分数。\n",
        "\n",
        "首先，通过元素乘法 (queries * keys) 计算目标商品 embedding 与每个历史行为商品 embedding 的对应元素乘积。\n",
        "\n",
        "然后，通过在 embedding_dim 维度上求和 (torch.sum(...)) 得到一个标量值，这个标量值代表了目标商品与某个历史行为商品之间的相似度。\n",
        "\n",
        "最终得到的张量 attention_scores 形状为 (batch_size, seq_len)，其中每个元素都代表目标商品与对应历史行为商品的相似度（注意力分数）。\n",
        "\n",
        "**直观理解**：\n",
        "\n",
        "可以将 queries 看作是目标商品的“查询向量”，将 keys 看作是历史行为商品的“键向量”。torch.sum(queries * keys, dim=-1) 的作用就是计算“查询向量”与每个“键向量”之间的相似度，相似度越高，注意力分数就越高，说明目标商品与该历史行为商品越相关。"
      ],
      "metadata": {
        "id": "k4saCipCSWki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用示例数据进行一次前向预测\n",
        "model.eval() # 设置模型为评估模式 (不进行梯度计算)\n",
        "with torch.no_grad(): # 上下文管理器，禁止梯度计算\n",
        "    input_data = {k: v for k, v in sample_data.items() if k != 'label'}\n",
        "    predictions = model(**input_data) # 将 input_data 字典作为参数传入模型\n",
        "    print(\"预测结果 (前 10 个样本):\", predictions[:10])\n",
        "    print(\"预测结果形状:\", predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmT2eWvC8G8e",
        "outputId": "6cc9a6e2-8c30-43f9-945c-57a592bfc631"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "预测结果 (前 10 个样本): tensor([4.5698e-01, 2.5763e-03, 5.2270e-03, 1.6382e-04, 1.5215e-02, 4.5323e-03,\n",
            "        2.8371e-01, 2.4951e-02, 3.9270e-01, 6.4684e-03])\n",
            "预测结果形状: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 定义模型的超参数\n",
        "embedding_dim = 32   # Embedding 向量的维度\n",
        "hidden_units = [64, 32]   # 全连接层隐藏单元数量\n",
        "num_users = 1000   # 用户总数\n",
        "num_items = 2000   # 商品总数\n",
        "num_categories = 50   # 商品类别总数\n",
        "num_genders = 2 # 用户性别数量\n",
        "\n",
        "# 定义一个函数来生成模拟数据\n",
        "def generate_sample_data(batch_size=64):\n",
        "    # 生成用户 ID\n",
        "    user_ids = torch.randint(0, num_users, (batch_size,))\n",
        "    # 生成用户性别\n",
        "    user_genders = torch.randint(0, num_genders, (batch_size,))\n",
        "    # 生成用户年龄\n",
        "    user_ages = torch.randint(18, 60, (batch_size,)).float()\n",
        "\n",
        "    # 生成商品 ID\n",
        "    item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成商品类别\n",
        "    item_categories = torch.randint(0, num_categories, (batch_size,))\n",
        "    # 生成商品价格\n",
        "    item_prices = torch.rand(batch_size) * 1000\n",
        "\n",
        "    # 生成用户的历史行为序列\n",
        "    history_item_ids_list = []\n",
        "    for _ in range(batch_size):\n",
        "        seq_len = torch.randint(1, 6, (1,)).item()\n",
        "        history_seq = torch.randint(0, num_items, (seq_len,))\n",
        "        history_item_ids_list.append(history_seq)\n",
        "\n",
        "    # 填充历史行为序列到相同长度\n",
        "    max_len = max([seq.size(0) for seq in history_item_ids_list])\n",
        "    padded_history_item_ids = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    for i, seq in enumerate(history_item_ids_list):\n",
        "        padded_history_item_ids[i, :seq.size(0)] = seq\n",
        "\n",
        "    # 生成目标商品 ID\n",
        "    target_item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成点击标签\n",
        "    labels = torch.randint(0, 2, (batch_size,)).float()\n",
        "\n",
        "    return {\n",
        "        'user_id': user_ids,\n",
        "        'user_gender': user_genders,\n",
        "        'user_age': user_ages.unsqueeze(1),\n",
        "        'item_id': item_ids,\n",
        "        'item_category': item_categories,\n",
        "        'item_price': item_prices.unsqueeze(1),\n",
        "        'history_item_ids': padded_history_item_ids,\n",
        "        'target_item_id': target_item_ids,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# 调用 generate_sample_data 函数生成一个 batch 的数据\n",
        "sample_data = generate_sample_data()\n",
        "# 打印示例数据的键值\n",
        "print(\"示例数据：\", sample_data.keys())\n",
        "# 打印历史行为序列的形状\n",
        "print(\"历史行为序列形状:\", sample_data['history_item_ids'].shape)\n",
        "\n",
        "\n",
        "# 注意力池化层的实现 (标准 MLP 注意力网络)\n",
        "class AttentionPoolingLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(AttentionPoolingLayer, self).__init__()\n",
        "        # 使用 MLP 注意力网络\n",
        "        self.attention_fc = nn.Sequential(\n",
        "            # 输入维度为 query 和 key 拼接后的维度\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            # 输出维度为 1，得到注意力权重\n",
        "            nn.Linear(embedding_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys):\n",
        "        \"\"\"\n",
        "        前向传播函数，计算注意力权重并进行加权池化\n",
        "        :param queries: 目标商品的嵌入向量 (batch_size, embedding_dim)\n",
        "        :param keys: 历史行为序列的嵌入向量 (batch_size, seq_len, embedding_dim)\n",
        "        :return: 注意力池化后的用户兴趣表示 (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # 1. 计算注意力分数\n",
        "        queries = queries.unsqueeze(1) # (batch_size, 1, embedding_dim)\n",
        "        # 扩展 queries 以便与 keys 进行拼接\n",
        "        queries = queries.expand(-1, keys.size(1), -1) # (batch_size, seq_len, embedding_dim)\n",
        "        # 将 query 和 key 拼接在一起\n",
        "        attention_input = torch.cat([queries, keys], dim=-1) # (batch_size, seq_len, embedding_dim * 2)\n",
        "        # 通过 MLP 注意力网络计算注意力分数\n",
        "        attention_scores = self.attention_fc(attention_input.view(-1, attention_input.size(-1))).view(keys.size(0), keys.size(1)) # (batch_size, seq_len)\n",
        "\n",
        "        # 2. 使用 Softmax 归一化注意力分数，得到注意力权重\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1) # (batch_size, seq_len)\n",
        "\n",
        "        # 3. 加权求和得到注意力池化后的结果\n",
        "        attention_output = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1) # (batch_size, embedding_dim)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class DIN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_categories, num_genders, embedding_dim, hidden_units):\n",
        "        super(DIN, self).__init__()\n",
        "\n",
        "        # 创建用户嵌入层\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # 创建用户性别嵌入层\n",
        "        self.user_gender_embedding = nn.Embedding(num_genders, embedding_dim)\n",
        "        # 创建商品嵌入层\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # 创建类别嵌入层\n",
        "        self.category_embedding = nn.Embedding(num_categories, embedding_dim)\n",
        "\n",
        "        # 创建数值型特征的线性层\n",
        "        self.numerical_fc = nn.Linear(2, embedding_dim)\n",
        "\n",
        "        # 创建注意力池化层\n",
        "        self.attention_pooling = AttentionPoolingLayer(embedding_dim)\n",
        "\n",
        "        # 创建全连接层\n",
        "        fc_layers = []\n",
        "        # 计算全连接层的输入维度 (6 个 embedding 特征)\n",
        "        input_dim = embedding_dim * 6  # 用户，用户性别，商品，类别，数值型特征，注意力输出\n",
        "        for units in hidden_units:\n",
        "            fc_layers.append(nn.Linear(input_dim, units))\n",
        "            fc_layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        fc_layers.append(nn.Linear(input_dim, 1))\n",
        "        self.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, user_id, user_gender, user_age, item_id, item_category, item_price, history_item_ids, target_item_id):\n",
        "        \"\"\"\n",
        "        DIN模型的前向传播函数\n",
        "\n",
        "        输入:\n",
        "            user_id: 用户ID, 形状: (batch_size,)\n",
        "            user_gender: 用户性别, 形状: (batch_size,)\n",
        "            user_age: 用户年龄, 形状: (batch_size, 1)\n",
        "            item_id: 商品ID, 形状: (batch_size,)\n",
        "            item_category: 商品类别, 形状: (batch_size,)\n",
        "            item_price: 商品价格, 形状: (batch_size, 1)\n",
        "            history_item_ids: 用户历史行为序列, 形状: (batch_size, seq_len)\n",
        "            target_item_id: 目标商品ID, 形状: (batch_size,)\n",
        "\n",
        "        输出:\n",
        "            output: 点击率预测值, 形状: (batch_size,)\n",
        "        \"\"\"\n",
        "        # 1. 获取各个特征的嵌入向量\n",
        "        user_embed = self.user_embedding(user_id)\n",
        "        user_gender_embed = self.user_gender_embedding(user_gender)\n",
        "        item_embed = self.item_embedding(item_id)\n",
        "        category_embed = self.category_embedding(item_category)\n",
        "        target_item_embed = self.item_embedding(target_item_id)\n",
        "        history_item_embed = self.item_embedding(history_item_ids)\n",
        "\n",
        "\n",
        "        # 2. 处理数值型特征\n",
        "        numerical_features = torch.cat([user_age, item_price], dim=-1)\n",
        "        numerical_embed = F.relu(self.numerical_fc(numerical_features))\n",
        "\n",
        "        # 3. 使用注意力池化层对历史行为进行加权\n",
        "        attention_output = self.attention_pooling(queries=target_item_embed, keys=history_item_embed)\n",
        "\n",
        "        # 4. 拼接所有特征\n",
        "        concat_features = torch.cat([user_embed, user_gender_embed, item_embed, category_embed, numerical_embed, attention_output], dim=-1)\n",
        "\n",
        "        # 5. 通过全连接层进行预测\n",
        "        output = self.fc(concat_features)\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# 初始化 DIN 模型\n",
        "num_users = 1000\n",
        "num_items = 2000\n",
        "num_categories = 50\n",
        "num_genders = 2\n",
        "embedding_dim = 64\n",
        "hidden_units = [128, 64]\n",
        "model = DIN(num_users, num_items, num_categories, num_genders, embedding_dim, hidden_units)\n",
        "print(model)\n",
        "\n",
        "# 损失函数和优化器\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# LogLoss 计算函数 (PyTorch 版本)\n",
        "def log_loss(y_true, y_pred):\n",
        "    y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
        "\n",
        "# 训练循环\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx in range(0, 1000, batch_size):\n",
        "        train_batch_data = generate_sample_data(batch_size)\n",
        "        labels = train_batch_data['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 前向传播\n",
        "        predictions = model(train_batch_data['user_id'],\n",
        "                          train_batch_data['user_gender'],\n",
        "                          train_batch_data['user_age'],\n",
        "                          train_batch_data['item_id'],\n",
        "                          train_batch_data['item_category'],\n",
        "                          train_batch_data['item_price'],\n",
        "                          train_batch_data['history_item_ids'],\n",
        "                          train_batch_data['target_item_id'])\n",
        "\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (1000 / batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"训练完成!\")\n",
        "\n",
        "# 评估循环\n",
        "def evaluate_model(model, batch_size=64):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    total_eval_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(0, 500, batch_size):\n",
        "            eval_batch_data = generate_sample_data(batch_size)\n",
        "            labels = eval_batch_data['label']\n",
        "\n",
        "            # 前向传播\n",
        "            predictions = model(eval_batch_data['user_id'],\n",
        "                              eval_batch_data['user_gender'],\n",
        "                              eval_batch_data['user_age'],\n",
        "                              eval_batch_data['item_id'],\n",
        "                              eval_batch_data['item_category'],\n",
        "                              eval_batch_data['item_price'],\n",
        "                              eval_batch_data['history_item_ids'],\n",
        "                              eval_batch_data['target_item_id'])\n",
        "\n",
        "            eval_loss = loss_fn(predictions, labels)\n",
        "            total_eval_loss += eval_loss.item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / (500 / batch_size)\n",
        "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
        "    logloss_score = log_loss(torch.tensor(all_labels), torch.tensor(all_predictions)).item()\n",
        "\n",
        "    print(f\"Evaluation - Average Loss: {avg_eval_loss:.4f}, AUC: {auc_score:.4f}, LogLoss: {logloss_score:.4f}\")\n",
        "    return avg_eval_loss, auc_score, logloss_score\n",
        "\n",
        "\n",
        "# 在训练完成后进行评估\n",
        "print(\"开始评估...\")\n",
        "evaluate_model(model)\n",
        "print(\"评估完成!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHGk7gUqZrCg",
        "outputId": "667b2850-0f8d-4949-df96-2487a2b600a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "示例数据： dict_keys(['user_id', 'user_gender', 'user_age', 'item_id', 'item_category', 'item_price', 'history_item_ids', 'target_item_id', 'label'])\n",
            "历史行为序列形状: torch.Size([64, 5])\n",
            "DIN(\n",
            "  (user_embedding): Embedding(1000, 64)\n",
            "  (user_gender_embedding): Embedding(2, 64)\n",
            "  (item_embedding): Embedding(2000, 64)\n",
            "  (category_embedding): Embedding(50, 64)\n",
            "  (numerical_fc): Linear(in_features=2, out_features=64, bias=True)\n",
            "  (attention_pooling): AttentionPoolingLayer(\n",
            "    (attention_fc): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch [1/10], Average Loss: 1.2607\n",
            "Epoch [2/10], Average Loss: 0.7328\n",
            "Epoch [3/10], Average Loss: 0.7200\n",
            "Epoch [4/10], Average Loss: 0.7217\n",
            "Epoch [5/10], Average Loss: 0.7504\n",
            "Epoch [6/10], Average Loss: 0.7309\n",
            "Epoch [7/10], Average Loss: 0.7435\n",
            "Epoch [8/10], Average Loss: 0.8102\n",
            "Epoch [9/10], Average Loss: 0.7208\n",
            "Epoch [10/10], Average Loss: 0.7192\n",
            "训练完成!\n",
            "开始评估...\n",
            "Evaluation - Average Loss: 0.7733, AUC: 0.4992, LogLoss: 0.7552\n",
            "评估完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile, io\n",
        "url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "metadata": {
        "id": "JiuMDQzgFSgi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 下载 MovieLens 100K 数据集 (如果尚未下载)\n",
        "# 可以手动下载：https://grouplens.org/datasets/movielens/100k/\n",
        "# 或者使用代码下载 (需要安装 requests 库)：\n",
        "# import requests, zipfile, io\n",
        "# url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "# r = requests.get(url)\n",
        "# z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "# z.extractall()\n",
        "\n",
        "# 加载用户数据\n",
        "users_df = pd.read_csv('ml-100k/u.user', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "# 加载电影数据\n",
        "items_df = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1',\n",
        "                       names=['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url', 'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "# 加载评分数据\n",
        "ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "\n",
        "print(\"用户数据 (users_df) 示例:\")\n",
        "print(users_df.head())\n",
        "print(\"\\n电影数据 (items_df) 示例:\")\n",
        "print(items_df.head())\n",
        "print(\"\\n评分数据 (ratings_df) 示例:\")\n",
        "print(ratings_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkkgilykFMb7",
        "outputId": "744b2bbc-036b-4b0b-c4b4-97f741a96061"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "用户数据 (users_df) 示例:\n",
            "   user_id  age gender  occupation zip_code\n",
            "0        1   24      M  technician    85711\n",
            "1        2   53      F       other    94043\n",
            "2        3   23      M      writer    32067\n",
            "3        4   24      M  technician    43537\n",
            "4        5   33      F       other    15213\n",
            "\n",
            "电影数据 (items_df) 示例:\n",
            "   item_id              title release_date  video_release_date  \\\n",
            "0        1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
            "1        2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
            "2        3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
            "3        4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
            "4        5     Copycat (1995)  01-Jan-1995                 NaN   \n",
            "\n",
            "                                            imdb_url  unknown  Action  \\\n",
            "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0       0   \n",
            "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0       1   \n",
            "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0       0   \n",
            "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0       1   \n",
            "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0       0   \n",
            "\n",
            "   Adventure  Animation  Children's  ...  Fantasy  Film-Noir  Horror  Musical  \\\n",
            "0          0          1           1  ...        0          0       0        0   \n",
            "1          1          0           0  ...        0          0       0        0   \n",
            "2          0          0           0  ...        0          0       0        0   \n",
            "3          0          0           0  ...        0          0       0        0   \n",
            "4          0          0           0  ...        0          0       0        0   \n",
            "\n",
            "   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
            "0        0        0       0         0    0        0  \n",
            "1        0        0       0         1    0        0  \n",
            "2        0        0       0         1    0        0  \n",
            "3        0        0       0         0    0        0  \n",
            "4        0        0       0         1    0        0  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "评分数据 (ratings_df) 示例:\n",
            "   user_id  item_id  rating  timestamp\n",
            "0      196      242       3  881250949\n",
            "1      186      302       3  891717742\n",
            "2       22      377       1  878887116\n",
            "3      244       51       2  880606923\n",
            "4      166      346       1  886397596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了简化示例，将任务简化为二分类任务，即用户是否与电影发生交互。\n",
        "\n",
        "正样本：用户对电影有评分的记录。\n",
        "\n",
        "负样本：采用负采样策略，例如对于每个正样本用户-电影对，随机选择用户未评分的电影作为负样本。"
      ],
      "metadata": {
        "id": "KgR4lGMvYWU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # 导入 PyTorch 库，用于构建和训练神经网络。\n",
        "import torch.nn as nn # 从 PyTorch 导入神经网络模块，包含各种神经网络层。\n",
        "import torch.nn.functional as F # 从 PyTorch 导入神经网络函数，例如激活函数和损失函数。\n",
        "import numpy as np # 导入 NumPy 库，用于数值计算，例如处理数组和矩阵。\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, log_loss # 从 scikit-learn 库导入评估指标，用于模型性能评估。\n",
        "from sklearn.preprocessing import LabelEncoder # 从 scikit-learn 库导入 LabelEncoder，用于将类别特征编码为数值。\n",
        "import pandas as pd # 导入 Pandas 库，用于数据处理和分析，特别是使用 DataFrame 数据结构。\n",
        "from sklearn.model_selection import train_test_split # 从 scikit-learn 库导入 train_test_split，用于划分数据集为训练集和测试集。\n",
        "import random # 导入 random 库，用于生成随机数，例如在负采样中使用。\n",
        "import matplotlib.pyplot as plt # 导入 matplotlib 库，用于绘图，例如绘制 ROC 曲线。\n",
        "\n",
        "# 加载数据\n",
        "users_df = pd.read_csv('ml-100k/u.user', sep='|', names=['user_id', 'age', 'gender', 'occupation', 'zip_code']) # 使用 pandas 读取用户数据文件 u.user，指定分隔符为 '|'，列名。 users_df: DataFrame，包含用户数据。\n",
        "items_df = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1', # 使用 pandas 读取物品数据文件 u.item，指定分隔符为 '|'，编码为 latin-1，列名。 items_df: DataFrame，包含物品数据。\n",
        "                       names=['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url', 'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "ratings_df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp']) # 使用 pandas 读取评分数据文件 u.data，指定分隔符为制表符 '\\t'，列名。 ratings_df: DataFrame，包含评分数据。\n",
        "\n",
        "# 1. 特征工程\n",
        "user_encoder = LabelEncoder() # 创建 LabelEncoder 对象，用于用户ID编码。 user_encoder: LabelEncoder 对象。\n",
        "users_df['user_id_encoded'] = user_encoder.fit_transform(users_df['user_id']) # 使用 LabelEncoder 对用户ID进行编码，并将编码后的ID存储在新列 'user_id_encoded' 中。 users_df['user_id_encoded']: Series，用户ID的数值编码。\n",
        "users_df['gender_encoded'] = user_encoder.fit_transform(users_df['gender']) # 使用 LabelEncoder 对用户性别进行编码，并将编码后的性别存储在新列 'gender_encoded' 中。 users_df['gender_encoded']: Series，用户性别的数值编码。\n",
        "occupation_encoder = LabelEncoder() # 创建 LabelEncoder 对象，用于职业编码。 occupation_encoder: LabelEncoder 对象。\n",
        "users_df['occupation_encoded'] = occupation_encoder.fit_transform(users_df['occupation']) # 使用 LabelEncoder 对用户职业进行编码，并将编码后的职业存储在新列 'occupation_encoded' 中。 users_df['occupation_encoded']: Series，用户职业的数值编码。\n",
        "\n",
        "item_encoder = LabelEncoder() # 创建 LabelEncoder 对象，用于物品ID编码。 item_encoder: LabelEncoder 对象。\n",
        "items_df['item_id_encoded'] = item_encoder.fit_transform(items_df['item_id']) # 使用 LabelEncoder 对物品ID进行编码，并将编码后的ID存储在新列 'item_id_encoded' 中。 items_df['item_id_encoded']: Series，物品ID的数值编码。\n",
        "genre_cols = ['Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'] # 定义电影类型列名列表。 genre_cols: list，电影类型列名。\n",
        "items_df['genres_encoded'] = items_df[genre_cols].values.tolist() # 将电影类型列转换为列表形式，存储在新列 'genres_encoded' 中。 items_df['genres_encoded']: Series，电影类型特征的列表。\n",
        "\n",
        "# 2. 按用户和时间戳排序评分数据\n",
        "ratings_df = ratings_df.sort_values(by=['user_id', 'timestamp']) # 按照用户ID和时间戳对评分数据进行排序，为生成用户历史行为序列做准备。 ratings_df: DataFrame，按用户和时间戳排序的评分数据。\n",
        "\n",
        "# 3. 生成训练数据\n",
        "train_data = [] # 初始化训练数据列表。 train_data: list，存储训练样本的列表。\n",
        "item_pool_raw = items_df['item_id'].unique().tolist() # 获取所有物品ID的原始列表，用于负采样。 item_pool_raw: list，原始物品ID列表。\n",
        "item_pool_encoded = items_df['item_id_encoded'].unique().tolist() # 获取所有物品编码后ID的列表，用于负采样。 item_pool_encoded: list，编码后物品ID列表。\n",
        "\n",
        "for user_id_raw in ratings_df['user_id'].unique(): # 遍历每个用户。 user_id_raw: int，原始用户ID。\n",
        "    user_ratings = ratings_df[ratings_df['user_id'] == user_id_raw] # 获取当前用户的评分记录。 user_ratings: DataFrame，当前用户的评分记录。\n",
        "    history_item_ids = [] # 初始化用户历史物品ID列表，用于存储用户已交互过的物品。 history_item_ids: list，当前用户的历史物品ID列表。\n",
        "    rated_item_ids_encoded = [item_encoder.transform([item_id])[0] for item_id in user_ratings['item_id'].tolist()] # 获取当前用户评分过的物品的编码ID列表，用于负采样时的排除。 rated_item_ids_encoded: list，当前用户评分过的物品的编码ID列表。\n",
        "\n",
        "    for index, row in user_ratings.iterrows(): # 遍历当前用户的每条评分记录。 row: Series，当前评分记录。\n",
        "        user = users_df[users_df['user_id'] == row['user_id']].iloc[0] # 获取当前用户信息。 user: Series，当前用户信息。\n",
        "        item = items_df[items_df['item_id'] == row['item_id']].iloc[0] # 获取当前物品信息。 item: Series，当前物品信息。\n",
        "        timestamp = row['timestamp'] # 获取当前评分的时间戳。 timestamp: int，当前评分时间戳。\n",
        "        target_item_id_raw = row['item_id'] # 获取目标物品的原始ID。 target_item_id_raw: int，目标物品原始ID。\n",
        "        target_item_id_encoded = item['item_id_encoded'] # 获取目标物品的编码ID。 target_item_id_encoded: int，目标物品编码ID。\n",
        "\n",
        "        # 正样本\n",
        "        positive_sample = { # 构建正样本字典。 positive_sample: dict，正样本数据。\n",
        "            'user_id': user['user_id_encoded'], # 编码后的用户ID。\n",
        "            'user_gender': user['gender_encoded'], # 编码后的用户性别。\n",
        "            'user_age': user['age'], # 用户年龄。\n",
        "            'user_occupation': user['occupation_encoded'], # 编码后的用户职业。\n",
        "            'item_id': item['item_id_encoded'], # 编码后的物品ID。\n",
        "            'item_genres': item['genres_encoded'], # 物品类型特征列表。\n",
        "            'history_item_ids': [item_encoder.transform([h_item_id])[0] for h_item_id in history_item_ids], # 用户历史交互物品的编码ID列表。\n",
        "            'target_item_id': item['item_id_encoded'], # 目标物品的编码ID。\n",
        "            'label': 1.0 # 正样本标签为 1.0。\n",
        "        }\n",
        "        train_data.append(positive_sample) # 将正样本添加到训练数据列表。\n",
        "\n",
        "        # 正负样本比 1:1\n",
        "\n",
        "        # 负采样\n",
        "        negative_item_id_raw = random.choice(item_pool_raw) # 随机选择一个物品ID作为负样本。 negative_item_id_raw: int，随机选择的原始物品ID。\n",
        "        negative_item_id_encoded = item_encoder.transform([negative_item_id_raw])[0] # 将负样本物品ID编码。 negative_item_id_encoded: int，负样本物品编码ID。\n",
        "        while negative_item_id_encoded in rated_item_ids_encoded: # 确保负样本物品不在用户已评分物品列表中。\n",
        "            negative_item_id_raw = random.choice(item_pool_raw) # 重新随机选择负样本物品ID。\n",
        "            negative_item_id_encoded = item_encoder.transform([negative_item_id_raw])[0] # 重新编码负样本物品ID。\n",
        "\n",
        "        negative_item = items_df[items_df['item_id'] == negative_item_id_raw].iloc[0] # 获取负样本物品信息。 negative_item: Series，负样本物品信息。\n",
        "        negative_sample = { # 构建负样本字典。 negative_sample: dict，负样本数据。\n",
        "            'user_id': user['user_id_encoded'], # 编码后的用户ID。\n",
        "            'user_gender': user['gender_encoded'], # 编码后的用户性别。\n",
        "            'user_age': user['age'], # 用户年龄。\n",
        "            'user_occupation': user['occupation_encoded'], # 编码后的用户职业。\n",
        "            'item_id': negative_item['item_id_encoded'], # 负样本物品的编码ID。\n",
        "            'item_genres': negative_item['genres_encoded'], # 负样本物品的类型特征列表。\n",
        "            'history_item_ids':  [item_encoder.transform([h_item_id])[0] for h_item_id in history_item_ids], # 用户历史交互物品的编码ID列表。\n",
        "            'target_item_id': negative_item['item_id_encoded'], # 负样本物品的编码ID。\n",
        "            'label': 0.0 # 负样本标签为 0.0。\n",
        "        }\n",
        "        train_data.append(negative_sample) # 将负样本添加到训练数据列表。\n",
        "\n",
        "        # 防止特征穿越\n",
        "        history_item_ids.append(target_item_id_raw) # 将当前交互的物品ID添加到用户历史物品ID列表中，用于下一个样本的历史行为特征。\n",
        "\n",
        "# 4. 划分训练集和评估集\n",
        "train_list, eval_list = train_test_split(train_data, test_size=0.2, random_state=42, shuffle=True) # 将训练数据划分为训练集和评估集，评估集占 20%。 train_list: list，训练集样本列表。 eval_list: list，评估集样本列表。\n",
        "\n",
        "print(\"\\n处理后的训练样本示例 (包含负采样和时间戳约束):\") # 打印处理后的训练样本示例。\n",
        "print(train_list[0]) # 打印第一个训练样本。\n",
        "print(\"\\n训练集样本数量 (包含负采样和时间戳约束):\", len(train_list)) # 打印训练集样本数量。\n",
        "print(\"评估集样本数量 (包含负采样和时间戳约束):\", len(eval_list)) # 打印评估集样本数量。\n",
        "\n",
        "# 定义模型的超参数\n",
        "embedding_dim = 64 # 定义 Embedding 层的维度。 embedding_dim: int，Embedding 维度。\n",
        "hidden_units = [128, 64] # 定义全连接层的隐藏单元数列表。 hidden_units: list，全连接层隐藏单元数。\n",
        "num_categories = 50 # 定义类别数量。 num_categories: int，类别数量。\n",
        "\n",
        "# 注意力池化层的实现 (标准 MLP 注意力网络)\n",
        "class AttentionPoolingLayer(nn.Module): # 定义注意力池化层类。\n",
        "    def __init__(self, embedding_dim): # 初始化方法，接收 Embedding 维度作为参数。 embedding_dim: int，Embedding 维度。\n",
        "        super(AttentionPoolingLayer, self).__init__() # 调用父类初始化方法。\n",
        "        self.attention_fc = nn.Sequential( # 定义注意力全连接网络序列。 self.attention_fc: nn.Sequential，注意力 MLP 网络。\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim), # 第一个全连接层，输入维度为 Embedding 维度 * 2，输出维度为 Embedding 维度。\n",
        "            nn.ReLU(), # ReLU 激活函数。\n",
        "            nn.Linear(embedding_dim, 1) # 第二个全连接层，输入维度为 Embedding 维度，输出维度为 1。\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys): # 前向传播方法，接收 queries 和 keys 作为输入。 queries: Tensor，查询向量。 keys: Tensor，键向量。\n",
        "        queries = queries.unsqueeze(1) # 将 queries 扩展一个维度，用于后续计算注意力权重。 queries: Tensor，扩展维度后的查询向量。\n",
        "        queries = queries.expand(-1, keys.size(1), -1) # 将 queries 在 keys 的序列长度维度上进行扩展，以便与 keys 进行逐元素计算。 queries: Tensor，扩展后的查询向量，维度与 keys 匹配，维度为 (batch_size, seq_len, embedding_dim)。\n",
        "        attention_input = torch.cat([queries, keys], dim=-1) # 将 queries 和 keys 在最后一个维度上拼接，作为注意力网络的输入。 attention_input: Tensor，注意力网络的输入，维度为 (batch_size, seq_len, embedding_dim * 2)。\n",
        "        attention_scores = self.attention_fc(attention_input.view(-1, attention_input.size(-1))).view(keys.size(0), keys.size(1)) # 通过注意力网络计算注意力得分。 attention_scores: Tensor，注意力得分，维度为 (batch_size, sequence_length)。\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1) # 使用 Softmax 函数将注意力得分转换为注意力权重。 attention_weights: Tensor，注意力权重，维度为 (batch_size, sequence_length)。\n",
        "        attention_output = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1) # 使用注意力权重对 keys 进行加权求和，得到注意力池化层的输出。 attention_output: Tensor，注意力池化层的输出，维度为 (batch_size, embedding_dim)。\n",
        "        return attention_output # 返回注意力池化层的输出。\n",
        "\n",
        "# DIN 模型定义 (MovieLens 版本)\n",
        "class DIN(nn.Module): # 定义 DIN 模型类。\n",
        "    def __init__(self, num_users, num_genders, num_occupations, num_items, embedding_dim, hidden_units): # 初始化方法，接收用户数量、性别数量、职业数量、物品数量、Embedding 维度和隐藏单元数列表作为参数。 num_users: int，用户数量。 num_genders: int，性别数量。 num_occupations: int，职业数量。 num_items: int，物品数量。 embedding_dim: int，Embedding 维度。 hidden_units: list，全连接层隐藏单元数。\n",
        "        super(DIN, self).__init__() # 调用父类初始化方法。\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim) # 定义用户 ID Embedding 层。 self.user_embedding: nn.Embedding，用户 ID Embedding 层。\n",
        "        self.user_gender_embedding = nn.Embedding(num_genders, embedding_dim) # 定义用户性别 Embedding 层。 self.user_gender_embedding: nn.Embedding，用户性别 Embedding 层。\n",
        "        self.occupation_embedding = nn.Embedding(num_occupations, embedding_dim) # 定义用户职业 Embedding 层。 self.occupation_embedding: nn.Embedding，用户职业 Embedding 层。\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim) # 定义物品 ID Embedding 层。 self.item_embedding: nn.Embedding，物品 ID Embedding 层。\n",
        "        self.genres_fc = nn.Linear(len(genre_cols), embedding_dim) # 定义电影类型特征全连接层。 self.genres_fc: nn.Linear，电影类型特征全连接层。\n",
        "        self.attention_pooling = AttentionPoolingLayer(embedding_dim) # 定义注意力池化层。 self.attention_pooling: AttentionPoolingLayer，注意力池化层。\n",
        "\n",
        "        fc_layers = [] # 初始化全连接层列表。 fc_layers: list，全连接层列表。\n",
        "        input_dim = embedding_dim * 6 # 定义全连接层输入维度。 input_dim: int，全连接层输入维度。\n",
        "\n",
        "        for units in hidden_units: # 遍历隐藏单元数列表。 units: int，当前全连接层隐藏单元数。\n",
        "            fc_layers.append(nn.Linear(input_dim, units)) # 添加全连接层到列表。\n",
        "            fc_layers.append(nn.ReLU()) # 添加 ReLU 激活函数到列表。\n",
        "            input_dim = units # 更新下一层的输入维度。\n",
        "        fc_layers.append(nn.Linear(input_dim, 1)) # 添加最后一层全连接层，输出维度为 1。\n",
        "        self.fc = nn.Sequential(*fc_layers) # 将全连接层列表转换为序列模型。 self.fc: nn.Sequential，全连接层序列模型。\n",
        "\n",
        "    def forward(self, user_id, user_gender, user_age, user_occupation, item_id, item_genres, history_item_ids, target_item_id): # 定义模型前向传播方法。 user_id: Tensor，用户ID。 user_gender: Tensor，用户性别。 user_age: Tensor，用户年龄。 user_occupation: Tensor，用户职业。 item_id: Tensor，物品ID。 item_genres: Tensor，物品类型特征。 history_item_ids: Tensor，用户历史交互物品ID序列。 target_item_id: Tensor，目标物品ID。\n",
        "        user_embed = self.user_embedding(user_id) # 获取用户 ID Embedding。 user_embed: Tensor，用户 ID Embedding，维度为 (batch_size, embedding_dim)。\n",
        "        user_gender_embed = self.user_gender_embedding(user_gender) # 获取用户性别 Embedding。 user_gender_embed: Tensor，用户性别 Embedding，维度为 (batch_size, embedding_dim)。\n",
        "        occupation_embed = self.occupation_embedding(user_occupation) # 获取用户职业 Embedding。 occupation_embed: Tensor，用户职业 Embedding，维度为 (batch_size, embedding_dim)。\n",
        "        item_embed = self.item_embedding(item_id) # 获取物品 ID Embedding。 item_embed: Tensor，物品 ID Embedding，维度为 (batch_size, embedding_dim)。\n",
        "        target_item_embed = self.item_embedding(target_item_id) # 获取目标物品 ID Embedding。 target_item_embed: Tensor，目标物品 ID Embedding，维度为 (batch_size, embedding_dim)。\n",
        "        history_item_embed = self.item_embedding(history_item_ids) # 获取用户历史交互物品 ID Embedding 序列。 history_item_embed: Tensor，用户历史交互物品 ID Embedding 序列，维度为 (batch_size, sequence_length, embedding_dim)。\n",
        "\n",
        "        genres_embed = F.relu(self.genres_fc(item_genres)) # 通过全连接层处理物品类型特征。 genres_embed: Tensor，物品类型特征 Embedding，维度为 (batch_size, embedding_dim)。\n",
        "\n",
        "        attention_output = self.attention_pooling(queries=target_item_embed, keys=history_item_embed) # 通过注意力池化层计算用户历史行为的注意力加权表示。 attention_output: Tensor，注意力池化层的输出，维度为 (batch_size, embedding_dim)。\n",
        "\n",
        "        concat_features = torch.cat([user_embed, user_gender_embed, occupation_embed, item_embed, genres_embed, attention_output], dim=-1) # 将所有特征 Embedding 拼接在一起。 concat_features: Tensor，拼接后的特征向量，维度为 (batch_size, embedding_dim * 6)。\n",
        "\n",
        "        output = self.fc(concat_features) # 通过全连接层序列模型进行预测。 output: Tensor，模型输出，维度为 (batch_size, 1)。\n",
        "        output = torch.sigmoid(output) # 使用 Sigmoid 激活函数将输出转换为概率值。 output: Tensor，模型输出概率，维度为 (batch_size, 1)。\n",
        "        return output.squeeze(1) # 移除输出维度为 1 的维度，得到最终预测概率。 output.squeeze(1): Tensor，最终预测概率，维度为 (batch_size)。\n",
        "\n",
        "# 初始化 DIN 模型 (MovieLens 版本)\n",
        "num_users = users_df['user_id_encoded'].nunique() # 获取用户数量。 num_users: int，用户数量。\n",
        "num_genders = users_df['gender_encoded'].nunique() # 获取性别数量。 num_genders: int，性别数量。\n",
        "num_occupations = users_df['occupation_encoded'].nunique() # 获取职业数量。 num_occupations: int，职业数量。\n",
        "num_items = items_df['item_id_encoded'].nunique() # 获取物品数量。 num_items: int，物品数量。\n",
        "model = DIN(num_users, num_genders, num_occupations, num_items, embedding_dim, hidden_units) # 初始化 DIN 模型。 model: DIN 对象，DIN 模型实例。\n",
        "\n",
        "# 损失函数和优化器\n",
        "loss_fn = nn.BCELoss() # 定义二元交叉熵损失函数。 loss_fn: BCELoss 对象，二元交叉熵损失函数。\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 定义 Adam 优化器，用于模型参数优化。 optimizer: Adam 对象，Adam 优化器。\n",
        "\n",
        "# 生成 batch 数据的函数 (MovieLens 版本)\n",
        "def generate_sample_data(data_list, item_pool, batch_size=64, start_index=0, neg_ratio=1): # 定义生成 batch 数据的函数。 data_list: list，数据集列表。 item_pool: list，物品池列表。 batch_size: int，batch 大小。 start_index: int，batch 起始索引。 neg_ratio: int，负采样比例。\n",
        "    batch_data = data_list[start_index:start_index + batch_size] # 获取当前 batch 的数据。 batch_data: list，当前 batch 的样本列表。\n",
        "    batch_samples = [] # 初始化 batch 样本列表。 batch_samples: list，当前 batch 的样本列表。\n",
        "\n",
        "    for sample in batch_data: # 遍历当前 batch 的每个样本。 sample: dict，当前样本数据。\n",
        "        # 正样本\n",
        "        positive_sample = sample.copy() # 复制正样本。 positive_sample: dict，正样本数据副本。\n",
        "        positive_sample['label'] = 1.0 # 设置正样本标签为 1.0。\n",
        "        batch_samples.append(positive_sample) # 添加正样本到 batch 样本列表。\n",
        "\n",
        "        # 负采样\n",
        "        for _ in range(neg_ratio): # 根据负采样比例进行负采样。 _: int，循环计数器。\n",
        "            negative_sample = sample.copy() # 复制正样本作为负样本的基础。 negative_sample: dict，负样本数据副本。\n",
        "            negative_item_id_encoded = random.choice(item_pool) # 从物品池中随机选择一个物品ID作为负样本。 negative_item_id_encoded: int，随机选择的物品编码ID。\n",
        "            while negative_item_id_encoded in sample['history_item_ids'] or negative_item_id_encoded == sample['target_item_id']: # 确保负样本物品不在用户历史交互物品中，且不是正样本物品。\n",
        "                negative_item_id_encoded = random.choice(item_pool) # 重新随机选择负样本物品ID。\n",
        "\n",
        "            negative_sample['item_id'] = negative_item_id_encoded # 设置负样本的物品ID。\n",
        "            negative_sample['target_item_id'] = negative_item_id_encoded # 设置负样本的目标物品ID。\n",
        "            negative_sample['label'] = 0.0 # 设置负样本标签为 0.0。\n",
        "            batch_samples.append(negative_sample) # 添加负样本到 batch 样本列表。\n",
        "\n",
        "    user_ids = torch.LongTensor([sample['user_id'] for sample in batch_samples]) # 从 batch 样本中提取用户ID，并转换为 LongTensor。 user_ids: Tensor，用户ID，维度为 (batch_size * (1 + neg_ratio))。\n",
        "    user_genders = torch.LongTensor([sample['user_gender'] for sample in batch_samples]) # 从 batch 样本中提取用户性别，并转换为 LongTensor。 user_genders: Tensor，用户性别，维度为 (batch_size * (1 + neg_ratio))。\n",
        "    user_ages = torch.FloatTensor([[sample['user_age']] for sample in batch_samples]) # 从 batch 样本中提取用户年龄，并转换为 FloatTensor。 user_ages: Tensor，用户年龄，维度为 (batch_size * (1 + neg_ratio), 1)。\n",
        "    user_occupations = torch.LongTensor([sample['user_occupation'] for sample in batch_samples]) # 从 batch 样本中提取用户职业，并转换为 LongTensor。 user_occupations: Tensor，用户职业，维度为 (batch_size * (1 + neg_ratio))。\n",
        "    item_ids = torch.LongTensor([sample['item_id'] for sample in batch_samples]) # 从 batch 样本中提取物品ID，并转换为 LongTensor。 item_ids: Tensor，物品ID，维度为 (batch_size * (1 + neg_ratio))。\n",
        "    item_genres_list = [sample['item_genres'] for sample in batch_samples] # 从 batch 样本中提取物品类型特征列表。 item_genres_list: list，物品类型特征列表。\n",
        "    item_genres = torch.FloatTensor(item_genres_list) # 将物品类型特征列表转换为 FloatTensor。 item_genres: Tensor，物品类型特征，维度为 (batch_size * (1 + neg_ratio), len(genre_cols))。\n",
        "    history_item_ids_list = [sample['history_item_ids'] for sample in batch_samples] # 从 batch 样本中提取用户历史交互物品ID列表。 history_item_ids_list: list，用户历史交互物品ID列表。\n",
        "    target_item_ids = torch.LongTensor([sample['target_item_id'] for sample in batch_samples]) # 从 batch 样本中提取目标物品ID，并转换为 LongTensor。 target_item_ids: Tensor，目标物品ID，维度为 (batch_size * (1 + neg_ratio))。\n",
        "    labels = torch.FloatTensor([sample['label'] for sample in batch_samples]) # 从 batch 样本中提取标签，并转换为 FloatTensor。 labels: Tensor，标签，维度为 (batch_size * (1 + neg_ratio))。\n",
        "\n",
        "    max_len = max([len(seq) for seq in history_item_ids_list]) if history_item_ids_list else 0 # 获取历史交互物品ID列表的最大长度，用于 padding。 max_len: int，最大序列长度。\n",
        "    padded_history_item_ids = torch.zeros((len(batch_samples), max_len), dtype=torch.long) # 初始化 padding 后的历史交互物品ID Tensor。 padded_history_item_ids: Tensor，padding 后的历史交互物品ID，维度为 (batch_size * (1 + neg_ratio), max_len)。\n",
        "    for i, seq in enumerate(history_item_ids_list): # 遍历历史交互物品ID列表。 i: int，样本索引。 seq: list，当前样本的历史交互物品ID列表。\n",
        "        padded_history_item_ids[i, :len(seq)] = torch.LongTensor(seq) # 将历史交互物品ID填充到 padding 后的 Tensor 中。\n",
        "\n",
        "    return { # 返回 batch 数据字典。\n",
        "        'user_id': user_ids, # 用户ID。\n",
        "        'user_gender': user_genders, # 用户性别。\n",
        "        'user_age': user_ages, # 用户年龄。\n",
        "        'user_occupation': user_occupations, # 用户职业。\n",
        "        'item_id': item_ids, # 物品ID。\n",
        "        'item_genres': item_genres, # 物品类型特征。\n",
        "        'history_item_ids': padded_history_item_ids, # padding 后的历史交互物品ID。\n",
        "        'target_item_id': target_item_ids, # 目标物品ID。\n",
        "        'label': labels # 标签。\n",
        "    }\n",
        "\n",
        "\n",
        "# 训练循环 (MovieLens 版本)\n",
        "num_epochs = 10 # 定义训练 epoch 数。 num_epochs: int，训练 epoch 数。\n",
        "batch_size = 64 # 定义 batch 大小。 batch_size: int，batch 大小。\n",
        "neg_ratio = 1 # 定义负采样比例。 neg_ratio: int，负采样比例。\n",
        "train_batch_num = len(train_list) // batch_size # 计算训练 batch 数量。 train_batch_num: int，训练 batch 数量。\n",
        "\n",
        "for epoch in range(num_epochs): # 遍历 epoch。 epoch: int，当前 epoch 索引。\n",
        "    model.train() # 将模型设置为训练模式。\n",
        "    total_loss = 0.0 # 初始化总损失。 total_loss: float，总损失。\n",
        "    for batch_idx in range(train_batch_num): # 遍历训练 batch。 batch_idx: int，batch 索引。\n",
        "        train_batch_data = generate_sample_data(train_list, item_pool_encoded, batch_size=batch_size, start_index=batch_idx * batch_size, neg_ratio=neg_ratio) # 生成训练 batch 数据。 train_batch_data: dict，训练 batch 数据字典。\n",
        "        labels = train_batch_data['label'] # 获取训练 batch 的标签。 labels: Tensor，训练 batch 标签。\n",
        "\n",
        "        optimizer.zero_grad() # 清空优化器梯度。\n",
        "        predictions = model(train_batch_data['user_id'], # 模型预测。 predictions: Tensor，模型预测概率，维度为 (batch_size * (1 + neg_ratio))。\n",
        "                          train_batch_data['user_gender'],\n",
        "                          train_batch_data['user_age'],\n",
        "                          train_batch_data['user_occupation'],\n",
        "                          train_batch_data['item_id'],\n",
        "                          train_batch_data['item_genres'],\n",
        "                          train_batch_data['history_item_ids'],\n",
        "                          train_batch_data['target_item_id'])\n",
        "        loss = loss_fn(predictions, labels) # 计算训练 batch 的损失。 loss: Tensor，训练 batch 损失。\n",
        "        loss.backward() # 反向传播计算梯度。\n",
        "        optimizer.step() # 使用优化器更新模型参数。\n",
        "        total_loss += loss.item() # 累加训练损失。\n",
        "\n",
        "    avg_loss = total_loss / train_batch_num # 计算平均训练损失。 avg_loss: float，平均训练损失。\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\") # 打印当前 epoch 的平均训练损失。\n",
        "\n",
        "print(\"训练完成!\") # 打印训练完成信息。\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWVikv9pFqT5",
        "outputId": "0db18d4d-1558-4778-b430-fcc40b7e8b42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "处理后的训练样本示例 (包含负采样和时间戳约束):\n",
            "{'user_id': 704, 'user_gender': 0, 'user_age': 21, 'user_occupation': 18, 'item_id': 180, 'item_genres': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0], 'history_item_ids': [285, 299, 297], 'target_item_id': 180, 'label': 1.0}\n",
            "\n",
            "训练集样本数量 (包含负采样和时间戳约束): 160000\n",
            "评估集样本数量 (包含负采样和时间戳约束): 40000\n",
            "Epoch [1/10], Average Loss: 0.4993\n",
            "Epoch [2/10], Average Loss: 0.1792\n",
            "Epoch [3/10], Average Loss: 0.1411\n",
            "Epoch [4/10], Average Loss: 0.1275\n",
            "Epoch [5/10], Average Loss: 0.1196\n",
            "Epoch [6/10], Average Loss: 0.1130\n",
            "Epoch [7/10], Average Loss: 0.1100\n",
            "Epoch [8/10], Average Loss: 0.1078\n",
            "Epoch [9/10], Average Loss: 0.1054\n",
            "Epoch [10/10], Average Loss: 0.1030\n",
            "训练完成!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估循环 (MovieLens 版本)\n",
        "def evaluate_model(model, eval_data_list, item_pool, batch_size=64, neg_ratio=1): # 定义模型评估函数。 model: DIN 对象，DIN 模型实例。 eval_data_list: list，评估数据集列表。 item_pool: list，物品池列表。 batch_size: int，batch 大小。 neg_ratio: int，负采样比例。\n",
        "    model.eval() # 将模型设置为评估模式。\n",
        "    all_labels = [] # 初始化所有真实标签列表。 all_labels: list，所有真实标签。\n",
        "    all_predictions = [] # 初始化所有预测概率列表。 all_predictions: list，所有预测概率。\n",
        "    total_eval_loss = 0.0 # 初始化评估损失。 total_eval_loss: float，总评估损失。\n",
        "    eval_batch_num = len(eval_data_list) // batch_size # 计算评估 batch 数量。 eval_batch_num: int，评估 batch 数量。\n",
        "\n",
        "    with torch.no_grad(): # 在评估过程中禁用梯度计算。\n",
        "        for batch_idx in range(eval_batch_num): # 遍历评估 batch。 batch_idx: int，batch 索引。\n",
        "            eval_batch_data = generate_sample_data(eval_data_list, item_pool, batch_size=batch_size, start_index=batch_idx * batch_size, neg_ratio=neg_ratio) # 生成评估 batch 数据。 eval_batch_data: dict，评估 batch 数据字典。\n",
        "            labels = eval_batch_data['label'] # 获取评估 batch 的标签。 labels: Tensor，评估 batch 标签。\n",
        "            predictions = model(eval_batch_data['user_id'], # 模型预测。 predictions: Tensor，模型预测概率，维度为 (batch_size * (1 + neg_ratio))。\n",
        "                              eval_batch_data['user_gender'],\n",
        "                              eval_batch_data['user_age'],\n",
        "                              eval_batch_data['user_occupation'],\n",
        "                              eval_batch_data['item_id'],\n",
        "                              eval_batch_data['item_genres'],\n",
        "                              eval_batch_data['history_item_ids'],\n",
        "                              eval_batch_data['target_item_id'])\n",
        "\n",
        "            eval_loss = loss_fn(predictions, labels) # 计算评估 batch 的损失。 eval_loss: Tensor，评估 batch 损失。\n",
        "            total_eval_loss += eval_loss.item() # 累加评估损失。\n",
        "            all_labels.extend(labels.cpu().numpy()) # 将评估 batch 的真实标签添加到列表中。\n",
        "            all_predictions.extend(predictions.cpu().numpy()) # 将评估 batch 的预测概率添加到列表中。\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / eval_batch_num # 计算平均评估损失。 avg_eval_loss: float，平均评估损失。\n",
        "    auc_score = roc_auc_score(all_labels, all_predictions) # 计算 AUC 值。 auc_score: float，AUC 值。\n",
        "    logloss_score = log_loss(torch.tensor(all_labels), torch.tensor(all_predictions)) # 计算 LogLoss 值。 logloss_score: float，LogLoss 值。\n",
        "\n",
        "    print(f\"Evaluation - Average Loss: {avg_eval_loss:.4f}, AUC: {auc_score:.4f}, LogLoss: {logloss_score:.4f}\") # 打印评估指标。\n",
        "    return all_labels, all_predictions, avg_eval_loss, auc_score, logloss_score # 返回所有真实标签、预测概率、平均评估损失、AUC 值和 LogLoss 值。\n",
        "\n",
        "\n",
        "# 在训练完成后进行评估\n",
        "print(\"开始评估...\") # 打印开始评估信息。\n",
        "all_labels, all_predictions, avg_eval_loss, auc_score, logloss_score = evaluate_model(model, eval_list, item_pool_encoded, batch_size=batch_size, neg_ratio=neg_ratio) # 调用评估函数进行模型评估。 all_labels: list，所有真实标签。 all_predictions: list，所有预测概率。 avg_eval_loss: float，平均评估损失。 auc_score: float，AUC 值。 logloss_score: float，LogLoss 值。\n",
        "print(\"评估完成!\") # 打印评估完成信息。\n",
        "\n",
        "# 绘制 ROC 曲线\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_predictions) # 计算 ROC 曲线的 FPR 和 TPR。 fpr: ndarray，假正率。 tpr: ndarray，真正率。 thresholds: ndarray，阈值。\n",
        "plt.figure(figsize=(8, 6)) # 创建 figure 对象，设置大小。 plt.figure: Figure 对象。\n",
        "plt.plot(fpr, tpr, label=f'DIN AUC = {auc_score:.4f}') # 绘制 ROC 曲线，并添加 AUC 值标签。\n",
        "plt.plot([0, 1], [0, 1], 'k--') # 绘制对角线作为随机分类器的基准。\n",
        "plt.xlabel('False Positive Rate') # 设置 x 轴标签。\n",
        "plt.ylabel('True Positive Rate') # 设置 y 轴标签。\n",
        "plt.title('ROC Curve') # 设置图表标题。\n",
        "plt.legend(loc='lower right') # 显示图例，并设置位置在右下角。\n",
        "plt.show() # 显示 ROC 曲线图。\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "feCeCWoe4z-h",
        "outputId": "c672c894-e29c-492d-d1f9-3f0c6a99e750"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "开始评估...\n",
            "Evaluation - Average Loss: 0.1043, AUC: 0.9888, LogLoss: 0.1043\n",
            "评估完成!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}