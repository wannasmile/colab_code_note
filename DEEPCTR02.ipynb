{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+mHC+lxXj+OOPTzb7ZKIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wannasmile/colab_code_note/blob/main/DEEPCTR02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "深度兴趣网络（Deep Interest Network, DIN）是阿里巴巴在2018年提出的一种用于点击率预测（CTR）的深度学习模型。其核心思想可以用一个生活化的比喻来理解：想象你是一位网购爱好者，平台需要根据你过去浏览的商品（比如运动鞋、咖啡机、小说）来猜测你现在可能对哪个广告感兴趣。传统方法就像把所有历史行为“一视同仁”地分析，但DIN却能像人类一样，**动态关注与当前广告最相关的行为**。例如，当你看到一款新跑鞋广告时，DIN会重点参考你过去浏览运动鞋的记录，而忽略咖啡机和小说这类无关行为。\n",
        "\n",
        "### 一、DIN解决了什么问题？\n",
        "点击率预测（CTR）是推荐系统和广告投放的核心任务，目标是预测用户点击某个内容（如广告、商品）的概率。传统模型（如逻辑回归、因子分解机）存在两大局限：\n",
        "1. **兴趣建模僵化**：用户的历史行为被压缩成固定长度的向量，无法灵活表达多样的兴趣。例如，用户可能同时喜欢“运动”和“文学”，但传统模型难以区分这两种兴趣在不同场景下的权重。\n",
        "2. **噪声干扰**：用户行为中混杂大量无关历史（例如误点或随意浏览），传统模型无法有效过滤这些噪声。\n",
        "\n",
        "### 二、DIN的核心创新：像人一样“动态关注”\n",
        "DIN通过以下三个关键设计解决上述问题：\n",
        "1. **注意力机制（Attention）**  \n",
        "   这是DIN的灵魂。模型会为每个用户行为计算一个“相关性权重”：与当前广告越相关的行为，权重越高。例如，用户的历史行为包括“运动鞋、咖啡机、小说”，当预测跑鞋广告的点击率时，模型会给“运动鞋”行为赋予高权重，而“咖啡机”和“小说”的权重则很低。这就像人类看到广告时，只会回想相关的购买经历。\n",
        "\n",
        "2. **自适应激活函数（Dice）**  \n",
        "   传统激活函数（如ReLU）的阈值是固定的，但DIN的Dice函数能根据数据分布动态调整阈值。例如，当用户行为数据差异较大时，Dice会自动适应不同场景，提升模型的灵活性。\n",
        "\n",
        "3. **高效正则化（Mini-batch Aware Regularization）**  \n",
        "   面对海量数据，传统正则化方法计算成本极高。DIN只对当前训练批次（mini-batch）中出现过的特征进行正则化，既防止过拟合，又大幅减少计算量。\n",
        "\n",
        "### 三、DIN的实际效果如何？\n",
        "实验表明，DIN在多个场景下显著优于传统模型：\n",
        "- **离线测试**：在亚马逊和MovieLens数据集上，DIN的AUC（衡量预测准确性的指标）比传统模型（如Wide&Deep、PNN）提升约1.89%。\n",
        "- **线上应用**：在阿里巴巴广告系统中，DIN使点击率（CTR）提升10%，广告收入增长3.8%。这意味着每展示100次广告，DIN能多带来1次点击，这在亿级流量场景下效益巨大。\n",
        "\n",
        "### 四、DIN的启示：从“静态画像”到“动态兴趣”\n",
        "DIN的成功揭示了推荐系统的未来方向：**用户的兴趣是多样且动态变化的**。与其用固定标签定义用户（如“运动爱好者”），不如根据具体场景实时捕捉兴趣焦点。这种思路也被后续模型（如DIEN）进一步扩展，加入了兴趣演化的时序建模。\n",
        "\n",
        "总结来说，DIN的核心思想是 **“动态相关性”** ——让模型像人一样，在不同场景下灵活关注最相关的历史行为，从而更精准地预测用户的点击意愿。这一创新不仅提升了技术指标，也推动了推荐系统从“粗放推荐”向“智能理解”的跨越。"
      ],
      "metadata": {
        "id": "6o_E1abjBxWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 生成一个包含 10 个随机整数的一维张量，范围是 [0, 10)\n",
        "random_tensor = torch.randint(0, 10, (10,))\n",
        "print(random_tensor)\n",
        "\n",
        "# 生成一个 3x2 的二维张量，范围是 [1, 5)\n",
        "random_tensor_2d = torch.randint(1, 5, (3, 2))\n",
        "print(random_tensor_2d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULNPxiIV9aRC",
        "outputId": "d37adc3d-d5af-4501-fe5e-97e22e9f8042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 9, 8, 6, 5, 7, 1, 7, 8, 8])\n",
            "tensor([[3, 4],\n",
            "        [4, 3],\n",
            "        [2, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**unsqueeze(1) 的作用？**\n",
        "\n",
        "unsqueeze(1) 是 PyTorch 张量的一个方法，用于在指定维度上增加一个维度。1 表示在第二个维度（索引为 1）上增加维度。\n",
        "\n",
        "例如，假设有一个形状为 (64,) 的张量 user_ages，表示 64 个用户的年龄。执行 user_ages.unsqueeze(1) 后，张量的形状会变成 (64, 1)。相当于把原来的一维张量变成了一个二维张量，其中每个元素都被放在一个单独的行中。\n",
        "\n",
        "**为什么需要增加一个维度？**\n",
        "\n",
        "在深度学习模型中，输入数据的维度通常需要满足特定的要求。例如，在 DIN 模型中，用户的年龄和商品价格都是数值型特征，通常会使用一个线性层来处理这些特征。线性层的输入需要是一个二维张量，其中第一维表示样本数量，第二维表示特征维度。\n",
        "\n",
        "因此，如果用户的年龄或商品价格只有一维，就需要使用 unsqueeze(1) 在第二个维度上增加一个维度，使其变成一个二维张量，以便与线性层的输入维度匹配。"
      ],
      "metadata": {
        "id": "Sis9aReW_rSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 定义模型的超参数\n",
        "embedding_dim = 32  # Embedding 向量的维度，每个用户/商品/类别都会被映射到一个 32 维的向量\n",
        "hidden_units = [64, 32]  # 全连接层隐藏单元数量，表示网络中两个隐藏层的节点数\n",
        "num_users = 1000  # 用户总数\n",
        "num_items = 2000  # 商品总数\n",
        "num_categories = 10  # 商品类别总数\n",
        "\n",
        "\n",
        "# 定义一个函数来生成模拟数据\n",
        "def generate_sample_data(batch_size=64):\n",
        "    # 生成用户 ID，范围在 [0, num_users) 之间，形状为 (batch_size,)\n",
        "    user_ids = torch.randint(0, num_users, (batch_size,))\n",
        "    # 生成用户性别，0 或 1，形状为 (batch_size,)\n",
        "    user_genders = torch.randint(0, 2, (batch_size,))\n",
        "    # 生成用户年龄，范围在 [18, 60) 之间，形状为 (batch_size,)，并转换为浮点数\n",
        "    user_ages = torch.randint(18, 60, (batch_size,)).float()\n",
        "\n",
        "    # 生成商品 ID，范围在 [0, num_items) 之间，形状为 (batch_size,)\n",
        "    item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成商品类别，范围在 [0, num_categories) 之间，形状为 (batch_size,)\n",
        "    item_categories = torch.randint(0, num_categories, (batch_size,))\n",
        "    # 生成商品价格，范围在 [0, 1000) 之间，形状为 (batch_size,)\n",
        "    item_prices = torch.rand(batch_size) * 1000\n",
        "\n",
        "    # 生成用户的历史行为序列，每个用户随机浏览 1-5 个商品\n",
        "    history_item_ids_list = []\n",
        "    for _ in range(batch_size):\n",
        "        # 随机生成序列长度，范围在 [1, 6) 之间\n",
        "        seq_len = torch.randint(1, 6, (1,)).item()\n",
        "        # 生成历史行为序列，范围在 [0, num_items) 之间，形状为 (seq_len,)\n",
        "        history_seq = torch.randint(0, num_items, (seq_len,))\n",
        "        # 将生成的序列添加到列表中\n",
        "        history_item_ids_list.append(history_seq)\n",
        "\n",
        "    # 将历史行为序列填充到相同长度 (最长序列长度)，并转换为张量\n",
        "    # 获取最长序列长度\n",
        "    max_len = max([seq.size(0) for seq in history_item_ids_list])\n",
        "    # 创建一个全零张量，形状为 (batch_size, max_len)，用于存储填充后的序列\n",
        "    padded_history_item_ids = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    # 遍历每个用户的历史行为序列\n",
        "    for i, seq in enumerate(history_item_ids_list):\n",
        "        # 将序列填充到 padded_history_item_ids 中\n",
        "        padded_history_item_ids[i, :seq.size(0)] = seq\n",
        "\n",
        "    # 生成目标商品 ID，范围在 [0, num_items) 之间，形状为 (batch_size,)\n",
        "    target_item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成点击标签，0 或 1，形状为 (batch_size,)，并转换为浮点数\n",
        "    labels = torch.randint(0, 2, (batch_size,)).float()\n",
        "\n",
        "    # 将所有数据打包成一个字典返回\n",
        "    return {\n",
        "        'user_id': user_ids,\n",
        "        'user_gender': user_genders,\n",
        "        'user_age': user_ages.unsqueeze(1),  # 增加一个维度，以便与模型输入匹配\n",
        "        'item_id': item_ids,\n",
        "        'item_category': item_categories,\n",
        "        'item_price': item_prices.unsqueeze(1),  # 增加一个维度，以便与模型输入匹配\n",
        "        'history_item_ids': padded_history_item_ids,\n",
        "        'target_item_id': target_item_ids,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# 调用 generate_sample_data 函数生成一个 batch 的数据\n",
        "sample_data = generate_sample_data()\n",
        "# 打印示例数据的键值，以便查看数据结构\n",
        "print(\"示例数据：\", sample_data.keys())\n",
        "# 打印历史行为序列的形状，以便查看数据维度\n",
        "print(\"历史行为序列形状:\", sample_data['history_item_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcJ_mWPW7N6S",
        "outputId": "62160fcb-055c-479c-b1fa-5d15248872b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "示例数据： dict_keys(['user_id', 'user_gender', 'user_age', 'item_id', 'item_category', 'item_price', 'history_item_ids', 'target_item_id', 'label'])\n",
            "历史行为序列形状: torch.Size([64, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Embedding 的工作原理**\n",
        "\n",
        "nn.Embedding 层的作用是将离散的特征（例如商品 ID）映射成连续的向量表示（embedding 向量）。它内部维护一个查找表，表中的每一行代表一个特征的 embedding 向量。\n",
        "\n",
        "当我们输入一个特征 ID 时，nn.Embedding 层会根据这个 ID 在查找表中找到对应的 embedding 向量并返回。\n",
        "\n",
        "**处理不同维度输入的关键**\n",
        "\n",
        "nn.Embedding 层可以处理任意形状的整数张量作为输入。它会将输入张量中的每个元素都视为一个特征 ID，并根据 ID 查找对应的 embedding 向量。\n",
        "\n",
        "处理 item_id: item_id 的形状是 (batch_size,)，它包含了每个样本的商品 ID。nn.Embedding 层会将 item_id 中的每个 ID 都映射成一个 embedding 向量，最终返回一个形状为 (batch_size, embedding_dim) 的张量。\n",
        "\n",
        "处理 history_item_ids: history_item_ids 的形状是 (batch_size, seq_len)，它包含了每个样本的历史行为序列，序列中的每个元素都是一个商品 ID。nn.Embedding 层会将 history_item_ids 中的每个 ID 都映射成一个 embedding 向量，最终返回一个形状为 (batch_size, seq_len, embedding_dim) 的张量。\n",
        "\n",
        "**总结**\n",
        "\n",
        "nn.Embedding 层能够处理不同维度的输入，因为它会将输入张量中的每个元素都视为一个特征 ID，并根据 ID 查找对应的 embedding 向量。\n",
        "\n",
        "因此，即使 item_id 和 history_item_ids 的维度不同，item_embedding 函数仍然可以处理它们，并返回相应形状的 embedding 向量张量。"
      ],
      "metadata": {
        "id": "KGZ_Iv5kMavS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DIN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_categories, embedding_dim, hidden_units):\n",
        "        super(DIN, self).__init__()\n",
        "\n",
        "        # 创建用户嵌入层，将用户ID映射到embedding_dim维度的向量空间\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # 创建商品嵌入层，将商品ID映射到embedding_dim维度的向量空间\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # 创建类别嵌入层，将类别ID映射到embedding_dim维度的向量空间\n",
        "        self.category_embedding = nn.Embedding(num_categories, embedding_dim)\n",
        "\n",
        "        # 创建数值型特征的线性层，将用户年龄和商品价格映射到embedding_dim维度的向量空间\n",
        "        self.numerical_fc = nn.Linear(2, embedding_dim)\n",
        "\n",
        "        # 创建注意力池化层，用于根据目标商品对历史行为进行加权\n",
        "        self.attention_pooling = AttentionPoolingLayer(embedding_dim)\n",
        "\n",
        "        # 创建全连接层，用于最终的点击率预测\n",
        "        fc_layers = []\n",
        "        # 初始化全连接层的输入维度，包括用户、商品、类别、数值型特征和注意力输出的embedding维度之和\n",
        "        input_dim = embedding_dim * 4 + embedding_dim\n",
        "        # 逐层构建全连接层，并使用ReLU作为激活函数\n",
        "        for units in hidden_units:\n",
        "            fc_layers.append(nn.Linear(input_dim, units))\n",
        "            fc_layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        # 添加最后一层全连接层，输出维度为1，表示点击率预测值\n",
        "        fc_layers.append(nn.Linear(input_dim, 1))\n",
        "        # 将所有全连接层组合成一个序列\n",
        "        self.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, user_id, user_gender, user_age, item_id, item_category, item_price, history_item_ids, target_item_id):\n",
        "        \"\"\"\n",
        "        DIN模型的前向传播函数\n",
        "\n",
        "        输入:\n",
        "            user_id: 用户ID, 形状: (batch_size,)\n",
        "            user_gender: 用户性别, 形状: (batch_size,)\n",
        "            user_age: 用户年龄, 形状: (batch_size, 1)\n",
        "            item_id: 商品ID, 形状: (batch_size,)\n",
        "            item_category: 商品类别, 形状: (batch_size,)\n",
        "            item_price: 商品价格, 形状: (batch_size, 1)\n",
        "            history_item_ids: 用户历史行为序列, 形状: (batch_size, seq_len)\n",
        "            target_item_id: 目标商品ID, 形状: (batch_size,)\n",
        "\n",
        "        输出:\n",
        "            output: 点击率预测值, 形状: (batch_size,)\n",
        "        \"\"\"\n",
        "        # 1. 获取各个特征的嵌入向量\n",
        "        # 获取用户嵌入向量\n",
        "        user_embed = self.user_embedding(user_id)\n",
        "        # 获取商品嵌入向量\n",
        "        item_embed = self.item_embedding(item_id)\n",
        "        # 获取类别嵌入向量\n",
        "        category_embed = self.category_embedding(item_category)\n",
        "        # 获取目标商品嵌入向量\n",
        "        target_item_embed = self.item_embedding(target_item_id)\n",
        "        # 获取历史行为序列嵌入向量\n",
        "        history_item_embed = self.item_embedding(history_item_ids)\n",
        "\n",
        "\n",
        "        # 2. 处理数值型特征\n",
        "        # 将用户年龄和商品价格拼接成一个张量\n",
        "        numerical_features = torch.cat([user_age, item_price], dim=-1)\n",
        "        # 使用线性层和ReLU激活函数将数值型特征映射到embedding_dim维度的向量空间\n",
        "        numerical_embed = F.relu(self.numerical_fc(numerical_features))\n",
        "\n",
        "        # 3. 使用注意力池化层对历史行为进行加权\n",
        "        # 使用注意力机制，根据目标商品，对用户的历史行为序列进行加权聚合，得到用户的兴趣表示\n",
        "        attention_output = self.attention_pooling(queries=target_item_embed, keys=history_item_embed)\n",
        "\n",
        "        # 4. 将所有特征拼接在一起\n",
        "        # 将用户、商品、类别、数值型特征和注意力输出的embedding向量拼接在一起\n",
        "        concat_features = torch.cat([user_embed, item_embed, numerical_embed, attention_output, target_item_embed], dim=-1)\n",
        "\n",
        "        # 5. 通过全连接层进行预测\n",
        "        # 将拼接后的特征向量输入到全连接层，得到预测结果\n",
        "        output = self.fc(concat_features)\n",
        "        # 使用 sigmoid 激活函数将输出转换为点击率 (0-1)\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        # 返回预测结果，并移除维度为 1 的维度\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "# 注意力池化层的实现（！存在问题！）\n",
        "class AttentionPoolingLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(AttentionPoolingLayer, self).__init__()\n",
        "        # 创建一个线性层，将 embedding 向量映射到注意力权重\n",
        "        self.attention_fc = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, queries, keys):\n",
        "        \"\"\"\n",
        "        前向传播函数，计算注意力权重并进行加权池化\n",
        "        :param queries: 目标商品的嵌入向量 (batch_size, embedding_dim)\n",
        "        :param keys: 历史行为序列的嵌入向量 (batch_size, seq_len, embedding_dim)\n",
        "        :return: 注意力池化后的用户兴趣表示 (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. 计算注意力分数\n",
        "        # 扩展目标商品嵌入向量的维度，以便与历史行为序列嵌入向量进行元素乘法\n",
        "        queries = queries.unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
        "        # 计算目标商品与每个历史行为商品的相似度，作为注意力分数\n",
        "        attention_scores = torch.sum(queries * keys, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 2. 使用全连接层进一步学习注意力分数\n",
        "        # 将历史行为序列嵌入向量进行reshape，然后通过线性层进行变换，得到新的注意力分数\n",
        "        attention_scores = self.attention_fc(keys.view(-1, keys.size(-1))).view(keys.size(0), keys.size(1))  # (batch_size, seq_len)\n",
        "\n",
        "        # 3. 使用 Softmax 归一化注意力分数，得到注意力权重\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "        # 4. 加权求和得到注意力池化后的结果\n",
        "        # 扩展注意力权重的维度，与历史行为序列嵌入向量相乘，然后在序列维度上求和\n",
        "        attention_output = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1)  # (batch_size, embedding_dim)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "# 初始化 DIN 模型\n",
        "model = DIN(num_users, num_items, num_categories, embedding_dim, hidden_units)\n",
        "print(model) # 打印模型结构"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GTkd9SI7cby",
        "outputId": "ded31875-a66c-488b-abbb-4ec370ca0431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DIN(\n",
            "  (user_embedding): Embedding(1000, 32)\n",
            "  (item_embedding): Embedding(2000, 32)\n",
            "  (category_embedding): Embedding(10, 32)\n",
            "  (numerical_fc): Linear(in_features=2, out_features=32, bias=True)\n",
            "  (attention_pooling): AttentionPoolingLayer(\n",
            "    (attention_fc): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**queries * keys**: 这是两个张量的元素乘法操作。\n",
        "\n",
        "queries 代表目标商品的 embedding 向量，形状为 (batch_size, 1, embedding_dim)。\n",
        "keys 代表用户历史行为序列中每个商品的 embedding 向量，形状为 (batch_size, seq_len, embedding_dim)。\n",
        "\n",
        "元素乘法操作会将 queries 和 keys 中对应位置的元素相乘，得到一个新的张量，形状为 (batch_size, seq_len, embedding_dim)。\n",
        "\n",
        "**torch.sum(...)**: 这是对张量进行求和的操作。\n",
        "\n",
        "dim=-1 指定了求和的维度，-1 表示最后一个维度，也就是 embedding_dim 这个维度。\n",
        "因此，torch.sum(queries * keys, dim=-1) 会将 queries * keys 这个张量在 embedding_dim 维度上进行求和，得到一个新的张量，形状为 (batch_size, seq_len)。\n",
        "\n",
        "\n",
        "**整体含义**:\n",
        "\n",
        "这行代码的整体含义是计算目标商品与每个历史行为商品之间的相似度，作为注意力分数。\n",
        "\n",
        "首先，通过元素乘法 (queries * keys) 计算目标商品 embedding 与每个历史行为商品 embedding 的对应元素乘积。\n",
        "\n",
        "然后，通过在 embedding_dim 维度上求和 (torch.sum(...)) 得到一个标量值，这个标量值代表了目标商品与某个历史行为商品之间的相似度。\n",
        "\n",
        "最终得到的张量 attention_scores 形状为 (batch_size, seq_len)，其中每个元素都代表目标商品与对应历史行为商品的相似度（注意力分数）。\n",
        "\n",
        "**直观理解**：\n",
        "\n",
        "可以将 queries 看作是目标商品的“查询向量”，将 keys 看作是历史行为商品的“键向量”。torch.sum(queries * keys, dim=-1) 的作用就是计算“查询向量”与每个“键向量”之间的相似度，相似度越高，注意力分数就越高，说明目标商品与该历史行为商品越相关。"
      ],
      "metadata": {
        "id": "k4saCipCSWki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用示例数据进行一次前向预测\n",
        "model.eval() # 设置模型为评估模式 (不进行梯度计算)\n",
        "with torch.no_grad(): # 上下文管理器，禁止梯度计算\n",
        "    input_data = {k: v for k, v in sample_data.items() if k != 'label'}\n",
        "    predictions = model(**input_data) # 将 input_data 字典作为参数传入模型\n",
        "    print(\"预测结果 (前 10 个样本):\", predictions[:10])\n",
        "    print(\"预测结果形状:\", predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmT2eWvC8G8e",
        "outputId": "dbd18528-8079-4c93-b04f-142a53572002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "预测结果 (前 10 个样本): tensor([1.6251e-01, 1.7627e-04, 2.3034e-01, 4.2826e-01, 5.1056e-04, 1.3396e-01,\n",
            "        4.6400e-03, 2.7882e-03, 3.0229e-02, 1.2459e-01])\n",
            "预测结果形状: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 定义模型的超参数\n",
        "embedding_dim = 32   # Embedding 向量的维度\n",
        "hidden_units = [64, 32]   # 全连接层隐藏单元数量\n",
        "num_users = 1000   # 用户总数\n",
        "num_items = 2000   # 商品总数\n",
        "num_categories = 50   # 商品类别总数\n",
        "num_genders = 2 # 用户性别数量\n",
        "\n",
        "# 定义一个函数来生成模拟数据\n",
        "def generate_sample_data(batch_size=64):\n",
        "    # 生成用户 ID\n",
        "    user_ids = torch.randint(0, num_users, (batch_size,))\n",
        "    # 生成用户性别\n",
        "    user_genders = torch.randint(0, num_genders, (batch_size,))\n",
        "    # 生成用户年龄\n",
        "    user_ages = torch.randint(18, 60, (batch_size,)).float()\n",
        "\n",
        "    # 生成商品 ID\n",
        "    item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成商品类别\n",
        "    item_categories = torch.randint(0, num_categories, (batch_size,))\n",
        "    # 生成商品价格\n",
        "    item_prices = torch.rand(batch_size) * 1000\n",
        "\n",
        "    # 生成用户的历史行为序列\n",
        "    history_item_ids_list = []\n",
        "    for _ in range(batch_size):\n",
        "        seq_len = torch.randint(1, 6, (1,)).item()\n",
        "        history_seq = torch.randint(0, num_items, (seq_len,))\n",
        "        history_item_ids_list.append(history_seq)\n",
        "\n",
        "    # 填充历史行为序列到相同长度\n",
        "    max_len = max([seq.size(0) for seq in history_item_ids_list])\n",
        "    padded_history_item_ids = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    for i, seq in enumerate(history_item_ids_list):\n",
        "        padded_history_item_ids[i, :seq.size(0)] = seq\n",
        "\n",
        "    # 生成目标商品 ID\n",
        "    target_item_ids = torch.randint(0, num_items, (batch_size,))\n",
        "    # 生成点击标签\n",
        "    labels = torch.randint(0, 2, (batch_size,)).float()\n",
        "\n",
        "    return {\n",
        "        'user_id': user_ids,\n",
        "        'user_gender': user_genders,\n",
        "        'user_age': user_ages.unsqueeze(1),\n",
        "        'item_id': item_ids,\n",
        "        'item_category': item_categories,\n",
        "        'item_price': item_prices.unsqueeze(1),\n",
        "        'history_item_ids': padded_history_item_ids,\n",
        "        'target_item_id': target_item_ids,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# 调用 generate_sample_data 函数生成一个 batch 的数据\n",
        "sample_data = generate_sample_data()\n",
        "# 打印示例数据的键值\n",
        "print(\"示例数据：\", sample_data.keys())\n",
        "# 打印历史行为序列的形状\n",
        "print(\"历史行为序列形状:\", sample_data['history_item_ids'].shape)\n",
        "\n",
        "\n",
        "# 注意力池化层的实现 (标准 MLP 注意力网络)\n",
        "class AttentionPoolingLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(AttentionPoolingLayer, self).__init__()\n",
        "        # 使用 MLP 注意力网络\n",
        "        self.attention_fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 2, embedding_dim), # 输入维度为 query 和 key 拼接后的维度\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, 1) # 输出维度为 1，表示注意力权重\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys):\n",
        "        \"\"\"\n",
        "        前向传播函数，计算注意力权重并进行加权池化\n",
        "        :param queries: 目标商品的嵌入向量 (batch_size, embedding_dim)\n",
        "        :param keys: 历史行为序列的嵌入向量 (batch_size, seq_len, embedding_dim)\n",
        "        :return: 注意力池化后的用户兴趣表示 (batch_size, embedding_dim)\n",
        "        \"\"\"\n",
        "        # 1. 计算注意力分数\n",
        "        queries = queries.unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
        "        # 扩展 queries 以便与 keys 进行拼接\n",
        "        queries = queries.expand(-1, keys.size(1), -1) # (batch_size, seq_len, embedding_dim)\n",
        "        # 将 query 和 key 拼接在一起\n",
        "        attention_input = torch.cat([queries, keys], dim=-1) # (batch_size, seq_len, embedding_dim * 2)\n",
        "        # 通过 MLP 注意力网络计算注意力分数\n",
        "        attention_scores = self.attention_fc(attention_input.view(-1, attention_input.size(-1))).view(keys.size(0), keys.size(1)) # (batch_size, seq_len)\n",
        "\n",
        "        # 2. 使用 Softmax 归一化注意力分数，得到注意力权重\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1) # (batch_size, seq_len)\n",
        "\n",
        "        # 3. 加权求和得到注意力池化后的结果\n",
        "        attention_output = torch.sum(attention_weights.unsqueeze(-1) * keys, dim=1) # (batch_size, embedding_dim)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class DIN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_categories, num_genders, embedding_dim, hidden_units):\n",
        "        super(DIN, self).__init__()\n",
        "\n",
        "        # 创建用户嵌入层\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        # 创建用户性别嵌入层\n",
        "        self.user_gender_embedding = nn.Embedding(num_genders, embedding_dim)\n",
        "        # 创建商品嵌入层\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        # 创建类别嵌入层\n",
        "        self.category_embedding = nn.Embedding(num_categories, embedding_dim)\n",
        "\n",
        "        # 创建数值型特征的线性层\n",
        "        self.numerical_fc = nn.Linear(2, embedding_dim)\n",
        "\n",
        "        # 创建注意力池化层\n",
        "        self.attention_pooling = AttentionPoolingLayer(embedding_dim)\n",
        "\n",
        "        # 创建全连接层\n",
        "        fc_layers = []\n",
        "        # 计算全连接层的输入维度 (6 个 embedding 特征)\n",
        "        input_dim = embedding_dim * 6  # 用户，用户性别，商品，类别，数值型特征，注意力输出\n",
        "        for units in hidden_units:\n",
        "            fc_layers.append(nn.Linear(input_dim, units))\n",
        "            fc_layers.append(nn.ReLU())\n",
        "            input_dim = units\n",
        "        fc_layers.append(nn.Linear(input_dim, 1))\n",
        "        self.fc = nn.Sequential(*fc_layers)\n",
        "\n",
        "\n",
        "    def forward(self, user_id, user_gender, user_age, item_id, item_category, item_price, history_item_ids, target_item_id):\n",
        "        \"\"\"\n",
        "        DIN模型的前向传播函数\n",
        "\n",
        "        输入:\n",
        "            user_id: 用户ID, 形状: (batch_size,)\n",
        "            user_gender: 用户性别, 形状: (batch_size,)\n",
        "            user_age: 用户年龄, 形状: (batch_size, 1)\n",
        "            item_id: 商品ID, 形状: (batch_size,)\n",
        "            item_category: 商品类别, 形状: (batch_size,)\n",
        "            item_price: 商品价格, 形状: (batch_size, 1)\n",
        "            history_item_ids: 用户历史行为序列, 形状: (batch_size, seq_len)\n",
        "            target_item_id: 目标商品ID, 形状: (batch_size,)\n",
        "\n",
        "        输出:\n",
        "            output: 点击率预测值, 形状: (batch_size,)\n",
        "        \"\"\"\n",
        "        # 1. 获取各个特征的嵌入向量\n",
        "        user_embed = self.user_embedding(user_id)\n",
        "        user_gender_embed = self.user_gender_embedding(user_gender)\n",
        "        item_embed = self.item_embedding(item_id)\n",
        "        category_embed = self.category_embedding(item_category)\n",
        "        target_item_embed = self.item_embedding(target_item_id)\n",
        "        history_item_embed = self.item_embedding(history_item_ids)\n",
        "\n",
        "\n",
        "        # 2. 处理数值型特征\n",
        "        numerical_features = torch.cat([user_age, item_price], dim=-1)\n",
        "        numerical_embed = F.relu(self.numerical_fc(numerical_features))\n",
        "\n",
        "        # 3. 使用注意力池化层对历史行为进行加权\n",
        "        attention_output = self.attention_pooling(queries=target_item_embed, keys=history_item_embed)\n",
        "\n",
        "        # 4. 拼接所有特征\n",
        "        concat_features = torch.cat([user_embed, user_gender_embed, item_embed, category_embed, numerical_embed, attention_output], dim=-1)\n",
        "\n",
        "        # 5. 通过全连接层进行预测\n",
        "        output = self.fc(concat_features)\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        return output.squeeze(1)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# 初始化 DIN 模型\n",
        "num_users = 1000\n",
        "num_items = 2000\n",
        "num_categories = 50\n",
        "num_genders = 2\n",
        "embedding_dim = 64\n",
        "hidden_units = [128, 64]\n",
        "model = DIN(num_users, num_items, num_categories, num_genders, embedding_dim, hidden_units)\n",
        "print(model)\n",
        "\n",
        "# 损失函数和优化器\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# LogLoss 计算函数 (PyTorch 版本)\n",
        "def log_loss(y_true, y_pred):\n",
        "    y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
        "\n",
        "# 训练循环\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx in range(0, 1000, batch_size):\n",
        "        train_batch_data = generate_sample_data(batch_size)\n",
        "        labels = train_batch_data['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3. 前向传播\n",
        "        predictions = model(train_batch_data['user_id'],\n",
        "                          train_batch_data['user_gender'],\n",
        "                          train_batch_data['user_age'],\n",
        "                          train_batch_data['item_id'],\n",
        "                          train_batch_data['item_category'],\n",
        "                          train_batch_data['item_price'],\n",
        "                          train_batch_data['history_item_ids'],\n",
        "                          train_batch_data['target_item_id'])\n",
        "\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / (1000 / batch_size)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"训练完成!\")\n",
        "\n",
        "# 评估循环\n",
        "def evaluate_model(model, batch_size=64):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    total_eval_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx in range(0, 500, batch_size):\n",
        "            eval_batch_data = generate_sample_data(batch_size)\n",
        "            labels = eval_batch_data['label']\n",
        "\n",
        "            # 2. 前向传播\n",
        "            predictions = model(eval_batch_data['user_id'],\n",
        "                              eval_batch_data['user_gender'],\n",
        "                              eval_batch_data['user_age'],\n",
        "                              eval_batch_data['item_id'],\n",
        "                              eval_batch_data['item_category'],\n",
        "                              eval_batch_data['item_price'],\n",
        "                              eval_batch_data['history_item_ids'],\n",
        "                              eval_batch_data['target_item_id'])\n",
        "\n",
        "            eval_loss = loss_fn(predictions, labels)\n",
        "            total_eval_loss += eval_loss.item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / (500 / batch_size)\n",
        "    auc_score = roc_auc_score(all_labels, all_predictions)\n",
        "    logloss_score = log_loss(torch.tensor(all_labels), torch.tensor(all_predictions)).item()\n",
        "\n",
        "    print(f\"Evaluation - Average Loss: {avg_eval_loss:.4f}, AUC: {auc_score:.4f}, LogLoss: {logloss_score:.4f}\")\n",
        "    return avg_eval_loss, auc_score, logloss_score\n",
        "\n",
        "\n",
        "# 在训练完成后进行评估\n",
        "print(\"开始评估...\")\n",
        "evaluate_model(model)\n",
        "print(\"评估完成!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHGk7gUqZrCg",
        "outputId": "0d3bf229-5041-45fb-d786-fd48de369044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "示例数据： dict_keys(['user_id', 'user_gender', 'user_age', 'item_id', 'item_category', 'item_price', 'history_item_ids', 'target_item_id', 'label'])\n",
            "历史行为序列形状: torch.Size([64, 5])\n",
            "DIN(\n",
            "  (user_embedding): Embedding(1000, 64)\n",
            "  (user_gender_embedding): Embedding(2, 64)\n",
            "  (item_embedding): Embedding(2000, 64)\n",
            "  (category_embedding): Embedding(50, 64)\n",
            "  (numerical_fc): Linear(in_features=2, out_features=64, bias=True)\n",
            "  (attention_pooling): AttentionPoolingLayer(\n",
            "    (attention_fc): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch [1/10], Average Loss: 1.2030\n",
            "Epoch [2/10], Average Loss: 0.7499\n",
            "Epoch [3/10], Average Loss: 0.7530\n",
            "Epoch [4/10], Average Loss: 0.7222\n",
            "Epoch [5/10], Average Loss: 0.7367\n",
            "Epoch [6/10], Average Loss: 0.7633\n",
            "Epoch [7/10], Average Loss: 0.7761\n",
            "Epoch [8/10], Average Loss: 0.7748\n",
            "Epoch [9/10], Average Loss: 0.7291\n",
            "Epoch [10/10], Average Loss: 0.7215\n",
            "训练完成!\n",
            "开始评估...\n",
            "Evaluation - Average Loss: 0.7267, AUC: 0.5025, LogLoss: 0.7097\n",
            "评估完成!\n"
          ]
        }
      ]
    }
  ]
}